# Configurazione Preprocessing Pipeline
# =================================

# Logging Configuration
logging:
  level: ${LOG_LEVEL:-INFO}
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: ${LOG_FILE:-'logs/pipeline.log'}
  console: ${LOG_CONSOLE:-true}
  rotate:
    enabled: ${LOG_ROTATE:-false}
    max_bytes: ${LOG_MAX_BYTES:-10485760}
    backup_count: ${LOG_BACKUP_COUNT:-5}

# Execution Configuration
execution:
  # Passi della pipeline da eseguire: [schema, dataset, preprocessing, training, evaluation]
  # ["all"] esegue tutti i passi
  # ["ml"] esegue solo preprocessing + training + evaluation
  steps: ["all"]
  # Se true, forza la rielaborazione anche se gli output intermedi esistono già
  force_reload: true

# Experiment tracking (Weights & Biases)
tracking:
  wandb:
    enabled: ${WANDB_ENABLED:-true}
    project: ${WANDB_PROJECT:-'stimatrix'}
    entity: ${WANDB_ENTITY:-null}
    group: ${WANDB_GROUP:-null}
    tags: []
    mode: ${WANDB_MODE:-'online'}  # online | offline | disabled
    name: null

# Paths Configuration
paths:
  raw_data: ${RAW_DATA_DIR:-'data/raw'}
  preprocessed_data: ${PREPROC_DIR:-'data/preprocessed'}
  schema: ${SCHEMA_PATH:-'schema/db_schema.json'}
  models_dir: ${MODELS_DIR:-'models'}
  raw_filename: 'raw.parquet'
  preprocessed_filename: 'preprocessed.parquet'

# Temporal and zone filtering (reduce drift and improve model quality)
temporal_filter:
  enabled: true
  min_year: 2022        # Filtra transazioni >= 2022 (elimina drift temporale)
  min_month: null       # Opzionale: mese minimo (null = tutto l'anno)
  exclude_zones:        # Zone con troppo pochi campioni
    - 'E1'
    - 'E2'
    - 'E3'
    - 'R1'
  exclude_tipologie:    # Escludi ville (pochi campioni)
    - '4'              # Ville: €172k medio - TROPPO POCHI (rischio overfitting)

# Database Configuration (for data retrieval)
database:
  schema_name: null
  selected_aliases: ['A', 'AI', 'PC', 'ISC', 'II', 'PC_OZ', 'OZ', 'OV', 'C1', 'C2']  # C1=attiimmobili_cened1, C2=attiimmobili_cened2
  # Custom alias mapping for specific tables/views (optional)
  # If not specified, defaults are used for known views (C1, C2)
  custom_aliases:
    attiimmobili_cened1: 'C1'
    attiimmobili_cened2: 'C2'
  use_poi: true
  use_ztl: true
  output_format: 'parquet'
  compression: 'snappy'

# Target configuration
target:
  column_candidates: ['AI_Prezzo_Ridistribuito']  # Solo prezzo redistribuito
  
  # Target transformation (applied to reduce skewness and improve model performance)
  # Options: 'none', 'log', 'log10', 'sqrt', 'boxcox', 'yeojohnson'
  transform: 'log'  # CAMBIATO: da boxcox a log (più stabile, meno gap train-test)
  
  # Optional: offset for log10 transform (y -> log10(y + offset))
  log10_offset: 1.0
  
  # Transformation guide:
  # - 'none': No transformation
  # - 'log': log(1+y) - handles zeros, best for right-skewed data
  # - 'log10': log10(y + offset) - similar to log but different scale
  # - 'sqrt': sqrt(y) - milder than log, requires y >= 0
  # - 'boxcox': Automatically finds optimal lambda, requires y > 0 (auto-shifts if needed)
  # - 'yeojohnson': Like Box-Cox but works with any y (recommended for flexibility)

# Outlier detection
outliers:
  method: 'ensemble'       # 'iqr' | 'zscore' | 'iso_forest' | 'ensemble'
  z_thresh: 3.0            # Threshold for z-score method
  iqr_factor: 1.2          # IQR factor for outlier detection
  iso_forest_contamination: 0.05  # Contamination rate for isolation forest
  group_by_col: 'AI_ZonaOmi'  # Column to group by for outlier detection
  min_group_size: 30       # minimum samples per group to apply group-wise outlier detection
  fallback_strategy: 'global'  # 'skip' keeps all small groups; 'global' applies global detection

# Missing value imputation
imputation:
  numeric_strategy: 'median'      # 'median' | 'mean'
  categorical_strategy: 'most_frequent'
  group_by_col: 'AI_ZonaOmi'  # CAMBIATO: da TipologiaEdilizia a ZonaOmi (più predittivo)

# Categorical encoding - Multi-strategy based on cardinality
encoding: &encoding_defaults
  # Strategy selection by number of unique values:
  one_hot_max: 10                     # ≤10 unique → One-Hot Encoding
  target_encoding_range: [11, 30]     # 11-30 → Target Encoding (with smoothing)
  frequency_encoding_range: [31, 100] # 31-100 → Frequency Encoding
  ordinal_encoding_range: [101, 200]  # 101-200 → Ordinal Encoding
  drop_above: 200                     # >200 unique → DROP (too high cardinality)
  
  # Target encoding configuration (requires y_train)
  target_encoder:
    smoothing: 1.0              # Bayesian smoothing for rare categories
    min_samples_leaf: 1         # Minimum samples per leaf for target encoding

# Temporal split configuration
temporal_split:
  year_col: 'A_AnnoStipula'
  month_col: 'A_MeseStipula'
  mode: 'fraction'      # 'date' | 'fraction'
  fraction:
    train: 0.7          # fraction of time-ordered data for train
    valid: 0.2          # fraction of time-ordered data for validation (0 disables val)
  date:
    test_start_year: 2023 # used only if mode=='date'
    test_start_month: 1   # used only if mode=='date'

# Numeric coercion configuration
numeric_coercion:
  enabled: true
  threshold: 0.95
  blacklist_globs:
    - 'II_*'
    - 'AI_Id*'
    - 'Foglio'
    - 'Particella*'
    - 'Subalterno'
    - 'SezioneAmministrativa'
    - 'ZonaOmi'
    - '*COD*'

# PCA configuration
pca: &pca_defaults
  enabled: false
  n_components: 0.95
  random_state: 42

# Correlation pruning
# La correlazione viene calcolata solo sulle colonne numeriche, non sulle OHE
correlation: &correlation_defaults
  numeric_threshold: 0.95

# Non-descriptive columns pruning
drop_non_descriptive:
  na_threshold: 0.80

# Feature extraction toggles
feature_extraction:
  geometry: false
  json: false

# Feature pruning
feature_pruning:
  drop_columns:
    - 'A_ImmobiliPrincipaliConSuperficieValorizzata'
    - 'AI_SuperficieCalcolata'
    - 'AI_SuperficieVisuraTotale'
    - 'AI_SuperficieVisuraTotaleE'
    - 'AI_SuperficieVisuraTotaleAttuale'
    - 'AI_SuperficieVisuraTotaleEAttuale'
    - 'A_AcquirentiCount'
    - 'A_VenditoriCount'
    - 'A_EtaMediaAcquirenti'
    - 'A_EtaMediaVenditori'
    - 'A_AcquirentiVenditoriStessoCognome'
    - 'A_VenditoriEredita'
    - 'AI_Subalterno'
    - 'AI_Piano'
    - 'AI_Rendita'

# Scaling configuration
scaling: &scaling_defaults
  scaler_type: 'standard'   # 'standard' | 'robust' | 'none'
  with_mean: true
  with_std: true

# Winsorization (quantile clipping on numeric features)
winsorization: &winsor_defaults
  enabled: true
  lower_quantile: 0.01
  upper_quantile: 0.99

# Profiles: generate multiple datasets tailored to model families
# SEMPLIFICATO: Solo 2 profili necessari per modelli tree-based
profiles:
  scaled:
    # Disabilitato: non più necessario dopo rimozione modelli lineari
    enabled: false
    output_prefix: 'scaled'
    encoding: *encoding_defaults
    winsorization: *winsor_defaults
    scaling: *scaling_defaults
    pca: *pca_defaults
    correlation: *correlation_defaults
  tree:
    # Profilo principale per XGBoost, LightGBM, Random Forest, GBR, HGBT
    enabled: true
    output_prefix: 'tree'
    encoding: *encoding_defaults
    correlation: *correlation_defaults
  catboost:
    # Profilo specifico per CatBoost (mantiene categoriche native)
    enabled: true
    output_prefix: 'catboost'
    correlation: *correlation_defaults

# Diagnostics configuration
diagnostics:
  # Residual analysis (per-group error analysis)
  residual_analysis:
    enabled: true
    by_groups:
      - AI_ZonaOmi
      - AI_IdCategoriaCatastale
      - price_quartile  # Automatic quartile grouping
    save_worst_predictions: true
    top_n_worst: 50
    plots:
      - residual_vs_predicted
      - residual_vs_actual
      - residual_distribution
      - qq_plot
  
  # Drift detection (train vs test distribution shift)
  drift_detection:
    enabled: true
    methods:
      - psi        # Population Stability Index
      - ks_test    # Kolmogorov-Smirnov test
    alert_threshold: 0.15  # PSI > 0.15 = significant drift
    save_report: true

# Uncertainty quantification
uncertainty:
  # Prediction intervals (bootstrap-based)
  prediction_intervals:
    enabled: true
    method: 'residual_bootstrap'
    n_bootstraps: 100
    confidence_levels: [0.8, 0.9]  # 80% and 90% confidence intervals

# Training configuration
training:
  # r2 | neg_mean_absolute_percentage_error | neg_mean_squared_error | neg_root_mean_squared_error | neg_mean_absolute_error
  primary_metric: "neg_mean_absolute_percentage_error"
  report_metrics: ["r2", "rmse", "mse", "mae", "mape"]
  sampler: "auto"   # auto (OptunaHub) | tpe
  seed: ${SEED:-42}
  n_jobs_default: ${N_JOBS:--1}
  timeout: ${TRAIN_TIMEOUT:-null}
  trials_base: &trials_base 50
  trials_advanced: &trials_advanced 100
  # Cross-validation usata quando non esiste un validation set esterno (valid_fraction=0)
  cv_when_no_val:
    enabled: true
    kind: kfold   # 'timeseries' | 'kfold'
    n_splits: 5
    shuffle: true     # usato solo per kfold

  # Definizione per-modello (schema minimale)
  # SEMPLIFICATO: Solo modelli tree-based, i migliori per dati immobiliari
  models:
    # ========================================
    # MODELLI LINEARI: DISABILITATI
    # ========================================
    linear:
      enabled: false
      profile: scaled
      trials: 1
      base_params: {}
      fit_params: {}
      search_space: {}
    ridge:
      enabled: false
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        alpha: {type: float, low: 0.000001, high: 1000.0, log: true}
    lasso:
      enabled: false
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        alpha: {type: float, low: 0.0001, high: 10.0, log: true}
    elasticnet:
      enabled: false
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        alpha: {type: float, low: 0.0001, high: 10.0, log: true}
        l1_ratio: {type: float, low: 0.0, high: 1.0}
    # ========================================
    # KNN: DISABILITATO
    # Lento e poco performante con molte feature
    # ========================================
    knn:
      enabled: false
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        n_neighbors: {type: int, low: 3, high: 50}
        weights: {type: categorical, choices: ["uniform", "distance"]}
        p: {type: int, low: 1, high: 2}
    # ========================================
    # SVR: DISABILITATO
    # Lento su dataset grandi, poco performante
    # ========================================
    svr:
      enabled: false
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        kernel: {type: categorical, choices: ["rbf", "poly", "sigmoid"]}
        C: {type: float, low: 0.01, high: 1000.0, log: true}
        epsilon: {type: float, low: 0.001, high: 1.0, log: true}
        gamma: {type: float, low: 0.0001, high: 10.0, log: true}
        degree: {type: int, low: 2, high: 5}
        coef0: {type: float, low: 0.0, high: 1.0}
    # ========================================
    # DECISION TREE SINGOLO: DISABILITATO
    # ========================================
    dt:
      enabled: false
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        max_depth: {type: int, low: 3, high: 30}
        min_samples_split: {type: int, low: 2, high: 20}
        min_samples_leaf: {type: int, low: 1, high: 20}
        max_features: {type: categorical, choices: ["sqrt", "log2", null]}
        min_impurity_decrease: {type: float, low: 0.0, high: 0.01}
    # ========================================
    # MODELLI TREE-BASED: ABILITATI ✅
    # I migliori per dati immobiliari complessi
    # ========================================
    rf:
      # Random Forest: robusto, veloce, ottimo baseline
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        n_estimators: {type: int, low: 200, high: 1200}
        max_depth: {type: int, low: 4, high: 40}
        min_samples_split: {type: int, low: 2, high: 20}
        min_samples_leaf: {type: int, low: 1, high: 20}
        max_features: {type: categorical, choices: ["sqrt", "log2", null]}
        bootstrap: {type: categorical, choices: [true, false]}
    gbr:
      # Gradient Boosting (sklearn): buone performance ma più lento
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
        n_estimators: {type: int, low: 200, high: 1500}
        max_depth: {type: int, low: 2, high: 8}
        subsample: {type: float, low: 0.5, high: 1.0}
        min_samples_leaf: {type: int, low: 1, high: 20}
        max_features: {type: categorical, choices: ["sqrt", "log2", null]}
        min_samples_split: {type: int, low: 2, high: 20}
        min_impurity_decrease: {type: float, low: 0.0, high: 0.01}
    hgbt:
      # HistGradientBoosting: veloce e performante, supporta NaN nativamente
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
        max_depth: {type: int, low: 2, high: 16}
        max_leaf_nodes: {type: int, low: 7, high: 255}
        l2_regularization: {type: float, low: 0.000001, high: 10.0, log: true}
        max_bins: {type: int, low: 64, high: 255}
        min_samples_leaf: {type: int, low: 1, high: 50}
    xgboost:
      # XGBoost: solitamente il migliore per problemi tabular
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        n_estimators: {type: int, low: 300, high: 1500}
        max_depth: {type: int, low: 3, high: 8}
        learning_rate: {type: float, low: 0.01, high: 0.1, log: true}
        subsample: {type: float, low: 0.6, high: 0.9}
        colsample_bytree: {type: float, low: 0.6, high: 0.9}
        min_child_weight: {type: float, low: 1.0, high: 10.0, log: true}
        reg_alpha: {type: float, low: 1e-4, high: 10.0, log: true}
        reg_lambda: {type: float, low: 1e-4, high: 10.0, log: true}
        gamma: {type: float, low: 1e-4, high: 10.0, log: true}
    lightgbm:
      # LightGBM: molto veloce, ottimo per dataset grandi
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        n_estimators: {type: int, low: 300, high: 1500}
        max_depth: {type: categorical, choices: [-1, 4, 5, 6, 7, 8]}
        learning_rate: {type: float, low: 0.01, high: 0.1, log: true}
        num_leaves: {type: int, low: 16, high: 63}
        subsample: {type: float, low: 0.6, high: 0.9}
        colsample_bytree: {type: float, low: 0.6, high: 0.9}
        reg_alpha: {type: float, low: 1e-4, high: 10.0, log: true}
        reg_lambda: {type: float, low: 1e-4, high: 10.0, log: true}
        min_child_samples: {type: int, low: 20, high: 200}
        min_split_gain: {type: float, low: 0.0, high: 1.0}
    catboost:
      # CatBoost: ottima gestione categoriche, no preprocessing necessario
      enabled: true
      profile: catboost
      trials: *trials_advanced
      base_params:
        allow_writing_files: false  # Disabilita creazione cartella catboost_info
      fit_params:
        cat_features: __categorical_indices__
      search_space:
        depth: {type: int, low: 4, high: 10}
        learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
        l2_leaf_reg: {type: float, low: 10.0, high: 100.0}
        bagging_temperature: {type: float, low: 0.5, high: 5.0}
        border_count: {type: int, low: 16, high: 255}
        random_strength: {type: float, low: 0.0, high: 2.0}
        rsm: {type: float, low: 0.5, high: 1.0}

  # SHAP feature importance
  shap:
    enabled: true
    sample_size: 500
    max_display: 50
    save_plots: true
    save_values: true

  # Ensemble settings
  ensembles:
    voting:
      enabled: true
      top_n: 3
      tune_weights: true
    stacking:
      enabled: true
      top_n: 5
      # Meta-learner: Ridge è ideale (semplice, previene overfitting, interpretabile)
      final_estimator: "ridge"
      cv_folds: 5

# Evaluation configuration
evaluation:
  group_metrics:
    enabled: true
    group_by_columns: ['AI_ZonaOmi', 'AI_IdCategoriaCatastale', 'AI_IdTipologiaEdilizia']
    min_group_size: 10
    original_scale: true
    report_metrics: ['r2', 'rmse', 'mse', 'mae', 'mape', 'medae']
    price_band:
      method: 'quantile'   # 'quantile' | 'fixed'
      quantiles: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
      # fixed_edges: [0, 100000, 200000, 400000, 800000, 1600000, .inf]
      mape_floor: 1000.0
      merge_small_bins: true
      label_prefix: 'PREZZO_'