# Configurazione Preprocessing Pipeline
# =================================

# Logging Configuration
logging:
  level: ${LOG_LEVEL:-INFO}
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: ${LOG_FILE:-'logs/pipeline.log'}
  console: ${LOG_CONSOLE:-true}
  rotate:
    enabled: ${LOG_ROTATE:-false}
    max_bytes: ${LOG_MAX_BYTES:-10485760}
    backup_count: ${LOG_BACKUP_COUNT:-5}

# Execution Configuration
execution:
  # Passi della pipeline da eseguire: [schema, dataset, preprocessing, training, evaluation] oppure ["all"]
  steps: ["all"]
  # Se true, forza la rielaborazione anche se gli output intermedi esistono già
  force_reload: false

# Experiment tracking (Weights & Biases)
tracking:
  wandb:
    enabled: ${WANDB_ENABLED:-true}
    project: ${WANDB_PROJECT:-'stimatrix'}
    entity: ${WANDB_ENTITY:-null}
    group: ${WANDB_GROUP:-null}
    tags: []
    mode: ${WANDB_MODE:-'online'}  # online | offline | disabled
    name: null

# Paths Configuration
paths:
  raw_data: ${RAW_DATA_DIR:-'data/raw'}
  preprocessed_data: ${PREPROC_DIR:-'data/preprocessed'}
  schema: ${SCHEMA_PATH:-'schema/db_schema.json'}
  models_dir: ${MODELS_DIR:-'models'}
  raw_filename: 'raw.parquet'
  preprocessed_filename: 'preprocessed.parquet'

# Database Configuration (for data retrieval)
database:
  schema_name: null
  selected_aliases: ['A', 'AI', 'PC', 'ISC', 'II', 'PC_OZ', 'OZ', 'OV']
  use_poi: true
  use_ztl: true
  output_format: 'parquet'
  compression: 'snappy'

# Target configuration
target:
  column_candidates: ['AI_Prezzo_Ridistribuito']
  log_transform: false

# Outlier detection
outliers:
  method: 'ensemble'       # 'iqr' | 'zscore' | 'iso_forest' | 'ensemble'
  z_thresh: 4.0
  iqr_factor: 3.0
  iso_forest_contamination: 0.01
  group_by_col: 'AI_IdTipologiaEdilizia'
  min_group_size: 30       # minimum samples per group to apply group-wise outlier detection
  fallback_strategy: 'global'  # 'skip' keeps all small groups; 'global' applies global detection

# Missing value imputation
imputation:
  numeric_strategy: 'median'      # 'median' | 'mean'
  categorical_strategy: 'most_frequent'
  group_by_col: 'AI_IdTipologiaEdilizia'

# Categorical encoding
encoding: &encoding_defaults
  max_ohe_cardinality: 12

# Temporal split configuration
temporal_split:
  year_col: 'A_AnnoStipula'
  month_col: 'A_MeseStipula'
  mode: 'fraction'      # 'date' | 'fraction'
  fraction:
    train: 0.7          # Ridotto per validation set più robusto
    valid: 0.2          # Raddoppiato per stabilità tuning
    test: 0.1           # Mantieni per test finale
  date:
    test_start_year: 2023 # used only if mode=='date'
    test_start_month: 1   # used only if mode=='date'
  # Nuove opzioni per controllo qualità split
  min_samples_per_split: 500  # Minimo campioni per split valido
  ensure_temporal_order: true # Verifica ordine temporale rigoroso

# Numeric coercion configuration
numeric_coercion:
  enabled: true
  threshold: 0.95
  blacklist_globs:
    - 'II_*'
    - 'AI_Id*'
    - 'Foglio'
    - 'Particella*'
    - 'Subalterno'
    - 'SezioneAmministrativa'
    - 'ZonaOmi'
    - '*COD*'

# PCA configuration
pca: &pca_defaults
  enabled: false
  n_components: 0.95
  random_state: 42

# Correlation pruning
correlation: &correlation_defaults
  numeric_threshold: 0.98

# Non-descriptive columns pruning
drop_non_descriptive:
  na_threshold: 0.98

# Feature extraction toggles
feature_extraction:
  geometry: true
  json: true
  
  # Nuove feature derivate basate su correlazioni EDA
  derived_features:
    enabled: true
    
    # Ratios
    price_ratios:
      enabled: true
      features:
        - "rendita_per_mq"          # ✅ SAFE: AI_Rendita / AI_Superficie
        - "superficie_vs_media_zona" # ✅ SAFE: AI_Superficie / media_per_zona
        - "rendita_vs_media_zona"    # ✅ SAFE: AI_Rendita / media_zona
        - "superficie_per_rendita"   # ✅ SAFE: AI_Superficie / AI_Rendita
    
    # Features geospaziali (POI shopping: 0.29)
    spatial_features:
      enabled: true
      features:
        - "poi_density_total"       # Somma tutti i POI
        - "poi_shopping_ratio"      # POI_shopping / POI_total
        - "distance_to_center"      # Distanza euclidea da centro città
        - "zona_omi_price_rank"     # Ranking prezzo medio per zona
    
    # Features temporali
    temporal_features:
      enabled: true
      features:
        - "anno_stipula_normalized" # (A_AnnoStipula - min_year) / (max_year - min_year)
        - "mese_sin"               # sin(2π * A_MeseStipula / 12)
        - "mese_cos"               # cos(2π * A_MeseStipula / 12)
        - "trimestre"              # Trimestre dell'anno
    
    # Features categoriche aggregate SENZA DATA LEAKAGE
    categorical_aggregates:
      enabled: true
      group_by: ["AI_ZonaOmi", "AI_IdCategoriaCatastale", "AI_IdTipologiaEdilizia"]
      features:
        - "superficie_media_gruppo"  # ✅ SAFE: media superficie per gruppo
        - "superficie_std_gruppo"    # ✅ SAFE: std superficie per gruppo
        - "rendita_media_gruppo"     # ✅ SAFE: media rendita per gruppo
        - "rendita_std_gruppo"       # ✅ SAFE: std rendita per gruppo
        - "count_per_gruppo"         # ✅ SAFE: conteggio per gruppo

# Surface handling
surface:
  drop_columns:
    - 'A_ImmobiliPrincipaliConSuperficieValorizzata'
    - 'AI_SuperficieCalcolata'
    - 'AI_SuperficieVisuraTotale'
    - 'AI_SuperficieVisuraTotaleE'
    - 'AI_SuperficieVisuraTotaleAttuale'
    - 'AI_SuperficieVisuraTotaleEAttuale'

# Scaling configuration
scaling: &scaling_defaults
  scaler_type: 'standard'   # 'standard' | 'robust' | 'none'
  with_mean: true
  with_std: true

# Winsorization (quantile clipping on numeric features)
winsorization: &winsor_defaults
  enabled: false
  lower_quantile: 0.01
  upper_quantile: 0.99

# Profiles: generate multiple datasets tailored to model families
profiles:
  scaled:
    enabled: true
    output_prefix: 'scaled'
    encoding: *encoding_defaults
    winsorization: *winsor_defaults
    scaling: *scaling_defaults
    pca: *pca_defaults
    correlation: *correlation_defaults
  tree:
    enabled: true
    output_prefix: 'tree'
    encoding: *encoding_defaults
    correlation: *correlation_defaults
  catboost:
    enabled: true
    output_prefix: 'catboost'
    correlation: *correlation_defaults

# Training configuration
training:
  # Opzioni globali
  primary_metric: "neg_root_mean_squared_error" # 'r2' | 'neg_root_mean_squared_error' | 'neg_mean_squared_error' | 'neg_mean_absolute_error' | 'neg_mean_absolute_percentage_error'
  report_metrics: ["r2", "rmse", "mse", "mae", "mape"]
  sampler: "auto"   # auto (OptunaHub) | tpe
  seed: ${SEED:-42}
  n_jobs_default: ${N_JOBS:--1}
  timeout: ${TRAIN_TIMEOUT:-null}
  trials_base: &trials_base 25        # Ridotto da 50 per evitare overfitting
  trials_advanced: &trials_advanced 50    # Ridotto da 100 per evitare overfitting
  trials_conservative: &trials_conservative 15  # Nuova categoria per modelli sensibili
  
  # Early stopping configuration per tutti i modelli
  early_stopping:
    enabled: true
    patience: 20               # Numero di iterazioni senza miglioramento
    min_delta: 0.0001         # Miglioramento minimo considerato significativo
    restore_best_weights: true # Ripristina i migliori pesi
    monitor_metric: "validation_loss"  # Metrica da monitorare
  
  # Controlli di stabilità e diagnostica avanzata
  stability_checks:
    enabled: true
    
    # Learning curve analysis
    learning_curves:
      enabled: true
      train_sizes: [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]
      cv_folds: 5
      save_plots: true
    
    # Residual analysis
    residual_analysis:
      enabled: true
      plot_types: ["scatter", "histogram", "qq"]
      save_plots: true
      sample_size: 1000
    
    # Feature importance stability
    feature_importance_stability:
      enabled: true
      n_bootstrap: 10
      bootstrap_ratio: 0.8
      save_importance_plots: true
    
    # Model comparison diagnostics
    model_comparison:
      enabled: true
      include_baselines: true
      statistical_tests: ["wilcoxon", "friedman"]
      confidence_level: 0.95
    
    # Overfitting detection thresholds
    overfitting_thresholds:
      r2_gap_warning: 0.15      # Gap R² train-test
      r2_gap_critical: 0.25
      rmse_ratio_warning: 1.5   # Ratio RMSE test/train  
      rmse_ratio_critical: 2.0
  # Cross-validation usata quando non esiste un validation set esterno (valid_fraction=0)
  cv_when_no_val:
    enabled: true
    kind: timeseries   # 'timeseries' | 'kfold'
    n_splits: 10       # Aumentato per maggiore robustezza
    shuffle: false     # usato solo per kfold
    test_size: 0.2     # Dimensione test set per TimeSeries CV
    gap: 0             # Gap temporale tra train e test (in giorni)
  
  # Nuova strategia: CV anche con validation set per doppio controllo
  cv_with_val:
    enabled: true      # Abilita CV anche quando hai validation set
    kind: timeseries
    n_splits: 5        # Meno split perché hai già validation set
    use_for_final_selection: false  # Usa solo per diagnostica
  
  # Tuning specifico per regolarizzazione (priorità alta per overfitting)
  regularization_tuning:
    enabled: true
    # Prima fase: tuning solo regolarizzazione con parametri base fissi
    phase1:
      enabled: true
      trials: 20
      focus_params: ["reg_alpha", "reg_lambda", "l2_regularization", "alpha", "l1_ratio"]
      fix_complexity: true  # Fissa parametri di complessità (depth, n_estimators, etc.)
    
    # Seconda fase: tuning complessità con regolarizzazione ottimale
    phase2:
      enabled: true  
      trials: 15
      focus_params: ["max_depth", "n_estimators", "num_leaves", "learning_rate"]
      use_phase1_regularization: true

  # Definizione per-modello (schema minimale)
  models:
    linear:
      enabled: true
      profile: scaled
      trials: 1 # no hyperparams
      base_params: {}
      fit_params: {}
      search_space: {}
    ridge:
      enabled: true
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        alpha: {type: float, low: 0.000001, high: 1000.0, log: true}
    lasso:
      enabled: true
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        alpha: {type: float, low: 0.0001, high: 10.0, log: true}
    elasticnet:
      enabled: true
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        alpha: {type: float, low: 0.0001, high: 10.0, log: true}
        l1_ratio: {type: float, low: 0.0, high: 1.0}
    knn:
      enabled: true
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        n_neighbors: {type: int, low: 3, high: 50}
        weights: {type: categorical, choices: ["uniform", "distance"]}
        p: {type: int, low: 1, high: 2}
    svr:
      enabled: false
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        kernel: {type: categorical, choices: ["rbf", "poly", "sigmoid"]}
        C: {type: float, low: 0.01, high: 1000.0, log: true}
        epsilon: {type: float, low: 0.001, high: 1.0, log: true}
        gamma: {type: float, low: 0.0001, high: 10.0, log: true}
        degree: {type: int, low: 2, high: 5}
        coef0: {type: float, low: 0.0, high: 1.0}
    dt:
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        max_depth: {type: int, low: 3, high: 30}
        min_samples_split: {type: int, low: 2, high: 20}
        min_samples_leaf: {type: int, low: 1, high: 20}
        max_features: {type: categorical, choices: ["sqrt", "log2", null]}
        min_impurity_decrease: {type: float, low: 0.0, high: 0.01}
    rf:
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        n_estimators: {type: int, low: 200, high: 1200}
        max_depth: {type: int, low: 4, high: 40}
        min_samples_split: {type: int, low: 2, high: 20}
        min_samples_leaf: {type: int, low: 1, high: 20}
        max_features: {type: categorical, choices: ["sqrt", "log2", null]}
        bootstrap: {type: categorical, choices: [true, false]}
    gbr:
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
        n_estimators: {type: int, low: 200, high: 1500}
        max_depth: {type: int, low: 2, high: 8}
        subsample: {type: float, low: 0.5, high: 1.0}
        min_samples_leaf: {type: int, low: 1, high: 20}
        max_features: {type: categorical, choices: ["sqrt", "log2", null]}
        min_samples_split: {type: int, low: 2, high: 20}
        min_impurity_decrease: {type: float, low: 0.0, high: 0.01}
    hgbt:
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
        max_depth: {type: int, low: 2, high: 16}
        max_leaf_nodes: {type: int, low: 7, high: 255}
        l2_regularization: {type: float, low: 0.000001, high: 10.0, log: true}
        max_bins: {type: int, low: 64, high: 255}
        min_samples_leaf: {type: int, low: 1, high: 50}
    xgboost:
      enabled: true
      profile: tree
      trials: *trials_conservative
      base_params:
        # Baseline conservativi
        n_estimators: 500         # Ridotto per compensare assenza early stopping
        max_depth: 6
        learning_rate: 0.05
        subsample: 0.8
        colsample_bytree: 0.8
        reg_alpha: 0.1
        reg_lambda: 0.1
        min_child_weight: 3
        gamma: 0.1
        # NOTA: early_stopping_rounds rimosso - richiede eval_set nel fit()
      fit_params: {}
      search_space:
        n_estimators: {type: int, low: 200, high: 800}           # era 300-1500
        max_depth: {type: int, low: 4, high: 8}                 # era 3-12
        learning_rate: {type: float, low: 0.01, high: 0.1, log: true} # era 0.001-0.3
        subsample: {type: float, low: 0.7, high: 0.9}           # era 0.5-1.0
        colsample_bytree: {type: float, low: 0.7, high: 0.9}    # era 0.5-1.0
        min_child_weight: {type: float, low: 1.0, high: 10.0}   # era 0.01-20.0
        reg_alpha: {type: float, low: 0.01, high: 1.0, log: true} # era 1e-8-10
        reg_lambda: {type: float, low: 0.01, high: 1.0, log: true} # era 1e-8-10
        gamma: {type: float, low: 0.01, high: 1.0, log: true}   # era 1e-8-10
    lightgbm:
      enabled: true
      profile: tree
      trials: *trials_conservative  # Ridotto per il miglior modello
      base_params: 
        # Baseline conservativi per evitare overfitting
        n_estimators: 500         # Ridotto per compensare assenza early stopping
        max_depth: 8
        learning_rate: 0.05
        num_leaves: 31
        subsample: 0.8
        colsample_bytree: 0.8
        reg_alpha: 0.1
        reg_lambda: 0.1
        min_child_samples: 20
        min_split_gain: 0.01
        verbose: -1
        # NOTA: early_stopping_rounds rimosso - richiede eval_set nel fit()
      fit_params: {}
      search_space:
        # Range molto più conservativi
        n_estimators: {type: int, low: 200, high: 800}           # era 300-1500
        max_depth: {type: categorical, choices: [6, 7, 8, 9, 10]} # era -1 a 12
        learning_rate: {type: float, low: 0.01, high: 0.1, log: true} # era 0.001-0.3
        num_leaves: {type: int, low: 15, high: 63}               # era 15-255
        subsample: {type: float, low: 0.7, high: 0.9}           # era 0.5-1.0
        colsample_bytree: {type: float, low: 0.7, high: 0.9}    # era 0.5-1.0
        reg_alpha: {type: float, low: 0.01, high: 1.0, log: true} # era 1e-8-10
        reg_lambda: {type: float, low: 0.01, high: 1.0, log: true} # era 1e-8-10
        min_child_samples: {type: int, low: 10, high: 50}       # era 5-100
        min_split_gain: {type: float, low: 0.001, high: 0.1}    # era 0.0-1.0
    catboost:
      enabled: true
      profile: catboost
      trials: *trials_conservative
      base_params:
        # Baseline conservativi
        iterations: 500           # Ridotto per compensare assenza early stopping
        depth: 6
        learning_rate: 0.05
        l2_leaf_reg: 3.0
        bagging_temperature: 1.0
        border_count: 128
        random_strength: 1.0
        rsm: 0.8
        verbose: False
        # NOTA: early_stopping_rounds rimosso - richiede eval_set nel fit()
      fit_params:
        cat_features: __categorical_indices__
      search_space:
        depth: {type: int, low: 4, high: 8}                     # era 4-10
        learning_rate: {type: float, low: 0.01, high: 0.1, log: true} # era 0.001-0.3
        l2_leaf_reg: {type: float, low: 1.0, high: 10.0}        # era 1.0-15.0
        bagging_temperature: {type: float, low: 0.5, high: 2.0} # era 0.0-5.0
        border_count: {type: int, low: 64, high: 200}           # era 16-255
        random_strength: {type: float, low: 0.5, high: 1.5}     # era 0.0-2.0
        rsm: {type: float, low: 0.7, high: 0.9}                # era 0.5-1.0

  # SHAP feature importance
  shap:
    enabled: true
    sample_size: 500
    max_display: 30
    save_plots: true
    save_values: true

  # Ensemble settings
  ensembles:
    # Ensemble diversificato con algoritmi eterogenei
    voting_diverse:
      enabled: true
      type: "voting"
      # Mix di algoritmi diversi invece di solo tree-based
      fixed_members: ["lightgbm", "ridge", "knn"]  # Tree + Linear + Instance-based
      tune_weights: true
      weights_search_space:
        lightgbm_weight: {type: float, low: 0.1, high: 0.8}
        ridge_weight: {type: float, low: 0.1, high: 0.6}
        knn_weight: {type: float, low: 0.1, high: 0.4}
    
    # Ensemble tradizionale con top performers
    voting_top:
      enabled: true
      type: "voting"
      top_n: 3              # Migliori 3 modelli
      tune_weights: true
      exclude_categories: []  # Non escludere nessuna categoria
    
    stacking_diverse:
      enabled: true
      type: "stacking"
      # Mix diversificato per stacking
      fixed_members: ["lightgbm", "xgboost", "ridge", "knn"]
      final_estimator: "xgboost"  # Meta-learner più sofisticato
      cv_folds: 10               # Più fold per stabilità
      passthrough: false         # Non passare feature originali
    
    stacking_top:
      enabled: true
      type: "stacking"
      top_n: 4                   # Top 4 modelli
      final_estimator: "ridge"
      cv_folds: 8
      passthrough: true          # Passa anche feature originali

# Evaluation configuration
evaluation:
  group_metrics:
    enabled: true
    group_by_columns: ['AI_ZonaOmi', 'AI_IdCategoriaCatastale', 'AI_IdTipologiaEdilizia']
    min_group_size: 30
    original_scale: true
    report_metrics: ['r2', 'rmse', 'mse', 'mae', 'mape', 'medae']
    price_band:
      method: 'quantile'   # 'quantile' | 'fixed'
      quantiles: [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
      # fixed_edges: [0, 100000, 200000, 400000, 800000, 1600000, .inf]
      mape_floor: 1000.0
      merge_small_bins: true
      label_prefix: 'PREZZO_'