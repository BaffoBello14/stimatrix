# Configurazione Preprocessing Pipeline
# =================================

# Logging Configuration
logging:
  level: INFO
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: 'logs/pipeline.log'

# Paths Configuration
paths:
  raw_data: 'data/raw'
  preprocessed_data: 'data/preprocessed'
  schema: 'schema/db_schema.json'
  models_dir: 'models'
  raw_filename: 'raw.parquet'
  preprocessed_filename: 'preprocessed.parquet'

# Database Configuration (for data retrieval)
database:
  schema_name: null
  selected_aliases: ['A', 'AI', 'PC', 'ISC', 'II', 'PC_OZ', 'OZ', 'OV']
  use_poi: true
  use_ztl: true
  output_format: 'parquet'
  compression: 'snappy'

# Target configuration
target:
  column_candidates: ['AI_Prezzo_Ridistribuito']
  log_transform: false

# Outlier detection
outliers:
  method: 'ensemble'       # 'iqr' | 'zscore' | 'iso_forest' | 'ensemble'
  z_thresh: 4.0
  iqr_factor: 1.5
  iso_forest_contamination: 0.02
  group_by_col: 'AI_IdCategoriaCatastale'
  min_group_size: 30       # minimum samples per group to apply group-wise outlier detection
  fallback_strategy: 'global'  # 'skip' keeps all small groups; 'global' applies global detection

# Missing value imputation
imputation:
  numeric_strategy: 'median'      # 'median' | 'mean'
  categorical_strategy: 'most_frequent'
  group_by_col: 'AI_IdCategoriaCatastale'

# Categorical encoding
encoding:
  max_ohe_cardinality: 12

# Temporal split configuration
temporal_split:
  year_col: 'A_AnnoStipula'
  month_col: 'A_MeseStipula'
  mode: 'fraction'      # 'date' | 'fraction'
  train_fraction: 0.8   # fraction of time-ordered data for train
  valid_fraction: 0.1   # fraction of time-ordered data for validation (0 disables val)
  test_start_year: 2023 # used only if mode=='date'
  test_start_month: 1   # used only if mode=='date'

# PCA configuration
pca:
  enabled: false
  n_components: 0.95
  random_state: 42

# Correlation pruning
correlation:
  numeric_threshold: 0.98

# Feature extraction toggles
feature_extraction:
  geometry: true
  json: true

# Scaling configuration
scaling:
  scaler_type: 'standard'   # 'standard' | 'robust' | 'none'
  with_mean: true
  with_std: true

# Winsorization (quantile clipping on numeric features)
winsorization:
  enabled: false
  lower_quantile: 0.01
  upper_quantile: 0.99

# Profiles: generate multiple datasets tailored to model families
profiles:
  scaled:
    enabled: true
    output_prefix: 'scaled'
    encoding:
      max_ohe_cardinality: 12
    winsorization:
      enabled: false
      lower_quantile: 0.01
      upper_quantile: 0.99
    scaling:
      scaler_type: 'standard'   # 'standard' | 'robust' | 'none'
      with_mean: true
      with_std: true
    pca:
      enabled: false
      n_components: 0.95
      random_state: 42
    correlation:
      numeric_threshold: 0.98
  tree:
    enabled: true
    output_prefix: 'tree'
    encoding:
      max_ohe_cardinality: 12
    correlation:
      numeric_threshold: 0.98
  catboost:
    enabled: true
    output_prefix: 'catboost'
    correlation:
      numeric_threshold: 0.98

# Training configuration
training:
  # Modelli da addestrare (flag booleani)
  models:
    linear: true
    ridge: true
    lasso: true
    elasticnet: true
    knn: true
    svr: true
    dt: true
    rf: true
    gbr: true
    hgbt: true
    xgboost: true
    lightgbm: true
    catboost: true

  # Profilo dataset per famiglia di modelli (se non presente, fallback ai file senza suffisso)
  profile_map:
    linear: "scaled"
    ridge: "scaled"
    lasso: "scaled"
    elasticnet: "scaled"
    knn: "scaled"
    svr: "scaled"
    dt: "tree"
    rf: "tree"
    gbr: "tree"
    hgbt: "tree"
    xgboost: "tree"
    lightgbm: "tree"
    catboost: "catboost"

  # Metrica primaria da massimizzare durante il tuning
  primary_metric: "r2"

  # Metrica per il report finale (sempre calcoliamo tutte queste positive); non influenza la direzione del tuning
  report_metrics: ["r2", "rmse", "mse", "mae", "mape"]

  # Optuna/OptunaHub
  optuna:
    n_trials: 50
    timeout: null
    sampler: "auto"
    seed: 42

  # Spazio di ricerca per iperparametri (per ciascun modello)
  search_spaces:
    ridge:
      alpha: {type: float, low: 0.001, high: 100.0, log: true}
    lasso:
      alpha: {type: float, low: 0.0001, high: 10.0, log: true}
    elasticnet:
      alpha: {type: float, low: 0.0001, high: 10.0, log: true}
      l1_ratio: {type: float, low: 0.0, high: 1.0}
    dt:
      max_depth: {type: int, low: 3, high: 30}
      min_samples_split: {type: int, low: 2, high: 20}
      min_samples_leaf: {type: int, low: 1, high: 20}
    knn:
      n_neighbors: {type: int, low: 3, high: 50}
      weights: {type: categorical, choices: ["uniform", "distance"]}
      p: {type: int, low: 1, high: 2}
    svr:
      kernel: {type: categorical, choices: ["rbf", "poly", "sigmoid"]}
      C: {type: float, low: 0.01, high: 1000.0, log: true}
      epsilon: {type: float, low: 0.001, high: 1.0, log: true}
      gamma: {type: categorical, choices: ["scale", "auto"]}
      degree: {type: int, low: 2, high: 5}
    rf:
      n_estimators: {type: int, low: 200, high: 1200}
      max_depth: {type: int, low: 4, high: 40}
      min_samples_split: {type: int, low: 2, high: 20}
      min_samples_leaf: {type: int, low: 1, high: 20}
      max_features: {type: categorical, choices: ["sqrt", "log2", null]}
    gbr:
      learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
      n_estimators: {type: int, low: 200, high: 1500}
      max_depth: {type: int, low: 2, high: 8}
      subsample: {type: float, low: 0.5, high: 1.0}
      min_samples_leaf: {type: int, low: 1, high: 20}
    hgbt:
      learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
      max_depth: {type: int, low: 2, high: 16}
      max_leaf_nodes: {type: int, low: 7, high: 255}
      l2_regularization: {type: float, low: 0.000001, high: 10.0, log: true}
    xgboost:
      n_estimators: {type: int, low: 300, high: 1500}
      max_depth: {type: int, low: 3, high: 12}
      learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
      subsample: {type: float, low: 0.5, high: 1.0}
      colsample_bytree: {type: float, low: 0.5, high: 1.0}
      min_child_weight: {type: float, low: 0.01, high: 20.0, log: true}
      reg_alpha: {type: float, low: 1e-8, high: 10.0, log: true}
      reg_lambda: {type: float, low: 1e-8, high: 10.0, log: true}
    lightgbm:
      n_estimators: {type: int, low: 300, high: 1500}
      max_depth: {type: int, low: -1, high: 12}
      learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
      num_leaves: {type: int, low: 15, high: 255}
      subsample: {type: float, low: 0.5, high: 1.0}
      colsample_bytree: {type: float, low: 0.5, high: 1.0}
      reg_alpha: {type: float, low: 1e-8, high: 10.0, log: true}
      reg_lambda: {type: float, low: 1e-8, high: 10.0, log: true}
    catboost:
      depth: {type: int, low: 4, high: 10}
      learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
      l2_leaf_reg: {type: float, low: 1.0, high: 15.0}
      bagging_temperature: {type: float, low: 0.0, high: 5.0}
      border_count: {type: int, low: 16, high: 255}

  # SHAP feature importance
  shap:
    enabled: true
    sample_size: 2000
    max_display: 30
    save_plots: true
    save_values: false

  # Ensemble settings
  ensembles:
    voting:
      enabled: true
      top_n: 3
      tune_weights: true
    stacking:
      enabled: true
      top_n: 5
      final_estimator: "ridge"
      cv_folds: 5