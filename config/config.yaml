# Configurazione Preprocessing Pipeline
# =================================

# Logging Configuration
logging:
  level: ${LOG_LEVEL:-INFO}
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: ${LOG_FILE:-'logs/pipeline.log'}
  console: ${LOG_CONSOLE:-true}
  rotate:
    enabled: ${LOG_ROTATE:-false}
    max_bytes: ${LOG_MAX_BYTES:-10485760}
    backup_count: ${LOG_BACKUP_COUNT:-5}

# Execution Configuration
execution:
  # Passi della pipeline da eseguire: [schema, dataset, preprocessing, training, evaluation] oppure ["all"]
  steps: ["all"]
  # Se true, forza la rielaborazione anche se gli output intermedi esistono già
  force_reload: true

# Experiment tracking (Weights & Biases)
tracking:
  wandb:
    enabled: ${WANDB_ENABLED:-true}
    project: ${WANDB_PROJECT:-'stimatrix'}
    entity: ${WANDB_ENTITY:-null}
    group: ${WANDB_GROUP:-null}
    tags: []
    mode: ${WANDB_MODE:-'online'}  # online | offline | disabled
    name: null

# Paths Configuration
paths:
  raw_data: ${RAW_DATA_DIR:-'data/raw'}
  preprocessed_data: ${PREPROC_DIR:-'data/preprocessed'}
  schema: ${SCHEMA_PATH:-'schema/db_schema.json'}
  models_dir: ${MODELS_DIR:-'models'}
  raw_filename: 'raw.parquet'
  preprocessed_filename: 'preprocessed.parquet'

# Database Configuration (for data retrieval)
database:
  schema_name: null
  selected_aliases: ['A', 'AI', 'PC', 'ISC', 'II', 'PC_OZ', 'OZ', 'OV', 'C1', 'C2']  # C1=attiimmobili_cened1, C2=attiimmobili_cened2
  # Custom alias mapping for specific tables/views (optional)
  # If not specified, defaults are used for known views (C1, C2)
  custom_aliases:
    attiimmobili_cened1: 'C1'
    attiimmobili_cened2: 'C2'
  use_poi: true
  use_ztl: true
  output_format: 'parquet'
  compression: 'snappy'

# Target configuration
target:
  # Choose among ['AI_Prezzo_Ridistribuito', 'AI_Prezzo_MQ']
  column_candidates: ['AI_Prezzo_Ridistribuito', 'AI_Prezzo_MQ']
  
  # Target transformation (applied to reduce skewness and improve model performance)
  # Options: 'none', 'log', 'log10', 'sqrt', 'boxcox', 'yeojohnson'
  transform: 'boxcox'  # Recommended: boxcox or yeojohnson for price data
  
  # Optional: offset for log10 transform (y -> log10(y + offset))
  log10_offset: 1.0
  
  # Transformation guide:
  # - 'none': No transformation
  # - 'log': log(1+y) - handles zeros, best for right-skewed data
  # - 'log10': log10(y + offset) - similar to log but different scale
  # - 'sqrt': sqrt(y) - milder than log, requires y >= 0
  # - 'boxcox': Automatically finds optimal lambda, requires y > 0 (auto-shifts if needed)
  # - 'yeojohnson': Like Box-Cox but works with any y (recommended for flexibility)

# Outlier detection
outliers:
  method: 'ensemble'       # 'iqr' | 'zscore' | 'iso_forest' | 'ensemble'
  z_thresh: 3.0
  iqr_factor: 1.5
  iso_forest_contamination: 0.02
  group_by_col: 'AI_IdTipologiaEdilizia'
  min_group_size: 30       # minimum samples per group to apply group-wise outlier detection
  fallback_strategy: 'global'  # 'skip' keeps all small groups; 'global' applies global detection

# Missing value imputation
imputation:
  numeric_strategy: 'median'      # 'median' | 'mean'
  categorical_strategy: 'most_frequent'
  group_by_col: 'AI_IdTipologiaEdilizia'

# Categorical encoding - Multi-strategy based on cardinality
encoding: &encoding_defaults
  # Strategy selection by number of unique values:
  one_hot_max: 10                     # ≤10 unique → One-Hot Encoding
  target_encoding_range: [11, 30]     # 11-30 → Target Encoding (with smoothing)
  frequency_encoding_range: [31, 100] # 31-100 → Frequency Encoding
  ordinal_encoding_range: [101, 200]  # 101-200 → Ordinal Encoding
  drop_above: 200                     # >200 unique → DROP (too high cardinality)
  
  # Target encoding configuration (requires y_train)
  target_encoder:
    smoothing: 1.0              # Bayesian smoothing for rare categories
    min_samples_leaf: 1         # Minimum samples per leaf for target encoding

# Temporal split configuration
temporal_split:
  year_col: 'A_AnnoStipula'
  month_col: 'A_MeseStipula'
  mode: 'fraction'      # 'date' | 'fraction'
  fraction:
    train: 0.7          # fraction of time-ordered data for train
    valid: 0.2          # fraction of time-ordered data for validation (0 disables val)
  date:
    test_start_year: 2023 # used only if mode=='date'
    test_start_month: 1   # used only if mode=='date'

# Numeric coercion configuration
numeric_coercion:
  enabled: true
  threshold: 0.95
  blacklist_globs:
    - 'II_*'
    - 'AI_Id*'
    - 'Foglio'
    - 'Particella*'
    - 'Subalterno'
    - 'SezioneAmministrativa'
    - 'ZonaOmi'
    - '*COD*'

# PCA configuration
pca: &pca_defaults
  enabled: false
  n_components: 0.95
  random_state: 42

# Correlation pruning
# NOTA: Threshold aumentato a 0.95 per evitare rimozione eccessiva di feature utili
# La correlazione viene calcolata solo sulle colonne numeriche, non sulle OHE
correlation: &correlation_defaults
  numeric_threshold: 0.95

# Non-descriptive columns pruning
# Threshold aumentato a 0.80 per essere meno aggressivi
drop_non_descriptive:
  na_threshold: 0.80

# Feature extraction toggles
feature_extraction:
  geometry: true
  json: true

# Feature pruning
feature_pruning:
  drop_columns:
    - 'A_ImmobiliPrincipaliConSuperficieValorizzata'
    - 'AI_SuperficieCalcolata'
    - 'AI_SuperficieVisuraTotale'
    - 'AI_SuperficieVisuraTotaleE'
    - 'AI_SuperficieVisuraTotaleAttuale'
    - 'AI_SuperficieVisuraTotaleEAttuale'
    - 'A_AcquirentiCount'
    - 'A_VenditoriCount'
    - 'A_EtaMediaAcquirenti'
    - 'A_EtaMediaVenditori'
    - 'A_AcquirentiVenditoriStessoCognome'
    - 'A_VenditoriEredita'
    - 'AI_Subalterno'
    - 'AI_Piano'
    - 'AI_Rendita'
  # Keep AI_Superficie as a feature (set to false to drop from X)
  include_ai_superficie: true

# Scaling configuration
scaling: &scaling_defaults
  scaler_type: 'standard'   # 'standard' | 'robust' | 'none'
  with_mean: true
  with_std: true

# Winsorization (quantile clipping on numeric features)
winsorization: &winsor_defaults
  enabled: true
  lower_quantile: 0.01
  upper_quantile: 0.99

# Profiles: generate multiple datasets tailored to model families
# SEMPLIFICATO: Solo 2 profili necessari per modelli tree-based
profiles:
  scaled:
    # Disabilitato: non più necessario dopo rimozione modelli lineari
    enabled: false
    output_prefix: 'scaled'
    encoding: *encoding_defaults
    winsorization: *winsor_defaults
    scaling: *scaling_defaults
    pca: *pca_defaults
    correlation: *correlation_defaults
  tree:
    # Profilo principale per XGBoost, LightGBM, Random Forest, GBR, HGBT
    enabled: true
    output_prefix: 'tree'
    encoding: *encoding_defaults
    correlation: *correlation_defaults
  catboost:
    # Profilo specifico per CatBoost (mantiene categoriche native)
    enabled: true
    output_prefix: 'catboost'
    correlation: *correlation_defaults

# Training configuration
training:
  # Opzioni globali
  primary_metric: "neg_root_mean_squared_error" # 'r2' | 'neg_root_mean_squared_error' | 'neg_mean_squared_error' | 'neg_mean_absolute_error' | 'neg_mean_absolute_percentage_error'
  report_metrics: ["r2", "rmse", "mse", "mae", "mape"]
  sampler: "auto"   # auto (OptunaHub) | tpe
  seed: ${SEED:-42}
  n_jobs_default: ${N_JOBS:--1}
  timeout: ${TRAIN_TIMEOUT:-null}
  trials_base: &trials_base 50
  trials_advanced: &trials_advanced 100
  # Cross-validation usata quando non esiste un validation set esterno (valid_fraction=0)
  cv_when_no_val:
    enabled: true
    kind: kfold   # 'timeseries' | 'kfold'
    n_splits: 5
    shuffle: true     # usato solo per kfold

  # Definizione per-modello (schema minimale)
  # SEMPLIFICATO: Solo modelli tree-based, i migliori per dati immobiliari
  models:
    # ========================================
    # MODELLI LINEARI: DISABILITATI
    # ========================================
    linear:
      enabled: false
      profile: scaled
      trials: 1
      base_params: {}
      fit_params: {}
      search_space: {}
    ridge:
      enabled: false
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        alpha: {type: float, low: 0.000001, high: 1000.0, log: true}
    lasso:
      enabled: false
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        alpha: {type: float, low: 0.0001, high: 10.0, log: true}
    elasticnet:
      enabled: false
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        alpha: {type: float, low: 0.0001, high: 10.0, log: true}
        l1_ratio: {type: float, low: 0.0, high: 1.0}
    # ========================================
    # KNN: DISABILITATO
    # Lento e poco performante con molte feature
    # ========================================
    knn:
      enabled: false
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        n_neighbors: {type: int, low: 3, high: 50}
        weights: {type: categorical, choices: ["uniform", "distance"]}
        p: {type: int, low: 1, high: 2}
    # ========================================
    # SVR: DISABILITATO
    # Lento su dataset grandi, poco performante
    # ========================================
    svr:
      enabled: false
      profile: scaled
      trials: *trials_base
      base_params: {}
      fit_params: {}
      search_space:
        kernel: {type: categorical, choices: ["rbf", "poly", "sigmoid"]}
        C: {type: float, low: 0.01, high: 1000.0, log: true}
        epsilon: {type: float, low: 0.001, high: 1.0, log: true}
        gamma: {type: float, low: 0.0001, high: 10.0, log: true}
        degree: {type: int, low: 2, high: 5}
        coef0: {type: float, low: 0.0, high: 1.0}
    # ========================================
    # DECISION TREE SINGOLO: DISABILITATO
    # ========================================
    dt:
      enabled: false
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        max_depth: {type: int, low: 3, high: 30}
        min_samples_split: {type: int, low: 2, high: 20}
        min_samples_leaf: {type: int, low: 1, high: 20}
        max_features: {type: categorical, choices: ["sqrt", "log2", null]}
        min_impurity_decrease: {type: float, low: 0.0, high: 0.01}
    # ========================================
    # MODELLI TREE-BASED: ABILITATI ✅
    # I migliori per dati immobiliari complessi
    # ========================================
    rf:
      # Random Forest: robusto, veloce, ottimo baseline
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        n_estimators: {type: int, low: 200, high: 1200}
        max_depth: {type: int, low: 4, high: 40}
        min_samples_split: {type: int, low: 2, high: 20}
        min_samples_leaf: {type: int, low: 1, high: 20}
        max_features: {type: categorical, choices: ["sqrt", "log2", null]}
        bootstrap: {type: categorical, choices: [true, false]}
    gbr:
      # Gradient Boosting (sklearn): buone performance ma più lento
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
        n_estimators: {type: int, low: 200, high: 1500}
        max_depth: {type: int, low: 2, high: 8}
        subsample: {type: float, low: 0.5, high: 1.0}
        min_samples_leaf: {type: int, low: 1, high: 20}
        max_features: {type: categorical, choices: ["sqrt", "log2", null]}
        min_samples_split: {type: int, low: 2, high: 20}
        min_impurity_decrease: {type: float, low: 0.0, high: 0.01}
    hgbt:
      # HistGradientBoosting: veloce e performante, supporta NaN nativamente
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
        max_depth: {type: int, low: 2, high: 16}
        max_leaf_nodes: {type: int, low: 7, high: 255}
        l2_regularization: {type: float, low: 0.000001, high: 10.0, log: true}
        max_bins: {type: int, low: 64, high: 255}
        min_samples_leaf: {type: int, low: 1, high: 50}
    xgboost:
      # XGBoost: solitamente il migliore per problemi tabular
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        n_estimators: {type: int, low: 300, high: 1500}
        max_depth: {type: int, low: 3, high: 8}
        learning_rate: {type: float, low: 0.01, high: 0.1, log: true}
        subsample: {type: float, low: 0.6, high: 0.9}
        colsample_bytree: {type: float, low: 0.6, high: 0.9}
        min_child_weight: {type: float, low: 1.0, high: 10.0, log: true}
        reg_alpha: {type: float, low: 1e-4, high: 10.0, log: true}
        reg_lambda: {type: float, low: 1e-4, high: 10.0, log: true}
        gamma: {type: float, low: 1e-4, high: 10.0, log: true}
    lightgbm:
      # LightGBM: molto veloce, ottimo per dataset grandi
      enabled: true
      profile: tree
      trials: *trials_advanced
      base_params: {}
      fit_params: {}
      search_space:
        n_estimators: {type: int, low: 300, high: 1500}
        max_depth: {type: categorical, choices: [-1, 4, 5, 6, 7, 8]}
        learning_rate: {type: float, low: 0.01, high: 0.1, log: true}
        num_leaves: {type: int, low: 16, high: 63}
        subsample: {type: float, low: 0.6, high: 0.9}
        colsample_bytree: {type: float, low: 0.6, high: 0.9}
        reg_alpha: {type: float, low: 1e-4, high: 10.0, log: true}
        reg_lambda: {type: float, low: 1e-4, high: 10.0, log: true}
        min_child_samples: {type: int, low: 20, high: 200}
        min_split_gain: {type: float, low: 0.0, high: 1.0}
    catboost:
      # CatBoost: ottima gestione categoriche, no preprocessing necessario
      enabled: true
      profile: catboost
      trials: *trials_advanced
      base_params:
        allow_writing_files: false  # Disabilita creazione cartella catboost_info
      fit_params:
        cat_features: __categorical_indices__
      search_space:
        iterations: {type: int, low: 500, high: 1500}  # Numero di alberi/iterazioni
        depth: {type: int, low: 4, high: 10}
        learning_rate: {type: float, low: 0.001, high: 0.3, log: true}
        l2_leaf_reg: {type: float, low: 1.0, high: 15.0}
        bagging_temperature: {type: float, low: 0.0, high: 5.0}
        border_count: {type: int, low: 16, high: 255}
        random_strength: {type: float, low: 0.0, high: 2.0}
        rsm: {type: float, low: 0.5, high: 1.0}

  # SHAP feature importance
  shap:
    enabled: true
    sample_size: 500
    max_display: 50
    save_plots: true
    save_values: true

  # Ensemble settings
  ensembles:
    voting:
      enabled: true
      top_n: 3
      tune_weights: true
    stacking:
      enabled: true
      top_n: 5
      # Meta-learner: Ridge è ideale (semplice, previene overfitting, interpretabile)
      final_estimator: "ridge"
      cv_folds: 5

# Evaluation configuration
evaluation:
  group_metrics:
    enabled: true
    group_by_columns: ['AI_ZonaOmi', 'AI_IdCategoriaCatastale', 'AI_IdTipologiaEdilizia']
    min_group_size: 10
    original_scale: true
    report_metrics: ['r2', 'rmse', 'mse', 'mae', 'mape', 'medae']
    price_band:
      method: 'quantile'   # 'quantile' | 'fixed'
      quantiles: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
      # fixed_edges: [0, 100000, 200000, 400000, 800000, 1600000, .inf]
      mape_floor: 1000.0
      merge_small_bins: true
      label_prefix: 'PREZZO_'