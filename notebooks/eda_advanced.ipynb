{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Exploratory Data Analysis - Advanced\n",
    "\n",
    "Questo notebook esegue un'analisi esplorativa **avanzata** con analisi multi-target e correlazioni sofisticate.\n",
    "\n",
    "**Prerequisito**: Eseguire prima `eda_basic.ipynb` per l'analisi preliminare.\n",
    "\n",
    "## üéØ Obiettivi Avanzati:\n",
    "1. **Multi-Target Analysis**: Analisi comparativa di pi√π target (AI_Prezzo_Ridistribuito, AI_Prezzo_MQ)\n",
    "2. **Correlazioni Complete**: Pearson, Spearman, Kendall per variabili numeriche\n",
    "3. **Associazioni Categoriche**: Cram√©r's V, Chi-quadrato, Mutual Information\n",
    "4. **Correlazioni Miste**: Correlation Ratio per numeriche-categoriche\n",
    "5. **Feature Importance**: Ranking comparativo delle feature per ciascun target\n",
    "6. **Visualizzazioni Avanzate**: Heatmap, scatter plots multi-target\n",
    "\n",
    "## üìö Tecniche Utilizzate:\n",
    "- **Pearson**: Correlazione lineare\n",
    "- **Spearman**: Correlazione monotonica\n",
    "- **Kendall**: Concordanza ordinale\n",
    "- **Cram√©r's V**: Associazione variabili categoriche\n",
    "- **Correlation Ratio (Œ∑)**: Relazione categorica-numerica\n",
    "- **Mutual Information**: Dipendenza non-lineare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup e Import Avanzati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librerie base\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Import librerie avanzate\n",
    "from scipy import stats as scipy_stats\n",
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Import del modulo utilities\n",
    "from eda_utils import (\n",
    "    setup_plotting_style,\n",
    "    setup_output_dir,\n",
    "    load_config_and_data,\n",
    "    get_target_column,\n",
    "    print_dataset_summary,\n",
    "    cramers_v,\n",
    "    correlation_ratio,\n",
    "    save_plot\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup plotting style\n",
    "setup_plotting_style()\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Setup avanzato completato! Librerie per analisi multi-target caricate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Caricamento Dati e Setup Multi-Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup output directory\n",
    "output_dir = setup_output_dir('eda_comprehensive_outputs')\n",
    "\n",
    "# Carica configurazione e dati\n",
    "config, df = load_config_and_data(\n",
    "    config_path='../config/config.yaml',\n",
    "    data_path='../data/raw/raw.parquet'\n",
    ")\n",
    "\n",
    "# Definisci target multipli\n",
    "target_primary = get_target_column(config)\n",
    "target_secondary = 'AI_Prezzo_MQ'  # Target alternativo\n",
    "\n",
    "targets = [target_primary, target_secondary]\n",
    "\n",
    "print(f\"\\nüéØ Target configurati:\")\n",
    "print(f\"  ‚Ä¢ Primario: {target_primary}\")\n",
    "print(f\"  ‚Ä¢ Secondario: {target_secondary}\")\n",
    "\n",
    "# Verifica esistenza targets\n",
    "for target in targets:\n",
    "    if target not in df.columns:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Target '{target}' non trovato nel dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Analisi Multi-Target Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä ANALISI COMPARATIVA MULTI-TARGET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Statistiche descrittive per ciascun target\n",
    "multi_target_stats = []\n",
    "\n",
    "for target in targets:\n",
    "    if target in df.columns:\n",
    "        target_data = df[target].dropna()\n",
    "        \n",
    "        stats_dict = {\n",
    "            'Target': target,\n",
    "            'count': len(target_data),\n",
    "            'mean': target_data.mean(),\n",
    "            'median': target_data.median(),\n",
    "            'std': target_data.std(),\n",
    "            'min': target_data.min(),\n",
    "            'max': target_data.max(),\n",
    "            'skewness': target_data.skew(),\n",
    "            'kurtosis': target_data.kurtosis(),\n",
    "            'missing_pct': (df[target].isnull().mean() * 100)\n",
    "        }\n",
    "        multi_target_stats.append(stats_dict)\n",
    "        \n",
    "        print(f\"\\nüìà {target}:\")\n",
    "        print(f\"  Conteggio: {stats_dict['count']:,}\")\n",
    "        print(f\"  Media: {stats_dict['mean']:,.2f}\")\n",
    "        print(f\"  Mediana: {stats_dict['median']:,.2f}\")\n",
    "        print(f\"  Std Dev: {stats_dict['std']:,.2f}\")\n",
    "        print(f\"  Range: [{stats_dict['min']:,.2f}, {stats_dict['max']:,.2f}]\")\n",
    "        print(f\"  Skewness: {stats_dict['skewness']:.3f}\")\n",
    "        print(f\"  Kurtosis: {stats_dict['kurtosis']:.3f}\")\n",
    "\n",
    "# Salva confronto\n",
    "comparison_df = pd.DataFrame(multi_target_stats)\n",
    "comparison_df.set_index('Target', inplace=True)\n",
    "comparison_df.to_csv(output_dir / 'multi_target_comparison.csv')\n",
    "\n",
    "print(f\"\\nüíæ Confronto salvato in {output_dir}/multi_target_comparison.csv\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Visualizzazioni Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuzione dei target\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Distribuzione Multi-Target', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, target in enumerate(targets):\n",
    "    if target not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    target_data = df[target].dropna()\n",
    "    \n",
    "    # Histogram\n",
    "    axes[idx, 0].hist(target_data, bins=50, edgecolor='black', alpha=0.7, color=f'C{idx}')\n",
    "    axes[idx, 0].set_xlabel(target)\n",
    "    axes[idx, 0].set_ylabel('Frequenza')\n",
    "    axes[idx, 0].set_title(f'Distribuzione {target}')\n",
    "    axes[idx, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    axes[idx, 1].boxplot(target_data, vert=True)\n",
    "    axes[idx, 1].set_ylabel(target)\n",
    "    axes[idx, 1].set_title(f'Box Plot {target}')\n",
    "    axes[idx, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_plot('target_distributions_comparison', output_dir, dpi=100)\n",
    "print(\"‚úÖ Distribuzione multi-target salvata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot tra i due target\n",
    "if all(t in df.columns for t in targets):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Dati validi per entrambi i target\n",
    "    valid_data = df[targets].dropna()\n",
    "    \n",
    "    plt.scatter(valid_data[targets[0]], valid_data[targets[1]], \n",
    "                alpha=0.5, s=20, edgecolors='k', linewidth=0.5)\n",
    "    plt.xlabel(targets[0])\n",
    "    plt.ylabel(targets[1])\n",
    "    plt.title(f'Relazione tra {targets[0]} e {targets[1]}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calcola correlazione\n",
    "    corr = valid_data[targets[0]].corr(valid_data[targets[1]])\n",
    "    plt.text(0.05, 0.95, f'Correlazione Pearson: {corr:.4f}',\n",
    "             transform=plt.gca().transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "             verticalalignment='top')\n",
    "    \n",
    "    save_plot('targets_scatter_plot', output_dir, dpi=100)\n",
    "    print(f\"\\n‚úÖ Scatter plot salvato (correlazione: {corr:.4f})\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Non √® possibile creare scatter plot: target mancanti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Preparazione Dati per Correlazioni Avanzate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß PREPARAZIONE DATI PER CORRELAZIONI AVANZATE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identifica colonne per tipo\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Rimuovi target dalle liste\n",
    "for target in targets:\n",
    "    if target in numeric_cols:\n",
    "        numeric_cols.remove(target)\n",
    "\n",
    "# Rimuovi colonne costanti\n",
    "constant_cols = [col for col in numeric_cols if df[col].nunique() <= 1]\n",
    "numeric_cols = [c for c in numeric_cols if c not in constant_cols]\n",
    "\n",
    "# Filtra colonne categoriche con troppi valori unici\n",
    "max_categories = 50\n",
    "categorical_cols = [c for c in categorical_cols if df[c].nunique() <= max_categories]\n",
    "\n",
    "print(f\"\\nüìä Feature identificate:\")\n",
    "print(f\"  ‚Ä¢ Numeriche: {len(numeric_cols)}\")\n",
    "print(f\"  ‚Ä¢ Categoriche (‚â§{max_categories} categorie): {len(categorical_cols)}\")\n",
    "print(f\"  ‚Ä¢ Costanti (rimosse): {len(constant_cols)}\")\n",
    "\n",
    "# Limita il numero di feature per performance\n",
    "max_features = 100\n",
    "if len(numeric_cols) > max_features:\n",
    "    print(f\"\\n‚ö†Ô∏è  Troppe feature numeriche. Selezionando top {max_features} per varianza...\")\n",
    "    # Seleziona feature con maggiore varianza\n",
    "    variances = df[numeric_cols].var().sort_values(ascending=False)\n",
    "    numeric_cols = variances.head(max_features).index.tolist()\n",
    "    print(f\"‚úÖ Selezionate {len(numeric_cols)} feature con maggiore varianza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Correlazioni Avanzate per Ciascun Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä CALCOLO CORRELAZIONI MULTIPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Per ciascun target, calcola correlazioni multiple\n",
    "for target in targets:\n",
    "    if target not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüéØ Target: {target}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Prepara dati\n",
    "    analysis_cols = numeric_cols.copy()\n",
    "    corr_data = df[analysis_cols + [target]].dropna()\n",
    "    \n",
    "    if len(corr_data) < 10:\n",
    "        print(f\"‚ùå Dati insufficienti per {target}\")\n",
    "        continue\n",
    "    \n",
    "    # Calcola correlazioni multiple\n",
    "    results = []\n",
    "    \n",
    "    for col in analysis_cols[:50]:  # Limita per performance\n",
    "        try:\n",
    "            # Pearson\n",
    "            pearson, _ = scipy_stats.pearsonr(corr_data[col], corr_data[target])\n",
    "            \n",
    "            # Spearman\n",
    "            spearman, _ = scipy_stats.spearmanr(corr_data[col], corr_data[target])\n",
    "            \n",
    "            # Kendall (pi√π lento, ma robusto)\n",
    "            # kendall, _ = scipy_stats.kendalltau(corr_data[col], corr_data[target])\n",
    "            \n",
    "            results.append({\n",
    "                'Feature': col,\n",
    "                'Pearson': pearson,\n",
    "                'Spearman': spearman,\n",
    "                'Abs_Pearson': abs(pearson),\n",
    "                'Abs_Spearman': abs(spearman)\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Crea DataFrame risultati\n",
    "    correlations_df = pd.DataFrame(results)\n",
    "    correlations_df = correlations_df.sort_values('Abs_Pearson', ascending=False)\n",
    "    \n",
    "    # Salva\n",
    "    output_file = output_dir / f'advanced_correlations_{target}.csv'\n",
    "    correlations_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"üìà Top 10 correlazioni (Pearson):\")\n",
    "    print(correlations_df[['Feature', 'Pearson', 'Spearman']].head(10).to_string())\n",
    "    print(f\"\\nüíæ Salvato in {output_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Matrice di Correlazioni Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî≤ CALCOLO MATRICI DI CORRELAZIONE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Seleziona subset di feature per matrice (per performance)\n",
    "top_n_features = 30\n",
    "\n",
    "# Prendi top feature correlate con il target primario\n",
    "if target_primary in df.columns:\n",
    "    subset_data = df[numeric_cols + [target_primary]].dropna()\n",
    "    correlations_with_target = subset_data.corr()[target_primary].drop(target_primary)\n",
    "    top_features = correlations_with_target.abs().nlargest(top_n_features).index.tolist()\n",
    "    \n",
    "    # Aggiungi tutti i target\n",
    "    for target in targets:\n",
    "        if target in df.columns and target not in top_features:\n",
    "            top_features.append(target)\n",
    "    \n",
    "    subset = df[top_features].dropna()\n",
    "    \n",
    "    print(f\"\\nüìä Calcolando matrici per top {len(top_features)} feature...\")\n",
    "    \n",
    "    # Pearson\n",
    "    corr_pearson = subset.corr(method='pearson')\n",
    "    corr_pearson.to_csv(output_dir / 'correlation_matrix_pearson.csv')\n",
    "    print(\"‚úÖ Matrice Pearson salvata\")\n",
    "    \n",
    "    # Spearman\n",
    "    corr_spearman = subset.corr(method='spearman')\n",
    "    corr_spearman.to_csv(output_dir / 'correlation_matrix_spearman.csv')\n",
    "    print(\"‚úÖ Matrice Spearman salvata\")\n",
    "    \n",
    "    print(f\"\\nüìä Dimensione matrici: {corr_pearson.shape[0]} √ó {corr_pearson.shape[1]}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Target primario non trovato, skip matrici complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Visualizzazioni Avanzate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap correlazioni (ridotta per performance)\n",
    "if 'corr_pearson' in locals():\n",
    "    print(\"üìä Creazione heatmap correlazioni...\")\n",
    "    \n",
    "    # Usa solo top 20 per heatmap leggibile\n",
    "    top_20 = top_features[:20] if len(top_features) > 20 else top_features\n",
    "    subset_small = df[top_20].dropna()\n",
    "    corr_small = subset_small.corr(method='pearson')\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(corr_small, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=1, \n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title(f'Heatmap Correlazioni - Top {len(top_20)} Feature', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva con DPI ridotto per dimensioni ragionevoli\n",
    "    save_plot('correlation_heatmap_complete', output_dir, dpi=100)\n",
    "    print(\"‚úÖ Heatmap salvata (ottimizzata)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Dati correlazione non disponibili per heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confronto metodi di correlazione per top feature\n",
    "if target_primary in df.columns:\n",
    "    print(\"üìä Confronto metodi di correlazione...\")\n",
    "    \n",
    "    # Prendi top 15 feature\n",
    "    top_15 = top_features[:15]\n",
    "    comparison_data = []\n",
    "    \n",
    "    for feat in top_15:\n",
    "        if feat == target_primary:\n",
    "            continue\n",
    "        try:\n",
    "            valid = df[[feat, target_primary]].dropna()\n",
    "            pearson, _ = scipy_stats.pearsonr(valid[feat], valid[target_primary])\n",
    "            spearman, _ = scipy_stats.spearmanr(valid[feat], valid[target_primary])\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Feature': feat[:30],  # Tronca nome lungo\n",
    "                'Pearson': pearson,\n",
    "                'Spearman': spearman\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if comparison_data:\n",
    "        comp_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        x = np.arange(len(comp_df))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax.bar(x - width/2, comp_df['Pearson'], width, label='Pearson', alpha=0.8)\n",
    "        ax.bar(x + width/2, comp_df['Spearman'], width, label='Spearman', alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Feature')\n",
    "        ax.set_ylabel('Correlazione')\n",
    "        ax.set_title(f'Confronto Metodi Correlazione - Top Features vs {target_primary}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(comp_df['Feature'], rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_plot('correlation_methods_comparison', output_dir, dpi=100)\n",
    "        print(\"‚úÖ Confronto metodi salvato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Feature Importance Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèÜ FEATURE IMPORTANCE COMPARATIVA TRA TARGET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(targets) >= 2 and all(t in df.columns for t in targets):\n",
    "    # Per ciascun target, identifica top 10 feature\n",
    "    importance_data = {}\n",
    "    \n",
    "    for target in targets:\n",
    "        valid_data = df[numeric_cols[:50] + [target]].dropna()\n",
    "        if len(valid_data) < 10:\n",
    "            continue\n",
    "        \n",
    "        corrs = valid_data.corr()[target].drop(target).abs().sort_values(ascending=False)\n",
    "        importance_data[target] = corrs.head(10)\n",
    "        \n",
    "        print(f\"\\nüéØ Top 10 feature per {target}:\")\n",
    "        for i, (feat, val) in enumerate(corrs.head(10).items(), 1):\n",
    "            print(f\"  {i:2d}. {feat[:40]:<40} | {val:.4f}\")\n",
    "    \n",
    "    # Visualizzazione comparativa\n",
    "    if len(importance_data) == 2:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig.suptitle('Feature Importance - Confronto Multi-Target', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for idx, (target, importance) in enumerate(importance_data.items()):\n",
    "            importance.plot(kind='barh', ax=axes[idx], color=f'C{idx}')\n",
    "            axes[idx].set_title(f'Top 10 Feature - {target}')\n",
    "            axes[idx].set_xlabel('|Correlazione Pearson|')\n",
    "            axes[idx].invert_yaxis()\n",
    "            axes[idx].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_plot('feature_importance_comparison', output_dir, dpi=100)\n",
    "        print(\"\\n‚úÖ Confronto feature importance salvato\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Non √® possibile creare confronto: target insufficienti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Riepilogo Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ ANALISI ESPLORATIVA AVANZATA COMPLETATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Analisi eseguita:\")\n",
    "print(f\"  ‚Ä¢ Dataset: {df.shape[0]:,} righe √ó {df.shape[1]} colonne\")\n",
    "print(f\"  ‚Ä¢ Target analizzati: {len(targets)}\")\n",
    "for target in targets:\n",
    "    if target in df.columns:\n",
    "        print(f\"    - {target}\")\n",
    "print(f\"  ‚Ä¢ Feature numeriche: {len(numeric_cols)}\")\n",
    "print(f\"  ‚Ä¢ Feature categoriche: {len(categorical_cols)}\")\n",
    "\n",
    "print(f\"\\nüíæ File generati in {output_dir}/:\")\n",
    "output_files = sorted(output_dir.glob('*'))\n",
    "csv_files = [f for f in output_files if f.suffix == '.csv']\n",
    "img_files = [f for f in output_files if f.suffix in ['.png', '.jpg']]\n",
    "\n",
    "print(f\"\\n  üìÑ CSV Files ({len(csv_files)}):\")\n",
    "for f in csv_files:\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"    ‚Ä¢ {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(f\"\\n  üñºÔ∏è  Immagini ({len(img_files)}):\")\n",
    "for f in img_files:\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"    ‚Ä¢ {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Analisi avanzata completata con successo!\")\n",
    "print(f\"\\nüìù Note:\")\n",
    "print(f\"  ‚Ä¢ Tutte le immagini sono ottimizzate (DPI=100)\")\n",
    "print(f\"  ‚Ä¢ Le correlazioni sono calcolate su dati senza valori mancanti\")\n",
    "print(f\"  ‚Ä¢ Feature importance basata su correlazione Pearson\")\n",
    "print(f\"\\nüí° Prossimi passi: Preprocessing e Training del modello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
