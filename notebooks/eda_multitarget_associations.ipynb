# Riepilogo
print("="*60)
print("üì¶ File generati (csv):")
for f in output_dir.rglob('*.csv'):
    print(" -", f.relative_to(output_dir).as_posix())
print("‚úÖ EDA esteso completato")# Associazioni feature-feature
# Numeriche: Pearson/Spearman + coppie ad alta correlazione
num_cols = df_raw.select_dtypes(include=[np.number]).columns.tolist()
num_cols = [c for c in num_cols if df_raw[c].nunique(dropna=True) > 1]
if len(num_cols) >= 2:
    corr_pearson = df_raw[num_cols].corr(method='pearson')
    corr_spearman = df_raw[num_cols].corr(method='spearman')
    pairs = []
    for i, c1 in enumerate(num_cols):
        for c2 in num_cols[i+1:]:
            r = corr_pearson.loc[c1, c2]
            if pd.notna(r) and abs(r) >= high_corr_threshold:
                pairs.append((c1, c2, float(r)))
    pairs_sorted = sorted(pairs, key=lambda x: -abs(x[2]))[:top_k_pairs]
    pd.DataFrame(pairs_sorted, columns=['col1','col2','pearson_r']).to_csv(
        associations_dir / f'numeric_numeric_highcorr_{int(high_corr_threshold*100)}.csv', index=False
    )
    corr_pearson.to_csv(associations_dir / 'numeric_numeric_pearson.csv')
    corr_spearman.to_csv(associations_dir / 'numeric_numeric_spearman.csv')

# Categoriche-categoriche: Cram√©r's V, Theil's U
cat_cols = df_raw.select_dtypes(include=['object','category','bool']).columns.tolist()
for c in cat_cols:
    if df_raw[c].dtype == 'bool':
        df_raw[c] = df_raw[c].astype('category')
cat_cols = [c for c in cat_cols if 1 < df_raw[c].nunique(dropna=True) <= max_cat_cardinality]
cramers_records, theils_records = [], []
for i, c1 in enumerate(cat_cols):
    for c2 in cat_cols[i+1:]:
        cramers_records.append((c1, c2, cramers_v_corrected(df_raw[c1], df_raw[c2])))
        theils_records.append((c1, c2, theils_u(df_raw[c1], df_raw[c2])))
if cramers_records:
    pd.DataFrame(cramers_records, columns=['col1','col2','cramers_v']).sort_values('cramers_v', ascending=False).head(top_k_pairs).to_csv(
        associations_dir / 'categorical_categorical_top_cramersV.csv', index=False
    )
if theils_records:
    pd.DataFrame(theils_records, columns=['col1','col2','theils_u']).sort_values('theils_u', ascending=False).head(top_k_pairs).to_csv(
        associations_dir / 'categorical_categorical_top_theilsU.csv', index=False
    )

# Miste (num-cat): eta, NMI
records_eta, records_nmi = [], []
for num in num_cols:
    for cat in cat_cols:
        eta = correlation_ratio(df_raw[cat], df_raw[num])
        dnum = discretize_numeric_series(df_raw[num])
        nmi = normalized_mutual_info_score(dnum, df_raw[cat].astype('category'))
        records_eta.append((cat, num, eta))
        records_nmi.append((cat, num, nmi))
if records_eta:
    pd.DataFrame(records_eta, columns=['cat','num','eta']).sort_values('eta', ascending=False).head(top_k_pairs).to_csv(
        associations_dir / 'mixed_cat_num_top_eta.csv', index=False
    )
if records_nmi:
    pd.DataFrame(records_nmi, columns=['cat','num','nmi']).sort_values('nmi', ascending=False).head(top_k_pairs).to_csv(
        associations_dir / 'mixed_cat_num_top_nmi.csv', index=False
    )# Correlazioni con i target (Pearson su numeriche)
for tgt in targets_presenti:
    print("="*60)
    print(f"üìä Correlazioni numeriche con {tgt}")
    num_cols = df_raw.select_dtypes(include=[np.number]).columns.tolist()
    if tgt in num_cols:
        num_cols.remove(tgt)
    num_cols = [c for c in num_cols if df_raw[c].nunique(dropna=True) > 1]
    data = df_raw[num_cols + [tgt]].dropna()
    if data.empty:
        print("Nessun dato per correlazioni.")
        continue
    corr = data.corr(numeric_only=True)[tgt].drop(tgt, errors='ignore')
    corr_df = (
        pd.DataFrame({'Colonna': corr.index, 'Correlazione': corr.values, 'Assoluta': np.abs(corr.values)})
        .sort_values('Assoluta', ascending=False)
    )
    display(corr_df.head(20))
    out_csv = targets_dir / f'correlations_with_target__{tgt}.csv'
    corr_df.to_csv(out_csv, index=False)
    print(f"üíæ {out_csv}")# Analisi per-target
for tgt in targets_presenti:
    print("="*60)
    print(f"üéØ Analisi target: {tgt}")
    s = pd.to_numeric(df_raw[tgt], errors='coerce').dropna()
    if s.empty:
        print("Target vuoto, skip")
        continue
    stats = s.describe()
    print(stats)
    fig, axes = plt.subplots(1,2, figsize=(16,6))
    axes[0].hist(s, bins=50, color='skyblue', edgecolor='black'); axes[0].set_title(f'Istogramma {tgt}'); axes[0].grid(True, alpha=0.3)
    axes[1].boxplot(s, vert=False); axes[1].set_title(f'Boxplot {tgt}'); axes[1].grid(True, alpha=0.3)
    plt.tight_layout(); plt.show()
    s_log = np.log1p(s.clip(lower=1e-6))
    scipy_stats.probplot(s_log.sample(min(len(s_log), 5000), random_state=42), dist='norm', plot=plt)
    plt.title(f'Q-Q plot log1p({tgt})'); plt.grid(True, alpha=0.3); plt.show()
    out_csv = targets_dir / f'target_statistics__{tgt}.csv'
    pd.DataFrame(stats).rename(columns={tgt:'value'}).to_csv(out_csv)
    print(f"üíæ {out_csv}")# Utility funzioni e parametri
from itertools import combinations
from math import sqrt
from sklearn.metrics import normalized_mutual_info_score

high_corr_threshold = 0.90
max_cat_cardinality = 50
top_k_pairs = 30

print(f"Soglie: high_corr_threshold={high_corr_threshold}, max_cat_cardinality={max_cat_cardinality}, top_k_pairs={top_k_pairs}")

def _safe_crosstab(x, y):
    x = pd.Series(x).astype('category')
    y = pd.Series(y).astype('category')
    return pd.crosstab(x, y)


def cramers_v_corrected(x, y):
    confusion_matrix = _safe_crosstab(x, y).values
    if confusion_matrix.size == 0:
        return np.nan
    chi2 = scipy_stats.chi2_contingency(confusion_matrix, correction=False)[0]
    n = confusion_matrix.sum()
    if n == 0:
        return np.nan
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    phi2corr = max(0, phi2 - (k - 1) * (r - 1) / (n - 1)) if n > 1 else 0
    rcorr = r - (r - 1) ** 2 / (n - 1) if n > 1 else r
    kcorr = k - (k - 1) ** 2 / (n - 1) if n > 1 else k
    denom = min((kcorr - 1), (rcorr - 1))
    if denom <= 0:
        return np.nan
    return sqrt(phi2corr / denom)


def theils_u(x, y):
    x = pd.Series(x).astype('category')
    y = pd.Series(y).astype('category')
    px = x.value_counts(normalize=True)
    py = y.value_counts(normalize=True)
    h_x = -np.sum(px * np.log2(px + 1e-12))
    if h_x <= 0:
        return np.nan
    h_x_given_y = 0.0
    for lvl, p_y in py.items():
        x_given_y = x[y == lvl]
        pxgy = x_given_y.value_counts(normalize=True)
        h_xgy = -np.sum(pxgy * np.log2(pxgy + 1e-12)) if len(pxgy) > 0 else 0.0
        h_x_given_y += p_y * h_xgy
    return max(0.0, (h_x - h_x_given_y) / (h_x + 1e-12))


def correlation_ratio(categories, measurements):
    cat = pd.Series(categories).astype('category')
    y = pd.to_numeric(pd.Series(measurements), errors='coerce')
    mask = cat.notna() & y.notna()
    cat = cat[mask]
    y = y[mask]
    if len(y) == 0:
        return np.nan
    overall_mean = y.mean()
    classes = cat.cat.categories
    ss_between = 0.0
    ss_total = ((y - overall_mean) ** 2).sum()
    for c in classes:
        y_c = y[cat == c]
        if len(y_c) == 0:
            continue
        ss_between += len(y_c) * (y_c.mean() - overall_mean) ** 2
    if ss_total <= 0:
        return 0.0
    return float(np.sqrt(ss_between / ss_total))


def discretize_numeric_series(s: pd.Series, bins: int = 10) -> pd.Series:
    s = pd.to_numeric(s, errors='coerce')
    s = s.replace([np.inf, -np.inf], np.nan)
    if s.notna().nunique() <= 1:
        return pd.Series(index=s.index, dtype='category')
    try:
        q = pd.qcut(s, q=min(bins, s.nunique()), duplicates='drop')
    except Exception:
        q = pd.cut(s, bins=min(bins, max(2, s.nunique())), include_lowest=True)
    return q.astype('category')# Directory output dedicate
targets_dir = output_dir / 'targets'
associations_dir = output_dir / 'associations'
for d in [targets_dir, associations_dir]:
    d.mkdir(parents=True, exist_ok=True)
print(f"üìÅ targets_dir: {targets_dir}")
print(f"üìÅ associations_dir: {associations_dir}")# Setup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import yaml
from pathlib import Path
from scipy import stats as scipy_stats
import warnings
warnings.filterwarnings('ignore')

plt.style.use('default')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

output_dir = Path('eda_outputs')
output_dir.mkdir(parents=True, exist_ok=True)
print(f"üìÅ Output dir: {output_dir}")

# Carica config e dati
config_path = '../config/config.yaml'
with open(config_path, 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

raw_path = '../data/raw/raw.parquet'
df_raw = pd.read_parquet(raw_path)
print(f"‚úÖ Dati caricati: {df_raw.shape}")

preferred_targets = config.get('target', {}).get('column_candidates', ['AI_Prezzo_Ridistribuito','AI_Prezzo_MQ'])
targets_presenti = [t for t in preferred_targets if t in df_raw.columns]
if not targets_presenti:
    raise ValueError("Nessun target presente nel dataset tra i candidati")
print(f"üéØ Target trovati: {targets_presenti}")