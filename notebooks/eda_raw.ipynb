# EDA Esteso in un'unica cella (robusto)
import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, yaml
from pathlib import Path
from scipy import stats as scipy_stats
from math import sqrt
from sklearn.metrics import normalized_mutual_info_score
import warnings
warnings.filterwarnings('ignore')

plt.style.use('default')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

# Output
output_dir = Path('eda_outputs'); output_dir.mkdir(parents=True, exist_ok=True)
targets_dir = output_dir / 'targets'; associations_dir = output_dir / 'associations'
for d in (targets_dir, associations_dir): d.mkdir(parents=True, exist_ok=True)

# Config e dati
config = yaml.safe_load(open('../config/config.yaml','r',encoding='utf-8'))
df_raw = pd.read_parquet('../data/raw/raw.parquet')
print('Dati:', df_raw.shape)

# Targets
preferred_targets = config.get('target',{}).get('column_candidates',['AI_Prezzo_Ridistribuito','AI_Prezzo_MQ'])
targets_presenti = [t for t in preferred_targets if t in df_raw.columns]
print('Targets:', targets_presenti)

# Parametri
high_corr_threshold = 0.90; max_cat_cardinality = 50; top_k_pairs = 30
print('Parametri:', high_corr_threshold, max_cat_cardinality, top_k_pairs)

# Panoramica
print('\nTipi di dati:')
print(df_raw.dtypes.value_counts())

# Missingness
missing_stats = pd.DataFrame({
    'Totale_Null': df_raw.isnull().sum(),
    'Percentuale_Null': (df_raw.isnull().mean() * 100).round(2),
    'Tipo': df_raw.dtypes
})
missing_stats.to_csv(output_dir / 'missingness_analysis.csv')
print('Missingness salvata')

# Utility associazioni
_def_nan = 1e-12

def _safe_crosstab(x, y):
    x = pd.Series(x).astype('category'); y = pd.Series(y).astype('category')
    return pd.crosstab(x, y)

def cramers_v_corrected(x, y):
    cm = _safe_crosstab(x, y).values
    if cm.size == 0: return np.nan
    chi2 = scipy_stats.chi2_contingency(cm, correction=False)[0]
    n = cm.sum();
    if n == 0: return np.nan
    phi2 = chi2 / n
    r, k = cm.shape
    phi2corr = max(0, phi2 - (k-1)*(r-1)/(n-1)) if n>1 else 0
    rcorr = r - (r-1)**2/(n-1) if n>1 else r
    kcorr = k - (k-1)**2/(n-1) if n>1 else k
    denom = min(kcorr-1, rcorr-1)
    if denom <= 0: return np.nan
    return float(np.sqrt(phi2corr / denom))

def theils_u(x, y):
    x = pd.Series(x).astype('category'); y = pd.Series(y).astype('category')
    px = x.value_counts(normalize=True); py = y.value_counts(normalize=True)
    h_x = -np.sum(px * np.log2(px + _def_nan))
    if h_x <= 0: return np.nan
    h_x_given_y = 0.0
    for lvl, p_y in py.items():
        x_given_y = x[y == lvl]
        pxgy = x_given_y.value_counts(normalize=True)
        h_xgy = -np.sum(pxgy * np.log2(pxgy + _def_nan)) if len(pxgy)>0 else 0.0
        h_x_given_y += p_y * h_xgy
    return max(0.0, (h_x - h_x_given_y) / (h_x + _def_nan))

def correlation_ratio(categories, measurements):
    cat = pd.Series(categories).astype('category')
    y = pd.to_numeric(pd.Series(measurements), errors='coerce')
    mask = cat.notna() & y.notna(); cat = cat[mask]; y = y[mask]
    if len(y) == 0: return np.nan
    overall = y.mean()
    ss_between = 0.0; ss_total = ((y - overall)**2).sum()
    for c in cat.cat.categories:
        y_c = y[cat == c]
        if len(y_c) == 0: continue
        ss_between += len(y_c) * (y_c.mean() - overall)**2
    if ss_total <= 0: return 0.0
    return float(np.sqrt(ss_between / ss_total))

def discretize_numeric_series(s: pd.Series, bins: int = 10) -> pd.Series:
    s = pd.to_numeric(s, errors='coerce').replace([np.inf, -np.inf], np.nan)
    if s.notna().nunique() <= 1: return pd.Series(index=s.index, dtype='category')
    try: q = pd.qcut(s, q=min(bins, s.nunique()), duplicates='drop')
    except Exception: q = pd.cut(s, bins=min(bins, max(2, s.nunique())), include_lowest=True)
    return q.astype('category')

# Analisi target
for tgt in targets_presenti:
    s = pd.to_numeric(df_raw[tgt], errors='coerce').dropna()
    if s.empty: continue
    stats = s.describe()
    pd.DataFrame(stats).rename(columns={tgt:'value'}).to_csv(targets_dir / f'target_statistics__{tgt}.csv')

# Correlazioni con i target (Pearson su numeriche)
for tgt in targets_presenti:
    num_cols = df_raw.select_dtypes(include=[np.number]).columns.tolist()
    if tgt in num_cols: num_cols.remove(tgt)
    num_cols = [c for c in num_cols if df_raw[c].nunique(dropna=True) > 1]
    data = df_raw[num_cols + [tgt]].dropna()
    if data.empty: continue
    corr = data.corr(numeric_only=True)[tgt].drop(tgt, errors='ignore')
    corr_df = (
        pd.DataFrame({'Colonna': corr.index, 'Correlazione': corr.values, 'Assoluta': np.abs(corr.values)})
        .sort_values('Assoluta', ascending=False)
    )
    corr_df.to_csv(targets_dir / f'correlations_with_target__{tgt}.csv', index=False)

# Associazioni feature-feature
# Numeriche
num_cols = df_raw.select_dtypes(include=[np.number]).columns.tolist()
num_cols = [c for c in num_cols if df_raw[c].nunique(dropna=True) > 1]
if len(num_cols) >= 2:
    corr_pearson = df_raw[num_cols].corr(method='pearson')
    corr_spearman = df_raw[num_cols].corr(method='spearman')
    corr_pearson.to_csv(associations_dir / 'numeric_numeric_pearson.csv')
    corr_spearman.to_csv(associations_dir / 'numeric_numeric_spearman.csv')
    pairs = []
    for i, c1 in enumerate(num_cols):
        for c2 in num_cols[i+1:]:
            r = corr_pearson.loc[c1, c2]
            if pd.notna(r) and abs(r) >= high_corr_threshold:
                pairs.append((c1, c2, float(r)))
    pd.DataFrame(sorted(pairs, key=lambda x: -abs(x[2]))[:top_k_pairs], columns=['col1','col2','pearson_r']).to_csv(
        associations_dir / f'numeric_numeric_highcorr_{int(high_corr_threshold*100)}.csv', index=False
    )

# Categoriche-categoriche
cat_cols = df_raw.select_dtypes(include=['object','category','bool']).columns.tolist()
for c in cat_cols:
    if df_raw[c].dtype == 'bool': df_raw[c] = df_raw[c].astype('category')
cat_cols = [c for c in cat_cols if 1 < df_raw[c].nunique(dropna=True) <= max_cat_cardinality]
cramers_records, theils_records = [], []
for i, c1 in enumerate(cat_cols):
    for c2 in cat_cols[i+1:]:
        cramers_records.append((c1, c2, cramers_v_corrected(df_raw[c1], df_raw[c2])))
        theils_records.append((c1, c2, theils_u(df_raw[c1], df_raw[c2])))
if cramers_records:
    pd.DataFrame(cramers_records, columns=['col1','col2','cramers_v']).sort_values('cramers_v', ascending=False).head(top_k_pairs).to_csv(
        associations_dir / 'categorical_categorical_top_cramersV.csv', index=False
    )
if theils_records:
    pd.DataFrame(theils_records, columns=['col1','col2','theils_u']).sort_values('theils_u', ascending=False).head(top_k_pairs).to_csv(
        associations_dir / 'categorical_categorical_top_theilsU.csv', index=False
    )

# Miste (num-cat)
records_eta, records_nmi = [], []
for num in num_cols:
    for cat in cat_cols:
        eta = correlation_ratio(df_raw[cat], df_raw[num])
        dnum = discretize_numeric_series(df_raw[num])
        nmi = normalized_mutual_info_score(dnum, df_raw[cat].astype('category'))
        records_eta.append((cat, num, eta))
        records_nmi.append((cat, num, nmi))
if records_eta:
    pd.DataFrame(records_eta, columns=['cat','num','eta']).sort_values('eta', ascending=False).head(top_k_pairs).to_csv(
        associations_dir / 'mixed_cat_num_top_eta.csv', index=False
    )
if records_nmi:
    pd.DataFrame(records_nmi, columns=['cat','num','nmi']).sort_values('nmi', ascending=False).head(top_k_pairs).to_csv(
        associations_dir / 'mixed_cat_num_top_nmi.csv', index=False
    )

# Summary
print('= Outputs (csv) =')
for f in output_dir.rglob('*.csv'):
    print(' -', f.relative_to(output_dir).as_posix())
print('EDA esteso completato')