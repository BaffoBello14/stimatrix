{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis - Raw Dataset\n",
        "\n",
        "Questo notebook esegue un'analisi esplorativa completa del dataset raw per il progetto StiMatrix.\n",
        "\n",
        "## Obiettivi dell'analisi:\n",
        "1. Caricamento configurazione e dati\n",
        "2. Overview del dataset (shape, dtypes, memoria, head)\n",
        "3. Analisi missingness e cardinalitÃ  categoriche\n",
        "4. Distribuzione del target `AI_Prezzo_Ridistribuito`\n",
        "5. Summary per gruppi (`AI_ZonaOmi`, `AI_IdCategoriaCatastale`)\n",
        "6. Correlazioni con il target\n",
        "7. Check geospaziale rapido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup e caricamento dati\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup completato!\n"
          ]
        }
      ],
      "source": [
        "# Import delle librerie necessarie\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yaml\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurazione matplotlib per grafici piÃ¹ leggibili\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"Setup completato!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Caricamento dati da: ${RAW_DATA_DIR:-'data\\raw'}\\raw.parquet\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "File non trovato: ${RAW_DATA_DIR:-'data\\raw'}\\raw.parquet",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Esegui caricamento\u001b[39;00m\n\u001b[32m     17\u001b[39m config = load_config()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m df_raw = \u001b[43mload_raw_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Configurazione caricata da: config/config.yaml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Dataset caricato: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_raw.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m righe Ã— \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_raw.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m colonne\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mload_raw_data\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCaricamento dati da: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile non trovato: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.read_parquet(raw_path)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: File non trovato: ${RAW_DATA_DIR:-'data\\raw'}\\raw.parquet"
          ]
        }
      ],
      "source": [
        "# Caricamento configurazione\n",
        "def load_config(config_path='../config/config.yaml'):\n",
        "    \"\"\"Carica il file di configurazione YAML\"\"\"\n",
        "    with open(config_path, 'r', encoding='utf-8') as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "# Caricamento dataset raw\n",
        "def load_raw_data(config):\n",
        "    \"\"\"Carica il dataset raw usando i path dalla configurazione\"\"\"\n",
        "    raw_path = Path(config['paths']['raw_data']) / config['paths']['raw_filename']\n",
        "    print(f\"Caricamento dati da: {raw_path}\")\n",
        "    if not raw_path.exists():\n",
        "        raise FileNotFoundError(f\"File non trovato: {raw_path}\")\n",
        "    return pd.read_parquet(raw_path)\n",
        "\n",
        "# Esegui caricamento\n",
        "config = load_config()\n",
        "df_raw = load_raw_data(config)\n",
        "\n",
        "print(f\"âœ… Configurazione caricata da: config/config.yaml\")\n",
        "print(f\"âœ… Dataset caricato: {df_raw.shape[0]:,} righe Ã— {df_raw.shape[1]} colonne\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loader robusto con espansione placeholder e ricerca percorsi\n",
        "import itertools\n",
        "\n",
        "PLACEHOLDER_RE = re.compile(r\"\\$\\{([^}:]+)(?::-(.*?))?\\}\")\n",
        "\n",
        "\n",
        "def _expand_placeholders(value: str) -> str:\n",
        "    if not isinstance(value, str):\n",
        "        return value\n",
        "\n",
        "    def _repl(m):\n",
        "        var = m.group(1)\n",
        "        default = m.group(2)\n",
        "        if default is not None:\n",
        "            default = default.strip()\n",
        "            # rimuovi eventuali apici singoli/doppi ai bordi\n",
        "            if (default.startswith(\"'\") and default.endswith(\"'\")) or (\n",
        "                default.startswith('\"') and default.endswith('\"')\n",
        "            ):\n",
        "                default = default[1:-1]\n",
        "        return os.environ.get(var, default if default is not None else \"\")\n",
        "\n",
        "    return PLACEHOLDER_RE.sub(_repl, value)\n",
        "\n",
        "\n",
        "def load_config(config_path_candidates=(\"config/config.yaml\", \"../config/config.yaml\")):\n",
        "    \"\"\"Carica la config YAML e espande i placeholder ${VAR:-default}.\"\"\"\n",
        "    for cfg_path in config_path_candidates:\n",
        "        p = Path(cfg_path)\n",
        "        if p.exists():\n",
        "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "                cfg = yaml.safe_load(f)\n",
        "            # Espandi solo i campi che usiamo\n",
        "            if \"paths\" in cfg:\n",
        "                for k in (\"raw_data\", \"raw_filename\"):\n",
        "                    if k in cfg[\"paths\"]:\n",
        "                        cfg[\"paths\"][k] = _expand_placeholders(cfg[\"paths\"][k])\n",
        "            print(f\"Config caricata da: {p}\")\n",
        "            return cfg\n",
        "    raise FileNotFoundError(\n",
        "        f\"Impossibile trovare config.yaml in: {config_path_candidates}\")\n",
        "\n",
        "\n",
        "def load_raw_data(config):\n",
        "    \"\"\"Carica il parquet raw provando vari percorsi candidati (root e notebooks).\"\"\"\n",
        "    raw_dir = config[\"paths\"][\"raw_data\"]\n",
        "    raw_fn = config[\"paths\"].get(\"raw_filename\", \"raw.parquet\")\n",
        "\n",
        "    candidates = []\n",
        "    bases = [Path(\".\"), Path(\"..\")]\n",
        "    # Se raw_dir Ã¨ assoluto, consideralo anche senza base\n",
        "    if Path(raw_dir).is_absolute():\n",
        "        candidates.append(Path(raw_dir) / raw_fn)\n",
        "    # Aggiungi combinazioni base/raw_dir/raw_fn\n",
        "    candidates.extend((b / raw_dir / raw_fn) for b in bases)\n",
        "\n",
        "    tried = []\n",
        "    for cand in candidates:\n",
        "        cand = cand.resolve()\n",
        "        tried.append(str(cand))\n",
        "        print(f\"Provo: {cand}\")\n",
        "        if cand.exists():\n",
        "            print(f\"Carico: {cand}\")\n",
        "            return pd.read_parquet(cand)\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"File raw non trovato. Tentati percorsi: \\n - \" + \"\\n - \".join(tried)\n",
        "    )\n",
        "\n",
        "# Ricarica usando il loader robusto\n",
        "config = load_config()\n",
        "df_raw = load_raw_data(config)\n",
        "print(f\"âœ… Dataset caricato: {df_raw.shape[0]:,} righe Ã— {df_raw.shape[1]} colonne\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Overview del dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informazioni generali sul dataset\n",
        "def dataset_overview(df):\n",
        "    \"\"\"Fornisce overview completo del dataset\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"OVERVIEW DATASET\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nðŸ“Š Dimensioni: {df.shape[0]:,} righe Ã— {df.shape[1]} colonne\")\n",
        "    memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    print(f\"ðŸ’¾ Memoria utilizzata: {memory_mb:.2f} MB\")\n",
        "    print(f\"\\nðŸ“‹ Tipi di dati:\")\n",
        "    dtype_counts = df.dtypes.value_counts()\n",
        "    for dtype, count in dtype_counts.items():\n",
        "        print(f\"  {dtype}: {count} colonne\")\n",
        "    print(f\"\\nðŸŽ¯ Target identificato: {config['target']['column_candidates'][0]}\")\n",
        "    return dtype_counts\n",
        "\n",
        "# Esegui overview\n",
        "dtype_summary = dataset_overview(df_raw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prime righe del dataset\n",
        "print(\"\\nðŸ” Prime 5 righe del dataset:\")\n",
        "display(df_raw.head())\n",
        "\n",
        "# Informazioni dettagliate sulle colonne\n",
        "print(\"\\nðŸ“ Informazioni dettagliate sulle colonne:\")\n",
        "df_info = df_raw.dtypes.to_frame('Tipo')\n",
        "df_info['Valori Non-Nulli'] = df_raw.count()\n",
        "df_info['Valori Nulli'] = df_raw.isnull().sum()\n",
        "df_info['% Completamento'] = (1 - df_raw.isnull().mean()) * 100\n",
        "display(df_info.round(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Analisi Missingness e CardinalitÃ \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisi missingness\n",
        "def analyze_missingness(df):\n",
        "    \"\"\"Analizza la missingness nel dataset\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ANALISI MISSINGNESS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    missing_stats = pd.DataFrame({\n",
        "        'Totale_Null': df.isnull().sum(),\n",
        "        'Percentuale_Null': (df.isnull().mean() * 100).round(2),\n",
        "        'Tipo': df.dtypes\n",
        "    })\n",
        "\n",
        "    high_missing = missing_stats[missing_stats['Percentuale_Null'] > 50].sort_values('Percentuale_Null', ascending=False)\n",
        "\n",
        "    print(f\"\\nâš ï¸  Colonne con >50% valori mancanti ({len(high_missing)} colonne):\")\n",
        "    if len(high_missing) > 0:\n",
        "        display(high_missing)\n",
        "    else:\n",
        "        print(\"  Nessuna colonna con >50% valori mancanti\")\n",
        "\n",
        "    return missing_stats\n",
        "\n",
        "# Analisi cardinalitÃ  categoriche\n",
        "def analyze_categorical_cardinality(df):\n",
        "    \"\"\"Analizza la cardinalitÃ  delle colonne categoriche\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ANALISI CARDINALITÃ€ CATEGORICHE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    if len(cat_cols) == 0:\n",
        "        print(\"\\nâŒ Nessuna colonna categorica trovata\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    cardinality_stats = []\n",
        "    for col in cat_cols:\n",
        "        unique_vals = df[col].nunique()\n",
        "        total_vals = len(df[col])\n",
        "        cardinality_ratio = unique_vals / total_vals\n",
        "\n",
        "        cardinality_stats.append({\n",
        "            'Colonna': col,\n",
        "            'Valori_Unici': unique_vals,\n",
        "            'Totale_Valori': total_vals,\n",
        "            'Ratio_CardinalitÃ ': round(cardinality_ratio, 4),\n",
        "            'Tipo_CardinalitÃ ': 'Alta' if cardinality_ratio > 0.1 else 'Bassa' if cardinality_ratio < 0.01 else 'Media'\n",
        "        })\n",
        "\n",
        "    cardinality_df = pd.DataFrame(cardinality_stats).sort_values('Valori_Unici', ascending=False)\n",
        "\n",
        "    print(f\"\\nðŸ“Š Colonne categoriche trovate: {len(cat_cols)}\")\n",
        "    print(f\"\\nðŸ” Top 10 colonne per cardinalitÃ :\")\n",
        "    display(cardinality_df.head(10))\n",
        "\n",
        "    return cardinality_df\n",
        "\n",
        "# Esegui analisi\n",
        "missing_stats = analyze_missingness(df_raw)\n",
        "cardinality_stats = analyze_categorical_cardinality(df_raw)\n",
        "\n",
        "# Salva risultati su CSV\n",
        "output_dir = Path('notebooks') / 'eda_outputs'\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "missing_stats.to_csv(output_dir / 'missingness_analysis.csv')\n",
        "cardinality_stats.to_csv(output_dir / 'categorical_cardinality.csv')\n",
        "\n",
        "print(f\"\\nðŸ’¾ Risultati salvati in {output_dir}/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Analisi del Target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identifica il target\n",
        "target_col = config['target']['column_candidates'][0]\n",
        "\n",
        "if target_col not in df_raw.columns:\n",
        "    print(f\"âŒ Target '{target_col}' non trovato nelle colonne!\")\n",
        "    print(f\"Colonne disponibili: {list(df_raw.columns)}\")\n",
        "    raise ValueError(f\"Target column '{target_col}' not found\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Analisi del target: {target_col}\")\n",
        "print(f\"ðŸ“Š Tipo: {df_raw[target_col].dtype}\")\n",
        "print(f\"ðŸ”¢ Valori non-nulli: {df_raw[target_col].count()}\")\n",
        "print(f\"ðŸ“ˆ Valori mancanti: {df_raw[target_col].isnull().sum()} ({df_raw[target_col].isnull().mean()*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistiche e fasce a quantili del target\n",
        "from scipy import stats as scipy_stats\n",
        "\n",
        "\n",
        "def analyze_target_distribution(df, target_col):\n",
        "    \"\"\"Analizza la distribuzione del target e crea fasce di prezzo\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"ANALISI DISTRIBUZIONE TARGET: {target_col}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    target_data = df[target_col].dropna()\n",
        "    stats = target_data.describe()\n",
        "\n",
        "    print(f\"\\nðŸ“ˆ Statistiche descrittive:\")\n",
        "    print(f\"  Conteggio: {stats['count']:,.0f}\")\n",
        "    print(f\"  Media: â‚¬{stats['mean']:,.2f}\")\n",
        "    print(f\"  Mediana: â‚¬{stats['50%']:,.2f}\")\n",
        "    print(f\"  Std Dev: â‚¬{stats['std']:,.2f}\")\n",
        "    print(f\"  Min: â‚¬{stats['min']:,.2f}\")\n",
        "    print(f\"  Max: â‚¬{stats['max']:,.2f}\")\n",
        "\n",
        "    # Calcola fasce di prezzo a quantili\n",
        "    quantiles = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "    price_bands = target_data.quantile(quantiles)\n",
        "\n",
        "    print(f\"\\nðŸ·ï¸  Fasce di prezzo (quantili):\")\n",
        "    for i in range(len(quantiles)-1):\n",
        "        q_low = quantiles[i]\n",
        "        q_high = quantiles[i+1]\n",
        "        price_low = price_bands.iloc[i]\n",
        "        price_high = price_bands.iloc[i+1]\n",
        "        print(f\"  Q{int(q_low*100):02d}-Q{int(q_high*100):02d}: â‚¬{price_low:,.0f} - â‚¬{price_high:,.0f}\")\n",
        "\n",
        "    return stats, price_bands\n",
        "\n",
        "\n",
        "def plot_target_distribution(df, target_col):\n",
        "    \"\"\"Crea grafici per la distribuzione del target\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    target_data = df[target_col].dropna()\n",
        "\n",
        "    # 1. Istogramma\n",
        "    axes[0,0].hist(target_data, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[0,0].set_title('Distribuzione Target - Istogramma')\n",
        "    axes[0,0].set_xlabel('Prezzo')\n",
        "    axes[0,0].set_ylabel('Frequenza')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Box plot\n",
        "    axes[0,1].boxplot(target_data, vert=False)\n",
        "    axes[0,1].set_title('Distribuzione Target - Box Plot')\n",
        "    axes[0,1].set_xlabel('Prezzo')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. QQ Plot\n",
        "    sample_size = min(5000, len(target_data))\n",
        "    sample_data = target_data.sample(n=sample_size, random_state=42)\n",
        "    scipy_stats.probplot(sample_data, dist=\"norm\", plot=axes[1,0])\n",
        "    axes[1,0].set_title('Q-Q Plot (confronto con normale)')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Quantili\n",
        "    quantiles = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "    axes[1,1].hist(target_data, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "    for q in quantiles[1:-1]:\n",
        "        price_q = target_data.quantile(q)\n",
        "        axes[1,1].axvline(price_q, color='red', linestyle='--', alpha=0.7)\n",
        "        axes[1,1].text(price_q, axes[1,1].get_ylim()[1]*0.9, f'Q{int(q*100)}',\n",
        "                       rotation=90, verticalalignment='top', fontweight='bold')\n",
        "    axes[1,1].set_title('Distribuzione con Quantili')\n",
        "    axes[1,1].set_xlabel('Prezzo')\n",
        "    axes[1,1].set_ylabel('Frequenza')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Esegui analisi target\n",
        "target_stats, price_bands = analyze_target_distribution(df_raw, target_col)\n",
        "plot_target_distribution(df_raw, target_col)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary per Gruppi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colonne di grouping dalla configurazione\n",
        "group_cols = ['AI_ZonaOmi', 'AI_IdCategoriaCatastale']\n",
        "\n",
        "# Verifica che le colonne esistano\n",
        "missing_group_cols = [col for col in group_cols if col not in df_raw.columns]\n",
        "if missing_group_cols:\n",
        "    print(f\"âŒ Colonne di grouping mancanti: {missing_group_cols}\")\n",
        "    print(f\"Colonne disponibili: {list(df_raw.columns)}\")\n",
        "else:\n",
        "    print(f\"âœ… Colonne di grouping trovate: {group_cols}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funzione per creare summary per gruppi\n",
        "def create_group_summary(df, group_col, target_col):\n",
        "    \"\"\"Crea summary statistico per una colonna di grouping\"\"\"\n",
        "    print(f\"\\nðŸ“Š Summary per gruppo: {group_col}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    valid_data = df[[group_col, target_col]].dropna()\n",
        "    if len(valid_data) == 0:\n",
        "        print(f\"âŒ Nessun dato valido per {group_col}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    group_stats = valid_data.groupby(group_col)[target_col].agg([\n",
        "        'count', 'mean', 'median', 'std', 'min', 'max'\n",
        "    ]).round(2)\n",
        "\n",
        "    group_stats = group_stats.sort_values('count', ascending=False)\n",
        "    group_stats['cv'] = (group_stats['std'] / group_stats['mean'] * 100).round(2)\n",
        "\n",
        "    print(f\"Gruppi trovati: {len(group_stats)}\")\n",
        "    print(f\"\\nðŸ” Top 10 gruppi per dimensione:\")\n",
        "    display(group_stats.head(10))\n",
        "\n",
        "    return group_stats\n",
        "\n",
        "# Esegui summary per entrambi i gruppi\n",
        "group_summaries = {}\n",
        "\n",
        "for group_col in group_cols:\n",
        "    if group_col in df_raw.columns:\n",
        "        group_summary = create_group_summary(df_raw, group_col, target_col)\n",
        "        group_summaries[group_col] = group_summary\n",
        "        \n",
        "        # Salva su CSV\n",
        "        output_file = output_dir / f'group_summary_{group_col}.csv'\n",
        "        group_summary.to_csv(output_file)\n",
        "        print(f\"ðŸ’¾ Salvato: {output_file}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Colonna {group_col} non trovata, skip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Correlazioni col Target, Colonne Costanti e NA elevati\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colonne numeriche\n",
        "\n",
        "def get_numeric_columns(df, target_col):\n",
        "    \"\"\"Identifica colonne numeriche escludendo il target\"\"\"\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if target_col in numeric_cols:\n",
        "        numeric_cols.remove(target_col)\n",
        "    print(f\"\\nðŸ”¢ Colonne numeriche identificate: {len(numeric_cols)}\")\n",
        "    return numeric_cols\n",
        "\n",
        "\n",
        "def identify_constant_columns(df, numeric_cols):\n",
        "    \"\"\"Identifica colonne numeriche costanti\"\"\"\n",
        "    constant_cols = [col for col in numeric_cols if df[col].nunique() <= 1]\n",
        "    print(f\"\\nâš ï¸  Colonne costanti identificate: {len(constant_cols)}\")\n",
        "    if constant_cols:\n",
        "        print(\"  Colonne:\", constant_cols)\n",
        "    return constant_cols\n",
        "\n",
        "\n",
        "def compute_correlations(df, target_col, numeric_cols, threshold=0.1):\n",
        "    \"\"\"Calcola correlazioni con il target\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"ANALISI CORRELAZIONI CON {target_col}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    corr_data = df[numeric_cols + [target_col]].dropna()\n",
        "    if len(corr_data) == 0:\n",
        "        print(\"âŒ Nessun dato valido per correlazioni\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    correlations = corr_data.corr(numeric_only=True)[target_col].drop(target_col)\n",
        "\n",
        "    corr_df = pd.DataFrame({\n",
        "        'Colonna': correlations.index,\n",
        "        'Correlazione': correlations.values,\n",
        "        'Correlazione_Assoluta': np.abs(correlations.values)\n",
        "    }).round(4).sort_values('Correlazione_Assoluta', ascending=False)\n",
        "\n",
        "    significant_corr = corr_df[corr_df['Correlazione_Assoluta'] >= threshold]\n",
        "    print(f\"\\nðŸ“ˆ Correlazioni significative (|r| >= {threshold}): {len(significant_corr)}\")\n",
        "    if len(significant_corr) > 0:\n",
        "        display(significant_corr.head(15))\n",
        "\n",
        "    print(f\"\\nðŸ“Š Statistiche correlazioni:\")\n",
        "    print(f\"  Max correlazione positiva: {corr_df['Correlazione'].max():.4f}\")\n",
        "    print(f\"  Max correlazione negativa: {corr_df['Correlazione'].min():.4f}\")\n",
        "    print(f\"  Media correlazione assoluta: {corr_df['Correlazione_Assoluta'].mean():.4f}\")\n",
        "\n",
        "    return corr_df\n",
        "\n",
        "\n",
        "# Colonne con NA elevati (> 50%)\n",
        "def identify_high_na_columns(df, threshold=0.5):\n",
        "    na_ratio = df.isna().mean()\n",
        "    high_na_cols = na_ratio[na_ratio > threshold].index.tolist()\n",
        "    print(f\"\\nâš ï¸  Colonne con NA > {int(threshold*100)}%: {len(high_na_cols)}\")\n",
        "    if high_na_cols:\n",
        "        print(\"  Colonne:\", high_na_cols[:30], (\"...\" if len(high_na_cols) > 30 else \"\"))\n",
        "    return high_na_cols\n",
        "\n",
        "\n",
        "# Esecuzione analisi\n",
        "numeric_cols = get_numeric_columns(df_raw, target_col)\n",
        "constant_cols = identify_constant_columns(df_raw, numeric_cols)\n",
        "high_na_cols = identify_high_na_columns(df_raw, threshold=0.5)\n",
        "correlation_results = compute_correlations(df_raw, target_col, [c for c in numeric_cols if c not in constant_cols])\n",
        "\n",
        "# Salvataggi\n",
        "if len(correlation_results) > 0:\n",
        "    correlation_results.to_csv(output_dir / 'correlations_with_target.csv', index=False)\n",
        "if constant_cols:\n",
        "    pd.Series(constant_cols, name='constant_columns').to_csv(output_dir / 'constant_numeric_columns.csv', index=False)\n",
        "if high_na_cols:\n",
        "    pd.Series(high_na_cols, name='high_na_columns').to_csv(output_dir / 'high_na_columns.csv', index=False)\n",
        "\n",
        "print(f\"\\nðŸ’¾ Output correlazioni salvati in {output_dir}/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Check Geospaziale (WKT/GeoJSON e coordinate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identifica colonne geospaziali candidate\n",
        "\n",
        "def identify_geospatial_columns(df):\n",
        "    \"\"\"Identifica colonne che potrebbero contenere dati geospaziali (WKT/GeoJSON/coordinate).\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CHECK GEOSPAZIALE RAPIDO\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    candidates = []\n",
        "    all_cols = df.columns.tolist()\n",
        "\n",
        "    # Pattern di interesse\n",
        "    geo_patterns = [\n",
        "        # WKT/GeoJSON\n",
        "        'wkt', 'geometry', 'geom', 'geojson', 'the_geom', 'shape', 'polygon', 'point', 'linestring',\n",
        "        # Coordinate\n",
        "        'lat', 'latitude', 'lon', 'lng', 'longitude', 'coord', 'x_', 'y_', 'easting', 'northing',\n",
        "        # Generici\n",
        "        'geo', 'spatial', 'location', 'posizione', 'indirizzo', 'address'\n",
        "    ]\n",
        "\n",
        "    for col in all_cols:\n",
        "        col_l = col.lower()\n",
        "        for pat in geo_patterns:\n",
        "            if pat in col_l:\n",
        "                sample_vals = df[col].dropna().head(3).tolist()\n",
        "                candidates.append({\n",
        "                    'Colonna': col,\n",
        "                    'Tipo': str(df[col].dtype),\n",
        "                    'Pattern': pat,\n",
        "                    'NonNull_Count': int(df[col].count()),\n",
        "                    'Esempi': sample_vals\n",
        "                })\n",
        "                break\n",
        "\n",
        "    # Heuristics contenuto: prova a riconoscere WKT / GeoJSON testuali\n",
        "    def looks_like_wkt(s):\n",
        "        if not isinstance(s, str):\n",
        "            return False\n",
        "        s0 = s.strip().upper()\n",
        "        return s0.startswith(('POINT', 'LINESTRING', 'POLYGON', 'MULTIPOLYGON', 'MULTILINESTRING', 'MULTIPOINT'))\n",
        "\n",
        "    def looks_like_geojson(s):\n",
        "        if not isinstance(s, (str, dict)):\n",
        "            return False\n",
        "        if isinstance(s, dict):\n",
        "            return 'type' in s and 'coordinates' in s\n",
        "        s0 = s.strip().lower()\n",
        "        return s0.startswith('{') and '\"type\"' in s0 and '\"coordinates\"' in s0\n",
        "\n",
        "    for col in all_cols:\n",
        "        if df[col].dtype == 'object':\n",
        "            sample = df[col].dropna().astype(str).head(5)\n",
        "            if len(sample) > 0:\n",
        "                if sample.map(looks_like_wkt).any():\n",
        "                    candidates.append({'Colonna': col, 'Tipo': 'object', 'Pattern': 'WKT_content',\n",
        "                                       'NonNull_Count': int(df[col].count()), 'Esempi': sample.head(3).tolist()})\n",
        "                elif sample.map(looks_like_geojson).any():\n",
        "                    candidates.append({'Colonna': col, 'Tipo': 'object', 'Pattern': 'GeoJSON_content',\n",
        "                                       'NonNull_Count': int(df[col].count()), 'Esempi': sample.head(3).tolist()})\n",
        "\n",
        "    if not candidates:\n",
        "        print(\"\\nâŒ Nessuna colonna geospaziale candidata individuata\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    geo_df = pd.DataFrame(candidates).drop_duplicates(subset=['Colonna', 'Pattern'])\n",
        "    print(f\"\\nðŸ—ºï¸  Colonne candidate geospaziali: {len(geo_df)}\")\n",
        "    display(geo_df)\n",
        "    return geo_df\n",
        "\n",
        "# Esegui check geospaziale\n",
        "geo_cols = identify_geospatial_columns(df_raw)\n",
        "if len(geo_cols) > 0:\n",
        "    geo_cols.to_csv(output_dir / 'geospatial_columns_check.csv', index=False)\n",
        "    print(f\"\\nðŸ’¾ Check geospaziale salvato in {output_dir}/geospatial_columns_check.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "strategia",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
