{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Model Results Deep Analysis\n",
        "\n",
        "Analisi approfondita dei risultati sfruttando **gli artefatti gi√† salvati in `models/`** (metriche, segmentazioni, prediction intervals, worst predictions).\n",
        "\n",
        "**Prerequisiti**:\n",
        "- Almeno una run di training/evaluation (`python main.py --config config/config.yaml --steps train evaluate`) *oppure* la disponibilit√† dei file gi√† inclusi nel repository\n",
        "- Nessun bisogno di rigenerare feature o modelli: il notebook legge direttamente i JSON/CSV salvati\n",
        "\n",
        "**Analisi**:\n",
        "1. **Leaderboard**: confronto rapido fra modelli, baseline ed ensemble\n",
        "2. **Metriche Train/Test**: approfondimento del modello selezionato\n",
        "3. **Validation Curve**: lettura di `validation_results.csv`\n",
        "4. **Segmentazioni degli errori**: per fascia di prezzo, Zona OMI, Tipologia Edilizia, Categoria Catastale\n",
        "5. **Worst Predictions**: casi peggiori con residui e % errore\n",
        "6. **Prediction Intervals**: copertura e ampiezza degli intervalli 80% / 90%\n",
        "\n",
        "**Output**: report e grafici in `model_analysis_outputs/`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports\n",
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from IPython.display import display\n",
        "\n",
        "from utils.config import load_config\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.options.display.float_format = \"{:.2f}\".format\n",
        "\n",
        "# Plot settings\n",
        "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"‚úÖ Setup completato\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Setup completato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configurazione\n",
        "CONFIG_PATH = Path(\"../config/config.yaml\")\n",
        "MODELS_DIR = Path(\"../models\")\n",
        "OUTPUT_DIR = Path(\"model_analysis_outputs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "SUMMARY_CANDIDATES = [\n",
        "    MODELS_DIR / \"summary.json\",\n",
        "    MODELS_DIR / \"results_summary.json\"\n",
        "]\n",
        "EVALUATION_SUMMARY_PATH = MODELS_DIR / \"evaluation_summary.json\"\n",
        "VALIDATION_RESULTS_PATH = MODELS_DIR / \"validation_results.csv\"\n",
        "\n",
        "\n",
        "def first_existing(paths):\n",
        "    for path in paths:\n",
        "        if path and path.exists():\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "\n",
        "def load_json_safe(path, default=None):\n",
        "    if path and path.exists():\n",
        "        with open(path, \"r\") as fp:\n",
        "            return json.load(fp)\n",
        "    return default if default is not None else {}\n",
        "\n",
        "\n",
        "def load_csv_optional(path, **kwargs):\n",
        "    if path.exists():\n",
        "        return pd.read_csv(path, **kwargs)\n",
        "    print(f\"‚ö†Ô∏è File non trovato: {path}\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "\n",
        "def save_plot(name, dpi=120):\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTPUT_DIR / f\"{name}.png\", dpi=dpi, bbox_inches=\"tight\")\n",
        "    print(f\"üíæ Salvato: {name}.png\")\n",
        "\n",
        "\n",
        "SUMMARY_PATH = first_existing(SUMMARY_CANDIDATES)\n",
        "\n",
        "print(f\"üìÇ Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"üìÇ Models directory: {MODELS_DIR}\")\n",
        "print(f\"üìÑ Summary file: {SUMMARY_PATH if SUMMARY_PATH else 'non trovato'}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Output directory: model_analysis_outputs\n",
            "üìÇ Preprocessed data: ..\\data\\preprocessed\n",
            "üìÇ Models directory: ..\\models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 1. Caricamento artefatti e selezione del modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Caricamento config e artefatti\n",
        "config = load_config(CONFIG_PATH)\n",
        "summary = load_json_safe(SUMMARY_PATH, default={})\n",
        "evaluation_summary = load_json_safe(EVALUATION_SUMMARY_PATH, default={})\n",
        "validation_df = load_csv_optional(VALIDATION_RESULTS_PATH)\n",
        "\n",
        "\n",
        "def build_leaderboard(summary_dict):\n",
        "    rows = []\n",
        "    sections = [\n",
        "        (\"models\", \"Optimized\"),\n",
        "        (\"ensembles\", \"Ensemble\"),\n",
        "        (\"baselines\", \"Baseline\"),\n",
        "    ]\n",
        "    for section, label in sections:\n",
        "        for name, payload in summary_dict.get(section, {}).items():\n",
        "            metrics = payload.get(\"metrics_test\", {})\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"Model\": name,\n",
        "                    \"Source\": label,\n",
        "                    \"R2\": metrics.get(\"r2\"),\n",
        "                    \"RMSE\": metrics.get(\"rmse\"),\n",
        "                    \"MAE\": metrics.get(\"mae\"),\n",
        "                }\n",
        "            )\n",
        "    df = pd.DataFrame(rows)\n",
        "    if df.empty:\n",
        "        return df\n",
        "    return df.dropna(subset=[\"R2\"]).sort_values(\"R2\", ascending=False)\n",
        "\n",
        "\n",
        "def normalize_model_name(label):\n",
        "    if not label:\n",
        "        return None\n",
        "    return label.split(\"_\")[-1].lower()\n",
        "\n",
        "\n",
        "def pick_best_model(leaderboard, candidates):\n",
        "    if not leaderboard.empty:\n",
        "        best_candidate = leaderboard.iloc[0][\"Model\"]\n",
        "        if best_candidate in candidates:\n",
        "            return best_candidate\n",
        "        normalized = normalize_model_name(best_candidate)\n",
        "        if normalized in candidates:\n",
        "            return normalized\n",
        "    return None\n",
        "\n",
        "\n",
        "def pick_from_validation(df, candidates):\n",
        "    if df.empty:\n",
        "        return None\n",
        "    best_row = df.sort_values(\"Test_R2\", ascending=False).iloc[0]\n",
        "    candidate = normalize_model_name(best_row[\"Model\"])\n",
        "    return candidate if candidate in candidates else None\n",
        "\n",
        "\n",
        "leaderboard_df = build_leaderboard(summary)\n",
        "available_model_dirs = sorted([p.name for p in MODELS_DIR.iterdir() if p.is_dir()])\n",
        "print(f\"üìÅ Modelli disponibili: {', '.join(available_model_dirs) if available_model_dirs else 'nessuno'}\")\n",
        "\n",
        "selected_model_key = pick_best_model(leaderboard_df, available_model_dirs)\n",
        "if not selected_model_key:\n",
        "    selected_model_key = pick_from_validation(validation_df, available_model_dirs)\n",
        "if not selected_model_key and available_model_dirs:\n",
        "    selected_model_key = available_model_dirs[0]\n",
        "\n",
        "if not selected_model_key:\n",
        "    raise FileNotFoundError(\"Nessuna directory modello disponibile sotto models/\")\n",
        "\n",
        "SELECTED_MODEL_KEY = selected_model_key\n",
        "MODEL_ARTIFACT_DIR = MODELS_DIR / SELECTED_MODEL_KEY\n",
        "print(f\"üèÜ Modello analizzato: {SELECTED_MODEL_KEY}\")\n",
        "\n",
        "model_metrics = load_json_safe(MODEL_ARTIFACT_DIR / \"metrics.json\", default={})\n",
        "price_band_df = load_csv_optional(MODEL_ARTIFACT_DIR / \"group_metrics_price_band.csv\")\n",
        "zona_metrics_df = load_csv_optional(MODEL_ARTIFACT_DIR / \"group_metrics_AI_ZonaOmi.csv\")\n",
        "tipologia_metrics_df = load_csv_optional(MODEL_ARTIFACT_DIR / \"group_metrics_AI_IdTipologiaEdilizia.csv\")\n",
        "categoria_metrics_df = load_csv_optional(MODEL_ARTIFACT_DIR / \"group_metrics_AI_IdCategoriaCatastale.csv\")\n",
        "worst_predictions_df = load_csv_optional(MODEL_ARTIFACT_DIR / f\"{SELECTED_MODEL_KEY}_worst_predictions.csv\")\n",
        "prediction_intervals = load_json_safe(\n",
        "    MODEL_ARTIFACT_DIR / f\"{SELECTED_MODEL_KEY}_prediction_intervals.json\", default={}\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Config e artefatti caricati\\n\")\n",
        "print(f\"üìä Leaderboard entries: {len(leaderboard_df)}\")\n",
        "print(f\"üìà Validation rows: {len(validation_df)}\")\n",
        "print(f\"üì¶ Metriche disponibili: {'s√¨' if model_metrics else 'no'}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Config e results caricati\n",
            "\n",
            "üìä Best Model: N/A\n",
            "üìä Best Params: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Leaderboard overview\n",
        "if leaderboard_df.empty:\n",
        "    print(\"‚ö†Ô∏è Nessuna metrica trovata in summary.json/results_summary.json\")\n",
        "else:\n",
        "    leaderboard_display = leaderboard_df.copy()\n",
        "    leaderboard_display[[\"R2\", \"RMSE\", \"MAE\"]] = leaderboard_display[[\"R2\", \"RMSE\", \"MAE\"]].round(3)\n",
        "    print(\"üèÅ Leaderboard globale (Top 10 per R¬≤)\")\n",
        "    display(leaderboard_display.head(10))\n",
        "    leaderboard_display.to_csv(OUTPUT_DIR / \"01_leaderboard.csv\", index=False)\n",
        "    print(\"üíæ Salvato: 01_leaderboard.csv\")\n",
        "    \n",
        "    top_plot = leaderboard_display.head(8)\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    sns.barplot(data=top_plot, x=\"Model\", y=\"R2\", hue=\"Source\", ax=ax)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_ylabel(\"R¬≤ (test)\")\n",
        "    ax.set_xlabel(\"\")\n",
        "    ax.set_title(\"R¬≤ test per modello\", fontsize=14, fontweight=\"bold\")\n",
        "    ax.grid(True, axis=\"y\", alpha=0.3)\n",
        "    plt.xticks(rotation=30, ha=\"right\")\n",
        "    save_plot(\"02_leaderboard_r2\")\n",
        "    plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Preprocessed data caricati\n",
            "\n",
            "üìä Shapes:\n",
            "   Train: X=(3897, 370), y=(3897,)\n",
            "   Val:   X=(1136, 370), y=(1136,)\n",
            "   Test:  X=(569, 370), y=(569,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Metriche train/test del modello selezionato\n",
        "if not model_metrics:\n",
        "    print(\"‚ö†Ô∏è metrics.json non trovato per il modello selezionato\")\n",
        "else:\n",
        "    metrics_rows = []\n",
        "    for split_name, split_key in [(\"Train\", \"metrics_train\"), (\"Test\", \"metrics_test\")]:\n",
        "        split_metrics = model_metrics.get(split_key, {})\n",
        "        if not split_metrics:\n",
        "            continue\n",
        "        metrics_rows.append(\n",
        "            {\n",
        "                \"Split\": split_name,\n",
        "                \"R2\": split_metrics.get(\"r2\"),\n",
        "                \"RMSE\": split_metrics.get(\"rmse\"),\n",
        "                \"MAE\": split_metrics.get(\"mae\"),\n",
        "                \"MAPE\": split_metrics.get(\"mape\"),\n",
        "                \"MedAE\": split_metrics.get(\"medae\"),\n",
        "            }\n",
        "        )\n",
        "    metrics_df = pd.DataFrame(metrics_rows)\n",
        "    if metrics_df.empty:\n",
        "        print(\"‚ö†Ô∏è Nessuna metrica disponibile nel file metrics.json\")\n",
        "    else:\n",
        "        metrics_df_round = metrics_df.copy()\n",
        "        metrics_df_round[[\"R2\", \"RMSE\", \"MAE\", \"MAPE\", \"MedAE\"]] = metrics_df_round[\n",
        "            [\"R2\", \"RMSE\", \"MAE\", \"MAPE\", \"MedAE\"]\n",
        "        ].round(3)\n",
        "        display(metrics_df_round)\n",
        "        metrics_df_round.to_csv(OUTPUT_DIR / f\"03_{SELECTED_MODEL_KEY}_metrics.csv\", index=False)\n",
        "        print(f\"üíæ Salvato: 03_{SELECTED_MODEL_KEY}_metrics.csv\")\n",
        "        \n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "        sns.barplot(data=metrics_df, x=\"Split\", y=\"RMSE\", ax=axes[0], palette=\"viridis\")\n",
        "        axes[0].set_title(\"RMSE (test vs train)\")\n",
        "        axes[0].grid(True, axis=\"y\", alpha=0.3)\n",
        "        sns.barplot(data=metrics_df, x=\"Split\", y=\"MAE\", ax=axes[1], palette=\"magma\")\n",
        "        axes[1].set_title(\"MAE (test vs train)\")\n",
        "        axes[1].grid(True, axis=\"y\", alpha=0.3)\n",
        "        save_plot(f\"04_{SELECTED_MODEL_KEY}_metrics\")\n",
        "        plt.show()\n",
        "        \n",
        "        best_params = model_metrics.get(\"best_params\", {})\n",
        "        if best_params:\n",
        "            print(\"‚öôÔ∏è Best params:\")\n",
        "            for k, v in best_params.items():\n",
        "                print(f\"   - {k}: {v}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  Best model non trovato, provo con modelli individuali...\n",
            "‚ùå ERRORE: Nessun modello trovato!\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "Nessun modello trovato",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(available_models) == \u001b[32m0\u001b[39m:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ùå ERRORE: Nessun modello trovato!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNessun modello trovato\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mModelli disponibili:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m available_models:\n",
            "\u001b[31mFileNotFoundError\u001b[39m: Nessun modello trovato"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 2. Validation results & stability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Validation results overview\n",
        "if validation_df.empty:\n",
        "    print(\"‚ö†Ô∏è validation_results.csv non trovato\")\n",
        "else:\n",
        "    val_display = validation_df.copy()\n",
        "    val_display[[\"Test_RMSE\", \"Test_R2\"]] = val_display[[\"Test_RMSE\", \"Test_R2\"]].astype(float).round(4)\n",
        "    display(val_display)\n",
        "    val_display.to_csv(OUTPUT_DIR / \"05_validation_results.csv\", index=False)\n",
        "    print(\"üíæ Salvato: 05_validation_results.csv\")\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    sns.scatterplot(\n",
        "        data=validation_df,\n",
        "        x=\"Test_RMSE\",\n",
        "        y=\"Test_R2\",\n",
        "        hue=\"Category\",\n",
        "        s=120,\n",
        "        ax=ax\n",
        "    )\n",
        "    for _, row in validation_df.iterrows():\n",
        "        ax.text(row[\"Test_RMSE\"] + 0.05, row[\"Test_R2\"] + 0.002, row[\"Model\"], fontsize=8)\n",
        "    ax.set_xlabel(\"Test RMSE\")\n",
        "    ax.set_ylabel(\"Test R¬≤\")\n",
        "    ax.set_title(\"Validation results (RMSE vs R¬≤)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    save_plot(\"06_validation_scatter\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Errori per fascia di prezzo (Price band)\n",
        "if price_band_df.empty:\n",
        "    print(\"‚ö†Ô∏è group_metrics_price_band.csv non trovato\")\n",
        "else:\n",
        "    price_band_display = price_band_df.copy()\n",
        "    metric_cols = [\"r2\", \"rmse\", \"mae\", \"mape\", \"medae\"]\n",
        "    price_band_display[metric_cols] = price_band_display[metric_cols].astype(float).round(3)\n",
        "    price_band_display.rename(columns={\"group\": \"Price_Range\", \"count\": \"Count\", \"mape\": \"MAPE\"}, inplace=True)\n",
        "    print(\"üìä Errori per fascia di prezzo (test set)\")\n",
        "    display(price_band_display)\n",
        "    price_band_display.to_csv(OUTPUT_DIR / f\"07_{SELECTED_MODEL_KEY}_price_band.csv\", index=False)\n",
        "    print(f\"üíæ Salvato: 07_{SELECTED_MODEL_KEY}_price_band.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualizzazione errori per fascia di prezzo\n",
        "if price_band_df.empty:\n",
        "    print(\"‚ö†Ô∏è Nessun dato per la fascia di prezzo\")\n",
        "else:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "    order = price_band_df.sort_values(\"rmse\", ascending=False)[\"group\"].tolist()\n",
        "    sns.barplot(data=price_band_df, x=\"group\", y=\"rmse\", ax=axes[0], order=order, color=\"steelblue\")\n",
        "    axes[0].set_ylabel(\"RMSE (‚Ç¨)\")\n",
        "    axes[0].set_xlabel(\"Fascia di prezzo\")\n",
        "    axes[0].set_title(\"RMSE per fascia di prezzo\")\n",
        "    axes[0].tick_params(axis=\"x\", rotation=60)\n",
        "    axes[0].grid(True, axis=\"y\", alpha=0.3)\n",
        "    \n",
        "    sns.barplot(data=price_band_df, x=\"group\", y=\"mape\", ax=axes[1], order=order, color=\"darkorange\")\n",
        "    axes[1].set_ylabel(\"MAPE\")\n",
        "    axes[1].set_xlabel(\"Fascia di prezzo\")\n",
        "    axes[1].set_title(\"MAPE per fascia di prezzo\")\n",
        "    axes[1].tick_params(axis=\"x\", rotation=60)\n",
        "    axes[1].grid(True, axis=\"y\", alpha=0.3)\n",
        "    \n",
        "    plt.suptitle(\"Distribuzione degli errori per fascia di prezzo\", fontsize=16, fontweight=\"bold\")\n",
        "    save_plot(f\"08_{SELECTED_MODEL_KEY}_price_band\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 3. Errori per Zona OMI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Metriche per Zona OMI\n",
        "if zona_metrics_df.empty:\n",
        "    print(\"‚ö†Ô∏è group_metrics_AI_ZonaOmi.csv non trovato\")\n",
        "else:\n",
        "    zona_display = zona_metrics_df.copy()\n",
        "    zona_display.rename(columns={\"group\": \"ZonaOMI\", \"count\": \"Count\", \"mape\": \"MAPE\"}, inplace=True)\n",
        "    zona_display[[\"r2\", \"rmse\", \"mae\", \"MAPE\"]] = zona_display[[\"r2\", \"rmse\", \"mae\", \"MAPE\"]].astype(float).round(3)\n",
        "    zona_display = zona_display.sort_values(\"rmse\", ascending=False)\n",
        "    display(zona_display)\n",
        "    zona_display.to_csv(OUTPUT_DIR / f\"09_{SELECTED_MODEL_KEY}_zona_omi.csv\", index=False)\n",
        "    print(f\"üíæ Salvato: 09_{SELECTED_MODEL_KEY}_zona_omi.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualizzazione Zona OMI\n",
        "if zona_metrics_df.empty:\n",
        "    print(\"‚ö†Ô∏è Nessun dato Zona OMI\")\n",
        "else:\n",
        "    top_zona = zona_metrics_df.sort_values(\"rmse\", ascending=False).head(10)\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.barplot(data=top_zona, x=\"rmse\", y=\"group\", palette=\"crest\", ax=ax)\n",
        "    ax.set_xlabel(\"RMSE (‚Ç¨)\")\n",
        "    ax.set_ylabel(\"Zona OMI\")\n",
        "    ax.set_title(\"Top 10 zone per errore (RMSE)\")\n",
        "    ax.grid(True, axis=\"x\", alpha=0.3)\n",
        "    save_plot(f\"10_{SELECTED_MODEL_KEY}_zona_omi\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Metriche per Tipologia Edilizia\n",
        "if tipologia_metrics_df.empty:\n",
        "    print(\"‚ö†Ô∏è group_metrics_AI_IdTipologiaEdilizia.csv non trovato\")\n",
        "else:\n",
        "    tipologia_display = tipologia_metrics_df.copy()\n",
        "    tipologia_display.rename(columns={\"group\": \"Tipologia\", \"count\": \"Count\", \"mape\": \"MAPE\"}, inplace=True)\n",
        "    tipologia_display[[\"r2\", \"rmse\", \"mae\", \"MAPE\"]] = tipologia_display[[\"r2\", \"rmse\", \"mae\", \"MAPE\"]].astype(float).round(3)\n",
        "    tipologia_display = tipologia_display.sort_values(\"MAPE\", ascending=False)\n",
        "    display(tipologia_display)\n",
        "    tipologia_display.to_csv(OUTPUT_DIR / f\"11_{SELECTED_MODEL_KEY}_tipologia.csv\", index=False)\n",
        "    print(f\"üíæ Salvato: 11_{SELECTED_MODEL_KEY}_tipologia.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 4. Tipologia Edilizia (errori)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualizzazione tipologie\n",
        "if tipologia_metrics_df.empty:\n",
        "    print(\"‚ö†Ô∏è Nessun dato sulla tipologia\")\n",
        "else:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    top_tipologie = tipologia_metrics_df.sort_values(\"mape\", ascending=False).head(12)\n",
        "    sns.barplot(data=top_tipologie, x=\"mape\", y=\"group\", palette=\"flare\", ax=ax)\n",
        "    ax.set_xlabel(\"MAPE\")\n",
        "    ax.set_ylabel(\"Tipologia edilizia\")\n",
        "    ax.set_title(\"Top tipologie per errore percentuale\")\n",
        "    ax.grid(True, axis=\"x\", alpha=0.3)\n",
        "    save_plot(f\"12_{SELECTED_MODEL_KEY}_tipologia\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 5. Categoria Catastale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Metriche per Categoria Catastale\n",
        "if categoria_metrics_df.empty:\n",
        "    print(\"‚ö†Ô∏è group_metrics_AI_IdCategoriaCatastale.csv non trovato\")\n",
        "else:\n",
        "    categoria_display = categoria_metrics_df.copy()\n",
        "    categoria_display.rename(columns={\"group\": \"CategoriaCatastale\", \"count\": \"Count\", \"mape\": \"MAPE\"}, inplace=True)\n",
        "    categoria_display[[\"r2\", \"rmse\", \"mae\", \"MAPE\"]] = categoria_display[[\"r2\", \"rmse\", \"mae\", \"MAPE\"]].astype(float).round(3)\n",
        "    categoria_display = categoria_display.sort_values(\"rmse\", ascending=False)\n",
        "    display(categoria_display)\n",
        "    categoria_display.to_csv(OUTPUT_DIR / f\"13_{SELECTED_MODEL_KEY}_categoria.csv\", index=False)\n",
        "    print(f\"üíæ Salvato: 13_{SELECTED_MODEL_KEY}_categoria.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualizzazione categorie catastali\n",
        "if categoria_metrics_df.empty:\n",
        "    print(\"‚ö†Ô∏è Nessun dato categoria catastale\")\n",
        "else:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    top_categorie = categoria_metrics_df.sort_values(\"rmse\", ascending=False).head(10)\n",
        "    sns.barplot(data=top_categorie, x=\"rmse\", y=\"group\", palette=\"rocket\", ax=ax)\n",
        "    ax.set_xlabel(\"RMSE (‚Ç¨)\")\n",
        "    ax.set_ylabel(\"Categoria catastale\")\n",
        "    ax.set_title(\"Categorie con errore pi√π elevato\")\n",
        "    ax.grid(True, axis=\"x\", alpha=0.3)\n",
        "    save_plot(f\"14_{SELECTED_MODEL_KEY}_categoria\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 6. Worst Predictions Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Worst predictions (gi√† salvate in fase di training)\n",
        "if worst_predictions_df.empty:\n",
        "    print(\"‚ö†Ô∏è Nessun file *_worst_predictions.csv trovato per il modello selezionato\")\n",
        "else:\n",
        "    worst_display = worst_predictions_df.copy()\n",
        "    worst_display.rename(\n",
        "        columns={\"true\": \"Actual\", \"predicted\": \"Predicted\", \"residual\": \"Residual\", \"abs_residual\": \"Abs_Error\", \"pct_error\": \"Pct_Error\"},\n",
        "        inplace=True,\n",
        "    )\n",
        "    worst_display[[\"Actual\", \"Predicted\", \"Residual\", \"Abs_Error\"]] = worst_display[\n",
        "        [\"Actual\", \"Predicted\", \"Residual\", \"Abs_Error\"]\n",
        "    ].astype(float).round(2)\n",
        "    worst_display[\"Pct_Error\"] = worst_display[\"Pct_Error\"].astype(float).round(2)\n",
        "    display(worst_display.head(20))\n",
        "    worst_display.to_csv(OUTPUT_DIR / f\"15_{SELECTED_MODEL_KEY}_worst_predictions.csv\", index=False)\n",
        "    print(f\"üíæ Salvato: 15_{SELECTED_MODEL_KEY}_worst_predictions.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 7. Prediction Intervals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prediction intervals (coverage vs target)\n",
        "if not prediction_intervals:\n",
        "    print(\"‚ö†Ô∏è File *_prediction_intervals.json non trovato o vuoto\")\n",
        "else:\n",
        "    interval_rows = []\n",
        "    for level, stats_dict in prediction_intervals.items():\n",
        "        interval_rows.append(\n",
        "            {\n",
        "                \"Interval\": level,\n",
        "                \"Target\": stats_dict.get(\"target_coverage\"),\n",
        "                \"Coverage\": stats_dict.get(\"coverage\"),\n",
        "                \"AvgWidth‚Ç¨\": stats_dict.get(\"average_width\"),\n",
        "                \"AvgWidth%\": stats_dict.get(\"average_width_pct\"),\n",
        "            }\n",
        "        )\n",
        "    interval_df = pd.DataFrame(interval_rows).sort_values(\"Target\")\n",
        "    interval_df[[\"Target\", \"Coverage\"]] = interval_df[[\"Target\", \"Coverage\"]].round(3)\n",
        "    interval_df[[\"AvgWidth‚Ç¨\", \"AvgWidth%\"]] = interval_df[[\"AvgWidth‚Ç¨\", \"AvgWidth%\"]].round(2)\n",
        "    display(interval_df)\n",
        "    interval_df.to_csv(OUTPUT_DIR / f\"16_{SELECTED_MODEL_KEY}_prediction_intervals.csv\", index=False)\n",
        "    print(f\"üíæ Salvato: 16_{SELECTED_MODEL_KEY}_prediction_intervals.csv\")\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    ax.plot(interval_df[\"Target\"], interval_df[\"Target\"], \"k--\", label=\"Target\")\n",
        "    ax.plot(interval_df[\"Target\"], interval_df[\"Coverage\"], marker=\"o\", label=\"Coverage\")\n",
        "    ax.set_xlabel(\"Target coverage\")\n",
        "    ax.set_ylabel(\"Observed coverage\")\n",
        "    ax.set_title(\"Prediction interval coverage\")\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    save_plot(f\"17_{SELECTED_MODEL_KEY}_prediction_intervals\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã 8. Summary Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Report finale (aggregato dagli artefatti)\n",
        "\n",
        "def top_records(df, sort_col, n=3):\n",
        "    if df is None or df.empty:\n",
        "        return []\n",
        "    return df.sort_values(sort_col, ascending=False).head(n).to_dict(\"records\")\n",
        "\n",
        "\n",
        "summary_report = {\n",
        "    \"selected_model\": SELECTED_MODEL_KEY,\n",
        "    \"available_models\": available_model_dirs,\n",
        "    \"leaderboard_top3\": leaderboard_df.head(3).to_dict(\"records\") if not leaderboard_df.empty else [],\n",
        "    \"metrics\": {\n",
        "        \"train\": model_metrics.get(\"metrics_train\", {}),\n",
        "        \"test\": model_metrics.get(\"metrics_test\", {}),\n",
        "        \"best_params\": model_metrics.get(\"best_params\", {}),\n",
        "    },\n",
        "    \"validation_rows\": int(len(validation_df)),\n",
        "    \"segments\": {\n",
        "        \"price_band_high_mape\": top_records(price_band_df, \"mape\"),\n",
        "        \"zona_high_rmse\": top_records(zona_metrics_df, \"rmse\"),\n",
        "        \"tipologia_high_mape\": top_records(tipologia_metrics_df, \"mape\"),\n",
        "        \"categoria_high_rmse\": top_records(categoria_metrics_df, \"rmse\"),\n",
        "    },\n",
        "    \"prediction_intervals\": prediction_intervals,\n",
        "    \"worst_prediction_sample\": worst_predictions_df.head(1).to_dict(\"records\")[0]\n",
        "    if not worst_predictions_df.empty\n",
        "    else None,\n",
        "}\n",
        "\n",
        "summary_path = OUTPUT_DIR / f\"18_{SELECTED_MODEL_KEY}_summary_report.json\"\n",
        "with open(summary_path, \"w\") as f:\n",
        "    json.dump(summary_report, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìã FINAL REPORT\")\n",
        "print(\"=\" * 80)\n",
        "print(json.dumps(summary_report, indent=2))\n",
        "print(f\"\\nüíæ Salvato: {summary_path.name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Conclusioni\n",
        "\n",
        "### File generati\n",
        "\n",
        "- `01_leaderboard.csv` / `02_leaderboard_r2.png`: confronto rapido di tutti i modelli\n",
        "- `03_<model>_metrics.csv` / `04_<model>_metrics.png`: metriche train/test del modello scelto\n",
        "- `05_validation_results.csv` / `06_validation_scatter.png`: performance sulle run di validazione\n",
        "- `07-14_*` CSV/PNG: segmentazioni per fascia prezzo, Zona OMI, Tipologia, Categoria catastale\n",
        "- `15_<model>_worst_predictions.csv`: elenco dei casi pi√π critici con residuo e % errore\n",
        "- `16_<model>_prediction_intervals.csv` / `17_<model>_prediction_intervals.png`: copertura intervalli\n",
        "- `18_<model>_summary_report.json`: riepilogo finale (metriche, segmentazioni, intervalli)\n",
        "\n",
        "### Key insights\n",
        "\n",
        "- **Leaderboard**: il modello selezionato √® quello con R¬≤ pi√π alto nelle metriche salvate; le alternative sono comunque disponibili per confronto rapido\n",
        "- **Segmentazioni**: le fasce di prezzo e le categorie catastali con RMSE/MAPE pi√π elevati emergono subito dai CSV dedicati\n",
        "- **Prediction intervals**: il grafico mette in evidenza l‚Äôeventuale under/over-coverage rispetto al target (80%/90%)\n",
        "- **Worst predictions**: disponibile un campione pronto per debugging qualitativo o per creare casi di studio\n",
        "\n",
        "### Next steps\n",
        "\n",
        "1. Concentrarsi sulle fasce/categorie con MAPE pi√π alto per capire possibili feature aggiuntive\n",
        "2. Se la copertura degli intervalli √® bassa, aumentare `n_bootstraps` o estendere il livello di confidenza\n",
        "3. Usare il file dei worst predictions per effettuare controlli di qualit√† sui dati sorgente o sulle etichette\n",
        "4. Condividere i grafici/CSV generati con il team business per raccogliere feedback mirati"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "stimatrix",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}