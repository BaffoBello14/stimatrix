{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Model Results Deep Analysis\n",
    "\n",
    "**Obiettivo**: Analisi approfondita dei risultati dei modelli trainati.\n",
    "\n",
    "**Prerequisiti**: Aver eseguito `python main.py --config config/config.yaml --steps train`\n",
    "\n",
    "**Analisi**:\n",
    "1. **Performance Metrics**: MAE, RMSE, MAPE, R¬≤ per train/test\n",
    "2. **Model Comparison**: Confronto tra modelli individuali e ensemble\n",
    "3. **Overfitting Analysis**: Gap metrics train-test\n",
    "4. **Group Performance**: Errori per categoria catastale, zona OMI, tipologia\n",
    "5. **Worst Predictions**: Analisi predizioni peggiori\n",
    "6. **Residual Plots**: Distribuzione residui dai grafici salvati\n",
    "7. **Prediction Intervals**: Coverage e larghezza intervalli\n",
    "\n",
    "**Output**: `model_analysis_outputs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import json\n",
    "import warnings\n",
    "from IPython.display import Image, display\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Setup completato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione\n",
    "MODELS_DIR = Path(\"../models\")\n",
    "SUMMARY_PATH = MODELS_DIR / \"summary.json\"\n",
    "VALIDATION_PATH = MODELS_DIR / \"validation_results.csv\"\n",
    "OUTPUT_DIR = Path(\"model_analysis_outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def save_plot(name, dpi=120):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / f\"{name}.png\", dpi=dpi, bbox_inches='tight')\n",
    "    print(f\"üíæ Salvato: {name}.png\")\n",
    "\n",
    "print(f\"üìÇ Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"üìÇ Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 1. Load Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check che i risultati esistano\n",
    "if not SUMMARY_PATH.exists():\n",
    "    print(\"‚ùå ERRORE: Results non trovati!\")\n",
    "    print(\"\\nEsegui prima il training:\")\n",
    "    print(\"  python main.py --config config/config.yaml --steps train\")\n",
    "    raise FileNotFoundError(f\"Results non trovati: {SUMMARY_PATH}\")\n",
    "\n",
    "# Load summary\n",
    "with open(SUMMARY_PATH, 'r') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "print(\"‚úÖ Results summary caricato\")\n",
    "print(f\"\\nüìä Modelli disponibili: {list(summary['models'].keys())}\")\n",
    "print(f\"üìä Ensemble disponibili: {list(summary.get('ensembles', {}).keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 2. Performance Comparison - All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrai metriche per tutti i modelli\n",
    "models_metrics = []\n",
    "\n",
    "# Modelli individuali\n",
    "for model_name, model_data in summary['models'].items():\n",
    "    test_metrics = model_data.get('metrics_test_original', {})\n",
    "    train_metrics = model_data.get('metrics_train_original', {})\n",
    "    overfit = model_data.get('overfit', {})\n",
    "    \n",
    "    models_metrics.append({\n",
    "        'Model': model_name.upper(),\n",
    "        'Type': 'Individual',\n",
    "        'Test_R2': test_metrics.get('r2', 0),\n",
    "        'Test_MAE': test_metrics.get('mae', 0),\n",
    "        'Test_RMSE': test_metrics.get('rmse', 0),\n",
    "        'Test_MAPE': test_metrics.get('mape', 0),\n",
    "        'Train_R2': train_metrics.get('r2', 0),\n",
    "        'Train_MAE': train_metrics.get('mae', 0),\n",
    "        'Overfit_Gap_R2': overfit.get('gap_r2', 0),\n",
    "    })\n",
    "\n",
    "# Ensemble\n",
    "for ensemble_name, ensemble_data in summary.get('ensembles', {}).items():\n",
    "    test_metrics = ensemble_data.get('metrics_test_original', {})\n",
    "    train_metrics = ensemble_data.get('metrics_train_original', {})\n",
    "    overfit = ensemble_data.get('overfit', {})\n",
    "    \n",
    "    models_metrics.append({\n",
    "        'Model': ensemble_name.upper(),\n",
    "        'Type': 'Ensemble',\n",
    "        'Test_R2': test_metrics.get('r2', 0),\n",
    "        'Test_MAE': test_metrics.get('mae', 0),\n",
    "        'Test_RMSE': test_metrics.get('rmse', 0),\n",
    "        'Test_MAPE': test_metrics.get('mape', 0),\n",
    "        'Train_R2': train_metrics.get('r2', 0),\n",
    "        'Train_MAE': train_metrics.get('mae', 0),\n",
    "        'Overfit_Gap_R2': overfit.get('gap_r2', 0),\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(models_metrics).sort_values('Test_R2', ascending=False)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\n\", metrics_df.to_string(index=False))\n",
    "\n",
    "# Salva\n",
    "metrics_df.to_csv(OUTPUT_DIR / \"01_model_comparison.csv\", index=False)\n",
    "print(f\"\\nüíæ Salvato: 01_model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Colors by type\n",
    "colors = ['steelblue' if t == 'Individual' else 'orange' for t in metrics_df['Type']]\n",
    "\n",
    "# R2\n",
    "ax = axes[0, 0]\n",
    "ax.barh(metrics_df['Model'], metrics_df['Test_R2'], color=colors, edgecolor='black')\n",
    "ax.set_xlabel('R¬≤ Score')\n",
    "ax.set_title('Test R¬≤ by Model')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# MAE\n",
    "ax = axes[0, 1]\n",
    "ax.barh(metrics_df['Model'], metrics_df['Test_MAE'], color=colors, edgecolor='black')\n",
    "ax.set_xlabel('MAE (‚Ç¨)')\n",
    "ax.set_title('Test MAE by Model')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# RMSE\n",
    "ax = axes[1, 0]\n",
    "ax.barh(metrics_df['Model'], metrics_df['Test_RMSE'], color=colors, edgecolor='black')\n",
    "ax.set_xlabel('RMSE (‚Ç¨)')\n",
    "ax.set_title('Test RMSE by Model')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# MAPE\n",
    "ax = axes[1, 1]\n",
    "ax.barh(metrics_df['Model'], metrics_df['Test_MAPE'] * 100, color=colors, edgecolor='black')\n",
    "ax.set_xlabel('MAPE (%)')\n",
    "ax.set_title('Test MAPE by Model')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "save_plot(\"02_model_comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 3. Overfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train vs Test comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# R2 comparison\n",
    "ax = axes[0]\n",
    "x = np.arange(len(metrics_df))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, metrics_df['Train_R2'], width, label='Train', color='lightgreen', edgecolor='black')\n",
    "ax.bar(x + width/2, metrics_df['Test_R2'], width, label='Test', color='steelblue', edgecolor='black')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_df['Model'], rotation=45, ha='right')\n",
    "ax.set_ylabel('R¬≤ Score')\n",
    "ax.set_title('Train vs Test R¬≤')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Overfitting gap\n",
    "ax = axes[1]\n",
    "colors_overfit = ['red' if gap > 0.1 else 'orange' if gap > 0.05 else 'green' \n",
    "                  for gap in metrics_df['Overfit_Gap_R2']]\n",
    "ax.bar(metrics_df['Model'], metrics_df['Overfit_Gap_R2'], color=colors_overfit, edgecolor='black')\n",
    "ax.axhline(0.05, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='Moderate (0.05)')\n",
    "ax.axhline(0.10, color='red', linestyle='--', linewidth=2, alpha=0.5, label='High (0.10)')\n",
    "ax.set_xticklabels(metrics_df['Model'], rotation=45, ha='right')\n",
    "ax.set_ylabel('Gap R¬≤ (Train - Test)')\n",
    "ax.set_title('Overfitting Gap')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Overfitting Analysis', fontsize=16, fontweight='bold')\n",
    "save_plot(\"03_overfitting_analysis\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Overfitting Interpretation:\")\n",
    "print(\"  üü¢ Gap < 0.05: Good generalization\")\n",
    "print(\"  üü† Gap 0.05-0.10: Moderate overfitting\")\n",
    "print(\"  üî¥ Gap > 0.10: High overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 4. Group Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trova tutti i model directories\n",
    "model_dirs = [d for d in MODELS_DIR.iterdir() if d.is_dir() and d.name not in ['__pycache__']]\n",
    "\n",
    "print(f\"üìÇ Model directories found: {[d.name for d in model_dirs]}\")\n",
    "\n",
    "# Analizza il miglior modello (primo in classifica)\n",
    "best_model = metrics_df.iloc[0]['Model'].lower()\n",
    "best_model_dir = MODELS_DIR / best_model\n",
    "\n",
    "print(f\"\\nüèÜ Analyzing best model: {best_model.upper()}\")\n",
    "\n",
    "# Cerca group metrics\n",
    "group_files = {\n",
    "    'Categoria Catastale': best_model_dir / 'group_metrics_AI_IdCategoriaCatastale.csv',\n",
    "    'Tipologia Edilizia': best_model_dir / 'group_metrics_AI_IdTipologiaEdilizia.csv',\n",
    "    'Zona OMI': best_model_dir / 'group_metrics_AI_ZonaOmi.csv',\n",
    "    'Price Band': best_model_dir / 'group_metrics_price_band.csv',\n",
    "}\n",
    "\n",
    "available_groups = {k: v for k, v in group_files.items() if v.exists()}\n",
    "\n",
    "if not available_groups:\n",
    "    print(\"‚ö†Ô∏è  Nessun group metrics trovato per il modello\")\n",
    "else:\n",
    "    print(f\"\\nüìä Group metrics disponibili: {list(available_groups.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize group performance\n",
    "if available_groups:\n",
    "    n_groups = len(available_groups)\n",
    "    fig, axes = plt.subplots(n_groups, 2, figsize=(16, 5 * n_groups))\n",
    "    \n",
    "    if n_groups == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, (group_name, group_file) in enumerate(available_groups.items()):\n",
    "        df_group = pd.read_csv(group_file)\n",
    "        \n",
    "        # Prendi prime 10 categorie per MAPE\n",
    "        df_group_sorted = df_group.sort_values('test_mape', ascending=False).head(10)\n",
    "        \n",
    "        # MAPE\n",
    "        ax = axes[idx, 0]\n",
    "        ax.barh(df_group_sorted.iloc[:, 0].astype(str), df_group_sorted['test_mape'] * 100, \n",
    "                color='coral', edgecolor='black')\n",
    "        ax.set_xlabel('MAPE (%)')\n",
    "        ax.set_title(f'{group_name} - Top 10 MAPE (Worst)')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "        # Count\n",
    "        ax = axes[idx, 1]\n",
    "        df_group_count = df_group.sort_values('count', ascending=False).head(10)\n",
    "        ax.barh(df_group_count.iloc[:, 0].astype(str), df_group_count['count'], \n",
    "                color='skyblue', edgecolor='black')\n",
    "        ax.set_xlabel('Count')\n",
    "        ax.set_title(f'{group_name} - Sample Distribution')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        ax.invert_yaxis()\n",
    "    \n",
    "    plt.suptitle(f'Group Performance Analysis - {best_model.upper()}', fontsize=16, fontweight='bold')\n",
    "    save_plot(\"04_group_performance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 5. Worst Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load worst predictions\n",
    "worst_pred_file = best_model_dir / f\"{best_model}_worst_predictions.csv\"\n",
    "\n",
    "if worst_pred_file.exists():\n",
    "    worst_df = pd.read_csv(worst_pred_file)\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(f\"WORST PREDICTIONS - {best_model.upper()}\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"\\n\", worst_df.head(20).to_string(index=False))\n",
    "    \n",
    "    # Salva\n",
    "    worst_df.to_csv(OUTPUT_DIR / \"05_worst_predictions.csv\", index=False)\n",
    "    print(f\"\\nüíæ Salvato: 05_worst_predictions.csv\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    top20 = worst_df.head(20)\n",
    "    x = range(len(top20))\n",
    "    \n",
    "    ax.scatter(x, top20['y_true'], color='green', s=100, alpha=0.6, label='Actual', marker='o')\n",
    "    ax.scatter(x, top20['y_pred'], color='red', s=100, alpha=0.6, label='Predicted', marker='x')\n",
    "    \n",
    "    for i in x:\n",
    "        ax.plot([i, i], [top20.iloc[i]['y_true'], top20.iloc[i]['y_pred']], 'k--', alpha=0.3)\n",
    "    \n",
    "    ax.set_xlabel('Prediction Index (sorted by error)')\n",
    "    ax.set_ylabel('Price (‚Ç¨)')\n",
    "    ax.set_title('Top 20 Worst Predictions: Actual vs Predicted')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    save_plot(\"06_worst_predictions\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Worst predictions file non trovato: {worst_pred_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 6. Residual Plots (from saved images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show residual plots\n",
    "residual_dir = best_model_dir / f\"{best_model}_residual_plots\"\n",
    "\n",
    "if residual_dir.exists():\n",
    "    residual_plots = sorted(residual_dir.glob(\"*.png\"))\n",
    "    \n",
    "    print(f\"üìä Residual plots disponibili: {len(residual_plots)}\")\n",
    "    \n",
    "    for plot_path in residual_plots:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìà {plot_path.stem.replace('_', ' ').title()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        display(Image(filename=str(plot_path)))\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Residual plots non trovati: {residual_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 7. Prediction Intervals Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction intervals\n",
    "pred_intervals_file = best_model_dir / f\"{best_model}_prediction_intervals.json\"\n",
    "\n",
    "if pred_intervals_file.exists():\n",
    "    with open(pred_intervals_file, 'r') as f:\n",
    "        pred_intervals = json.load(f)\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(f\"PREDICTION INTERVALS - {best_model.upper()}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Converti in DataFrame\n",
    "    intervals_data = []\n",
    "    for level, metrics in pred_intervals.items():\n",
    "        intervals_data.append({\n",
    "            'Confidence_Level': level,\n",
    "            'Target_Coverage': metrics['target_coverage'],\n",
    "            'Actual_Coverage': metrics['coverage'],\n",
    "            'Coverage_Gap': metrics['coverage'] - metrics['target_coverage'],\n",
    "            'Avg_Width_EUR': metrics['average_width'],\n",
    "            'Avg_Width_PCT': metrics['average_width_pct'],\n",
    "        })\n",
    "    \n",
    "    intervals_df = pd.DataFrame(intervals_data)\n",
    "    print(\"\\n\", intervals_df.to_string(index=False))\n",
    "    \n",
    "    # Salva\n",
    "    intervals_df.to_csv(OUTPUT_DIR / \"07_prediction_intervals.csv\", index=False)\n",
    "    print(f\"\\nüíæ Salvato: 07_prediction_intervals.csv\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Coverage\n",
    "    ax = axes[0]\n",
    "    x = range(len(intervals_df))\n",
    "    width = 0.35\n",
    "    ax.bar([i - width/2 for i in x], intervals_df['Target_Coverage'], width, \n",
    "           label='Target', color='lightgreen', edgecolor='black')\n",
    "    ax.bar([i + width/2 for i in x], intervals_df['Actual_Coverage'], width, \n",
    "           label='Actual', color='steelblue', edgecolor='black')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(intervals_df['Confidence_Level'])\n",
    "    ax.set_ylabel('Coverage')\n",
    "    ax.set_title('Prediction Interval Coverage')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Width\n",
    "    ax = axes[1]\n",
    "    ax.bar(intervals_df['Confidence_Level'], intervals_df['Avg_Width_EUR'], \n",
    "           color='coral', edgecolor='black')\n",
    "    ax.set_ylabel('Average Width (‚Ç¨)')\n",
    "    ax.set_title('Prediction Interval Width')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle(f'Prediction Intervals Analysis - {best_model.upper()}', fontsize=16, fontweight='bold')\n",
    "    save_plot(\"08_prediction_intervals\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Interpretation:\")\n",
    "    print(\"  - Coverage: % di osservazioni reali che cadono nell'intervallo\")\n",
    "    print(\"  - Coverage Gap: differenza tra coverage attuale e target (idealmente ~0)\")\n",
    "    print(\"  - Avg Width: larghezza media dell'intervallo in euro\")\n",
    "    print(\"  - Avg Width %: larghezza media in percentuale rispetto al prezzo\")\n",
    "    \n",
    "    # Diagnostics\n",
    "    print(\"\\nüîç Diagnostics:\")\n",
    "    for _, row in intervals_df.iterrows():\n",
    "        level = row['Confidence_Level']\n",
    "        gap = row['Coverage_Gap']\n",
    "        if abs(gap) < 0.02:\n",
    "            status = \"üü¢ Well calibrated\"\n",
    "        elif gap < -0.05:\n",
    "            status = \"üî¥ Under-coverage (intervallo troppo stretto)\"\n",
    "        elif gap > 0.05:\n",
    "            status = \"üü† Over-coverage (intervallo troppo largo)\"\n",
    "        else:\n",
    "            status = \"üü° Acceptable\"\n",
    "        \n",
    "        print(f\"  {level}: {status} (gap={gap:.4f})\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Prediction intervals file non trovato: {pred_intervals_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã 8. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final report\n",
    "report = {\n",
    "    'best_model': best_model.upper(),\n",
    "    'best_model_metrics': {\n",
    "        'test_r2': float(metrics_df.iloc[0]['Test_R2']),\n",
    "        'test_mae': float(metrics_df.iloc[0]['Test_MAE']),\n",
    "        'test_rmse': float(metrics_df.iloc[0]['Test_RMSE']),\n",
    "        'test_mape': float(metrics_df.iloc[0]['Test_MAPE']),\n",
    "        'overfitting_gap': float(metrics_df.iloc[0]['Overfit_Gap_R2']),\n",
    "    },\n",
    "    'models_compared': len(metrics_df),\n",
    "    'group_metrics_available': list(available_groups.keys()) if available_groups else [],\n",
    "}\n",
    "\n",
    "# Add prediction intervals if available\n",
    "if pred_intervals_file.exists():\n",
    "    report['prediction_intervals'] = pred_intervals\n",
    "\n",
    "# Salva JSON\n",
    "with open(OUTPUT_DIR / \"00_analysis_summary.json\", 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìã ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(json.dumps(report, indent=2))\n",
    "print(f\"\\nüíæ Salvato: 00_analysis_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusioni\n",
    "\n",
    "### File Generati\n",
    "\n",
    "1. `00_analysis_summary.json` - Report completo\n",
    "2. `01_model_comparison.csv` - Confronto metriche tra modelli\n",
    "3. `02_model_comparison.png` - Grafici confronto modelli\n",
    "4. `03_overfitting_analysis.png` - Analisi overfitting\n",
    "5. `04_group_performance.png` - Performance per gruppo\n",
    "6. `05_worst_predictions.csv` - Top worst predictions\n",
    "7. `06_worst_predictions.png` - Grafico worst predictions\n",
    "8. `07_prediction_intervals.csv` - Analisi intervalli predizione\n",
    "9. `08_prediction_intervals.png` - Grafici intervalli\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Best Model**: Modello con R¬≤ test pi√π alto\n",
    "- **Overfitting**: Gap train-test indica generalizzazione\n",
    "- **Group Performance**: Identifica categorie problematiche\n",
    "- **Prediction Intervals**: Quantifica incertezza del modello\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Se overfitting alto (gap > 0.10): aumenta regularization\n",
    "2. Se coverage intervals bassa: rivedi calibrazione\n",
    "3. Se errori alti su gruppi specifici: feature engineering mirato\n",
    "4. Analizza worst predictions per pattern comuni"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stimatrix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
