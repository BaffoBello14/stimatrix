# Riepilogo finale
import os
outs = sorted(os.listdir(output_dir))
print('\n' + '='*60)
print('üéâ OUTPUT GENERATI')
print('='*60)
for f in outs:
    print('‚Ä¢', f)
print('\n‚úÖ EDA esteso completato.')# Check geospaziale rapido
candidates = []
geo_patterns = ['wkt', 'geometry', 'geom', 'geojson', 'the_geom', 'shape', 'polygon', 'point', 'linestring',
                'lat', 'latitude', 'lon', 'lng', 'longitude', 'coord', 'x_', 'y_', 'easting', 'northing',
                'geo', 'spatial', 'location', 'posizione', 'indirizzo', 'address']
for col in df.columns:
    col_l = col.lower()
    for pat in geo_patterns:
        if pat in col_l:
            sample_vals = df[col].dropna().head(3).tolist()
            candidates.append({
                'Colonna': col,
                'Tipo': str(df[col].dtype),
                'Pattern': pat,
                'NonNull_Count': int(df[col].count()),
                'Esempi': str(sample_vals)[:100]
            })
            break
geo_df = pd.DataFrame(candidates).drop_duplicates(subset=['Colonna', 'Pattern']) if candidates else pd.DataFrame()
if not geo_df.empty:
    geo_df.to_csv(output_dir / 'geospatial_columns_check.csv', index=False)
    print(f"üíæ Salvato check geospaziale: {output_dir/'geospatial_columns_check.csv'}")
else:
    print('Nessuna colonna geospaziale candidata individuata')# Matrice cat-num (eta-squared) opzionale
cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
if len(cat_cols) > 0 and len(num_cols) > 0 and len(cat_cols) <= 40:
    eta_mat = pd.DataFrame(index=cat_cols, columns=num_cols, dtype=float)
    for c in cat_cols:
        for n in num_cols:
            eta_mat.loc[c, n] = assoc_categorical_numeric(df[c], df[n])['eta2']
    eta_mat.to_csv(output_dir / 'matrix_catnum_eta2.csv')
    print('üíæ Salvata matrice cat-num eta2')
elif len(cat_cols) > 40:
    print('‚ÑπÔ∏è Troppe categoriche per eta2 completo (>', len(cat_cols), ') ‚Äî skip')# Matrici di associazione (feature-feature)
# Numeriche: Pearson e Spearman
num_df = df.select_dtypes(include=[np.number])
if num_df.shape[1] >= 2:
    pear = num_df.corr(method='pearson')
    spear = num_df.corr(method='spearman')
    pear.to_csv(output_dir / 'matrix_numeric_pearson.csv')
    spear.to_csv(output_dir / 'matrix_numeric_spearman.csv')
    print('üíæ Salvate matrici numeriche Pearson/Spearman')

# Categoriali: Cram√©r's V (limitato per performance)
cat_df = df.select_dtypes(exclude=[np.number])
if cat_df.shape[1] >= 2 and cat_df.shape[1] <= 30:
    cols = cat_df.columns.tolist()
    m = pd.DataFrame(np.nan, index=cols, columns=cols)
    for i, c1 in enumerate(cols):
        for j, c2 in enumerate(cols):
            if j < i:
                continue
            v = assoc_categorical_categorical(cat_df[c1], cat_df[c2])['cramers_v']
            m.loc[c1, c2] = v
            m.loc[c2, c1] = v
    m.to_csv(output_dir / 'matrix_categorical_cramers_v.csv')
    print("üíæ Salvata matrice categoriche Cram√©r's V")
elif cat_df.shape[1] > 30:
    print('‚ÑπÔ∏è Troppe categoriche per Cram√©r\'s V completo (>', cat_df.shape[1], ') ‚Äî skip')# Associazioni col target (numeriche e categoriche)
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()

for target in candidate_targets:
    if target not in df.columns:
        continue
    # Numeriche
    assoc_rows = []
    for col in [c for c in num_cols if c != target]:
        res = assoc_numeric_numeric(df[col], df[target])
        assoc_rows.append({'feature': col, **res})
    assoc_num_df = pd.DataFrame(assoc_rows).sort_values(by='pearson', key=lambda s: s.abs(), ascending=False)
    assoc_num_df.to_csv(output_dir / f'associations_numeric_with_target__{target}.csv', index=False)
    print(f'üíæ Salvate associazioni numeriche col target: associations_numeric_with_target__{target}.csv')
    # Categoriche
    assoc_c_rows = []
    for col in cat_cols:
        res = assoc_categorical_numeric(df[col], df[target])
        assoc_c_rows.append({'feature': col, **res, 'nunique': df[col].nunique()})
    assoc_cat_df = pd.DataFrame(assoc_c_rows).sort_values(by='eta2', ascending=False)
    assoc_cat_df.to_csv(output_dir / f'associations_categorical_with_target__{target}.csv', index=False)
    print(f'üíæ Salvate associazioni categoriche col target: associations_categorical_with_target__{target}.csv')# Distribuzione target multi-target (statistiche + quantili + log)
for target in candidate_targets:
    if target not in df.columns:
        continue
    t = df[target].dropna()
    if t.empty:
        continue
    desc = t.describe()
    print('\n' + '-'*50)
    print(f'üéØ Target: {target}')
    print(desc)
    quantiles = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
    bands = t.quantile(quantiles)
    bands.to_csv(output_dir / f'target_quantiles__{target}.csv')
    tlog = np.log1p(t)
    log_stats = pd.Series({
        'mean_log': tlog.mean(),
        'median_log': tlog.median(),
        'std_log': tlog.std()
    })
    log_stats.to_csv(output_dir / f'target_log_stats__{target}.csv')
print('‚úÖ Distribuzioni target salvate (quantili/log stats)')# Missingness
missing_stats = pd.DataFrame({
    'Totale_Null': df.isnull().sum(),
    'Percentuale_Null': (df.isnull().mean() * 100).round(2),
    'Tipo': df.dtypes
})
missing_stats.to_csv(output_dir / 'missingness_analysis.csv')
print(f"üíæ Salvata missingness in: {output_dir/'missingness_analysis.csv'}")

high_missing = missing_stats[missing_stats['Percentuale_Null'] > 50].sort_values('Percentuale_Null', ascending=False)
print(f"‚ö†Ô∏è Colonne con >50% NaN: {len(high_missing)}")# Overview dataset
memory_mb = df.memory_usage(deep=True).sum() / 1024**2
print('='*60)
print('üìã OVERVIEW DATASET')
print('='*60)
print(f'üìä Dimensioni: {df.shape[0]:,} righe √ó {df.shape[1]} colonne')
print(f'üíæ Memoria utilizzata: {memory_mb:.2f} MB')
print('\nüìã Tipi di dati:')
dtype_counts = df.dtypes.value_counts()
for dtype, count in dtype_counts.items():
    print(f'  {dtype}: {count} colonne')# Helper: funzioni per misure di associazione
import numpy as np
import pandas as pd
from scipy.stats import pearsonr, spearmanr, kendalltau, pointbiserialr
from sklearn.metrics import mutual_info_regression

def _dropna_pair(x: pd.Series, y: pd.Series):
    mask = x.notna() & y.notna()
    return x[mask].to_numpy(), y[mask].to_numpy()

def assoc_numeric_numeric(x: pd.Series, y: pd.Series) -> dict:
    xv, yv = _dropna_pair(x, y)
    if len(xv) < 3:
        return {'pearson': np.nan, 'spearman': np.nan, 'kendall': np.nan, 'mi': np.nan}
    pr = pearsonr(xv, yv)[0]
    sr = spearmanr(xv, yv)[0]
    kr = kendalltau(xv, yv)[0]
    try:
        mi = mutual_info_regression(xv.reshape(-1, 1), yv, random_state=42)
        mi_val = float(mi[0])
    except Exception:
        mi_val = np.nan
    return {'pearson': pr, 'spearman': sr, 'kendall': kr, 'mi': mi_val}

def assoc_categorical_categorical(x: pd.Series, y: pd.Series) -> dict:
    from scipy.stats import chi2_contingency
    xt = pd.crosstab(x, y)
    if xt.shape[0] < 2 or xt.shape[1] < 2:
        return {'cramers_v': np.nan}
    chi2, _, _, _ = chi2_contingency(xt)
    n = xt.values.sum()
    phi2 = chi2 / n
    r, k = xt.shape
    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))
    rcorr = r - ((r - 1) ** 2) / (n - 1)
    kcorr = k - ((k - 1) ** 2) / (n - 1)
    denom = min(kcorr - 1, rcorr - 1)
    if denom <= 0:
        return {'cramers_v': np.nan}
    v = np.sqrt(phi2corr / denom)
    return {'cramers_v': float(v)}

def assoc_categorical_numeric(cat: pd.Series, num: pd.Series) -> dict:
    cv = cat.astype('category')
    groups = [num[cv == c].dropna().values for c in cv.cat.categories]
    if len(groups) < 2 or sum(len(g) for g in groups) < 3:
        eta2 = np.nan
    else:
        overall_mean = num.dropna().mean()
        ss_between = sum(len(g) * (g.mean() - overall_mean) ** 2 for g in groups if len(g) > 0)
        ss_total = ((num.dropna() - overall_mean) ** 2).sum()
        eta2 = float(ss_between / ss_total) if ss_total > 0 else np.nan
    try:
        if cv.nunique() == 2:
            xbin = (cv.cat.codes == cv.cat.categories.tolist().index(cv.cat.categories[1])).astype(int)
            xb, yv = _dropna_pair(pd.Series(xbin, index=cv.index), num)
            pbr = pointbiserialr(xb, yv)[0] if len(xb) >= 3 else np.nan
        else:
            pbr = np.nan
    except Exception:
        pbr = np.nan
    return {'eta2': eta2, 'point_biserial': pbr}# Percorsi e caricamento config/dati
output_dir = Path('eda_outputs') / 'extended'
output_dir.mkdir(parents=True, exist_ok=True)
print(f'üìÅ Directory output: {output_dir}')

config_path = '../config/config.yaml'
with open(config_path, 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)
print(f'‚úÖ Config caricata da: {config_path}')

data_path = '../data/raw/raw.parquet'
df = pd.read_parquet(data_path)
print(f'‚úÖ Dataset caricato: {df.shape[0]:,} righe √ó {df.shape[1]} colonne')

candidate_targets = list(dict.fromkeys(config.get('target', {}).get('column_candidates', [])))
print('üéØ Target candidati:', candidate_targets)# Setup e caricamento librerie
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import yaml
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from scipy import stats as scipy_stats
from scipy.stats import pearsonr, spearmanr, kendalltau, pointbiserialr
from sklearn.metrics import mutual_info_regression

plt.style.use('default')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

print('‚úÖ Setup completato! Librerie caricate.')