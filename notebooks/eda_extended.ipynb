# Riepilogo finale
import os
outs = sorted(os.listdir(output_dir))
print("\n" + "="*60)
print("üéâ OUTPUT GENERATI")
print("="*60)
for f in outs:
    print("‚Ä¢", f)
print("\n‚úÖ EDA esteso completato.")# Matrice eta2 per cat-num (opzionale, pu√≤ essere costosa)
cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()

if len(cat_cols) > 0 and len(num_cols) > 0 and len(cat_cols) <= 40:
    eta_mat = pd.DataFrame(index=cat_cols, columns=num_cols, dtype=float)
    for c in cat_cols:
        for n in num_cols:
            eta_mat.loc[c, n] = assoc_categorical_numeric(df[c], df[n])["eta2"]
    eta_mat.to_csv(output_dir / 'matrix_catnum_eta2.csv')
    print("üíæ Salvata matrice cat-num eta2")
else:
    if len(cat_cols) > 40:
        print("‚ÑπÔ∏è Troppe categoriche per eta2 completo (>", len(cat_cols), ") ‚Äî skip")# Check geospaziale rapido
candidates = []
geo_patterns = ['wkt', 'geometry', 'geom', 'geojson', 'the_geom', 'shape', 'polygon', 'point', 'linestring',
                'lat', 'latitude', 'lon', 'lng', 'longitude', 'coord', 'x_', 'y_', 'easting', 'northing',
                'geo', 'spatial', 'location', 'posizione', 'indirizzo', 'address']
for col in df.columns:
    col_l = col.lower()
    for pat in geo_patterns:
        if pat in col_l:
            sample_vals = df[col].dropna().head(3).tolist()
            candidates.append({
                'Colonna': col,
                'Tipo': str(df[col].dtype),
                'Pattern': pat,
                'NonNull_Count': int(df[col].count()),
                'Esempi': str(sample_vals)[:100]
            })
            break

geo_df = pd.DataFrame(candidates).drop_duplicates(subset=['Colonna', 'Pattern']) if candidates else pd.DataFrame()
if not geo_df.empty:
    geo_df.to_csv(output_dir / 'geospatial_columns_check.csv', index=False)
    print(f"üíæ Salvato check geospaziale: {output_dir/'geospatial_columns_check.csv'}")
else:
    print("Nessuna colonna geospaziale candidata individuata")# Distribuzione target multi-target (statistiche + quantili + log)
import numpy as np

for target in candidate_targets:
    if target not in df.columns:
        continue
    t = df[target].dropna()
    if t.empty:
        continue
    desc = t.describe()
    print("\n" + "-"*50)
    print(f"üéØ Target: {target}")
    print(desc)

    # Fasce a quantili
    quantiles = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
    bands = t.quantile(quantiles)
    bands.to_csv(output_dir / f"target_quantiles__{target}.csv")

    # Statistiche log
    tlog = np.log1p(t)
    log_stats = pd.Series({
        'mean_log': tlog.mean(),
        'median_log': tlog.median(),
        'std_log': tlog.std()
    })
    log_stats.to_csv(output_dir / f"target_log_stats__{target}.csv")

print("‚úÖ Distribuzioni target salvate (quantili/log stats)")# Missingness
missing_stats = pd.DataFrame({
    'Totale_Null': df.isnull().sum(),
    'Percentuale_Null': (df.isnull().mean() * 100).round(2),
    'Tipo': df.dtypes
})
missing_stats.to_csv(output_dir / 'missingness_analysis.csv')
print(f"üíæ Salvata missingness in: {output_dir/'missingness_analysis.csv'}")

high_missing = missing_stats[missing_stats['Percentuale_Null'] > 50].sort_values('Percentuale_Null', ascending=False)
print(f"‚ö†Ô∏è Colonne con >50% NaN: {len(high_missing)}")# Overview dataset
memory_mb = df.memory_usage(deep=True).sum() / 1024**2
print("="*60)
print("üìã OVERVIEW DATASET")
print("="*60)
print(f"üìä Dimensioni: {df.shape[0]:,} righe √ó {df.shape[1]} colonne")
print(f"üíæ Memoria utilizzata: {memory_mb:.2f} MB")
print("\nüìã Tipi di dati:")
dtype_counts = df.dtypes.value_counts()
for dtype, count in dtype_counts.items():
    print(f"  {dtype}: {count} colonne")# Matrici di associazione feature-feature (overview)
import numpy as np
import pandas as pd

# Numeriche: Pearson e Spearman
num_df = df.select_dtypes(include=[np.number])
if num_df.shape[1] >= 2:
    pear = num_df.corr(method='pearson')
    spear = num_df.corr(method='spearman')
    pear.to_csv(output_dir / 'matrix_numeric_pearson.csv')
    spear.to_csv(output_dir / 'matrix_numeric_spearman.csv')
    print("üíæ Salvate matrici numeriche Pearson/Spearman")

# Categoriali: Cram√©r's V (pu√≤ essere lento su alta cardinalit√†)
cat_df = df.select_dtypes(exclude=[np.number])
if cat_df.shape[1] >= 2 and cat_df.shape[1] <= 30:
    cols = cat_df.columns.tolist()
    m = pd.DataFrame(np.nan, index=cols, columns=cols)
    for i, c1 in enumerate(cols):
        for j, c2 in enumerate(cols):
            if j < i:
                continue
            v = assoc_categorical_categorical(cat_df[c1], cat_df[c2])["cramers_v"]
            m.loc[c1, c2] = v
            m.loc[c2, c1] = v
    m.to_csv(output_dir / 'matrix_categorical_cramers_v.csv')
    print("üíæ Salvata matrice categoriche Cram√©r's V")
else:
    if cat_df.shape[1] > 30:
        print("‚ÑπÔ∏è Troppe categoriche per Cram√©r's V completo (>", cat_df.shape[1], ") ‚Äî skip")# Analisi multi-target: statistiche, gruppi e associazioni con target
import numpy as np
import pandas as pd

num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()

for target in candidate_targets:
    if target not in df.columns:
        print(f"‚ùå Target mancante: {target}")
        continue
    print("\n" + "="*60)
    print(f"üéØ ANALISI PER TARGET: {target}")
    print("="*60)

    tser = df[target]
    print(f"Non-null: {tser.count():,} | Missing: {tser.isna().sum():,}")
    desc = tser.describe()
    stats_df = pd.DataFrame(desc).reset_index()
    stats_df.columns = ['Statistic', 'Value']
    stats_df.to_csv(output_dir / f"target_stats__{target}.csv", index=False)

    # Group summaries per colonne chiave (se presenti)
    group_cols = ['AI_ZonaOmi', 'AI_IdCategoriaCatastale']
    for gcol in group_cols:
        if gcol in df.columns:
            g = df[[gcol, target]].dropna()
            if len(g) > 0:
                gsum = g.groupby(gcol)[target].agg(['count', 'mean', 'median', 'std', 'min', 'max']).round(3)
                gsum['cv'] = (gsum['std'] / gsum['mean']).replace([np.inf, -np.inf], np.nan).round(3)
                outp = output_dir / f"group_summary__{gcol}__{target}.csv"
                gsum.to_csv(outp)
                print(f"üíæ Salvato summary gruppo: {outp.name}")

    # Associazioni numeriche col target
    assoc_rows = []
    for col in [c for c in num_cols if c != target]:
        res = assoc_numeric_numeric(df[col], df[target])
        assoc_rows.append({"feature": col, **res})
    assoc_num_df = pd.DataFrame(assoc_rows).sort_values(by='pearson', key=lambda s: s.abs(), ascending=False)
    assoc_num_df.to_csv(output_dir / f"associations_numeric_with_target__{target}.csv", index=False)
    print(f"üíæ Salvate associazioni numeriche col target: associations_numeric_with_target__{target}.csv")

    # Associazioni categoriche col target (eta2 / point-biserial su binarie)
    assoc_c_rows = []
    for col in cat_cols:
        res = assoc_categorical_numeric(df[col], df[target])
        assoc_c_rows.append({"feature": col, **res, "nunique": df[col].nunique()})
    assoc_cat_df = pd.DataFrame(assoc_c_rows).sort_values(by='eta2', ascending=False)
    assoc_cat_df.to_csv(output_dir / f"associations_categorical_with_target__{target}.csv", index=False)
    print(f"üíæ Salvate associazioni categoriche col target: associations_categorical_with_target__{target}.csv")
# Helper: funzioni per misure di associazione
import numpy as np
import pandas as pd
from scipy.stats import pearsonr, spearmanr, kendalltau, pointbiserialr
from sklearn.metrics import mutual_info_regression


def _dropna_pair(x: pd.Series, y: pd.Series) -> tuple[np.ndarray, np.ndarray]:
    mask = x.notna() & y.notna()
    return x[mask].to_numpy(), y[mask].to_numpy()


def assoc_numeric_numeric(x: pd.Series, y: pd.Series) -> dict:
    xv, yv = _dropna_pair(x, y)
    if len(xv) < 3:
        return {"pearson": np.nan, "spearman": np.nan, "kendall": np.nan, "mi": np.nan}
    pr = pearsonr(xv, yv)[0]
    sr = spearmanr(xv, yv)[0]
    kr = kendalltau(xv, yv)[0]
    try:
        mi = mutual_info_regression(xv.reshape(-1, 1), yv, random_state=42)
        mi_val = float(mi[0])
    except Exception:
        mi_val = np.nan
    return {"pearson": pr, "spearman": sr, "kendall": kr, "mi": mi_val}


def assoc_categorical_categorical(x: pd.Series, y: pd.Series) -> dict:
    # Cram√©r's V con correzione
    from scipy.stats import chi2_contingency
    xt = pd.crosstab(x, y)
    if xt.shape[0] < 2 or xt.shape[1] < 2:
        return {"cramers_v": np.nan}
    chi2, _, _, _ = chi2_contingency(xt)
    n = xt.values.sum()
    phi2 = chi2 / n
    r, k = xt.shape
    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))
    rcorr = r - ((r - 1) ** 2) / (n - 1)
    kcorr = k - ((k - 1) ** 2) / (n - 1)
    denom = min(kcorr - 1, rcorr - 1)
    if denom <= 0:
        return {"cramers_v": np.nan}
    v = np.sqrt(phi2corr / denom)
    return {"cramers_v": float(v)}


def assoc_categorical_numeric(cat: pd.Series, num: pd.Series) -> dict:
    # Eta-squared (ANOVA one-way) e point-biserial se binaria
    cv = cat.astype("category")
    groups = [num[cv == c].dropna().values for c in cv.cat.categories]
    # Eta-squared
    if len(groups) < 2 or sum(len(g) for g in groups) < 3:
        eta2 = np.nan
    else:
        overall_mean = num.dropna().mean()
        ss_between = sum(len(g) * (g.mean() - overall_mean) ** 2 for g in groups if len(g) > 0)
        ss_total = ((num.dropna() - overall_mean) ** 2).sum()
        eta2 = float(ss_between / ss_total) if ss_total > 0 else np.nan
    # Point-biserial
    try:
        if cv.nunique() == 2:
            xbin = (cv.cat.codes == cv.cat.categories.tolist().index(cv.cat.categories[1])).astype(int)
            xb, yv = _dropna_pair(pd.Series(xbin, index=cv.index), num)
            pbr = pointbiserialr(xb, yv)[0] if len(xb) >= 3 else np.nan
        else:
            pbr = np.nan
    except Exception:
        pbr = np.nan
    return {"eta2": eta2, "point_biserial": pbr}
# Percorsi e caricamento config/dati
from pathlib import Path
import yaml
import pandas as pd

output_dir = Path('eda_outputs') / 'extended'
output_dir.mkdir(parents=True, exist_ok=True)
print(f"üìÅ Directory output: {output_dir}")

config_path = '../config/config.yaml'
with open(config_path, 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)
print(f"‚úÖ Config caricata da: {config_path}")

data_path = '../data/raw/raw.parquet'
df = pd.read_parquet(data_path)
print(f"‚úÖ Dataset caricato: {df.shape[0]:,} righe √ó {df.shape[1]} colonne")

candidate_targets = list(dict.fromkeys(config.get('target', {}).get('column_candidates', [])))
print("üéØ Target candidati:", candidate_targets)