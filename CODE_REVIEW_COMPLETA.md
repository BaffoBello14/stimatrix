# üìã REVIEW COMPLETA DEL CODICE - STIMATRIX PIPELINE

**Data Review**: 2025-11-11  
**Revisore**: AI Assistant  
**Versione Codice**: Branch `cursor/review-code-and-suggest-configurations-ef0d`

---

## üìä EXECUTIVE SUMMARY

**Stimatrix** √® una pipeline **production-ready** end-to-end per la stima automatica dei prezzi immobiliari, costruita con best practices professionali e architettura modulare eccellente.

### ‚úÖ Punti di Forza (Eccellenze)

1. **üèóÔ∏è Architettura Modulare Eccezionale**: Separazione perfetta tra `db`, `preprocessing`, `training`, `utils`
2. **üîí Sicurezza Robusta**: Credenziali da env vars, input sanitization, audit logging
3. **üß™ Testing Completo**: 11 test files con coverage end-to-end (preprocessing, training, encoding, overflow, ecc.)
4. **üìà Experiment Tracking**: Integrazione W&B nativa e configurabile
5. **üéØ Target Transform Avanzato**: Supporto Box-Cox, Yeo-Johnson, log, sqrt con Duan smearing
6. **üîç Diagnostics Avanzate**: Residual analysis, drift detection, prediction intervals, SHAP
7. **üìä Profili Multipli**: `scaled`, `tree`, `catboost` per famiglie di modelli diverse
8. **‚öôÔ∏è Configurazione Flessibile**: YAML con env vars expansion, profili per-model
9. **üöÄ Feature Engineering Intelligente**: Estrazione WKT, JSON, GeoJSON, floor parsing
10. **üîÑ Backward Compatibility**: File senza suffisso per compatibilit√† legacy

### ‚ö†Ô∏è Aree di Miglioramento (Non Critiche)

1. **üìù Documentazione**: Manca docstring in alcune funzioni interne
2. **üßπ Refactoring Minore**: Alcune funzioni lunghe (`run_preprocessing`, `run_training`) potrebbero essere split
3. **‚ö° Performance**: Considerare caching di query DB e preprocessing intermedio
4. **üîç Monitoring**: Aggiungere health checks e alerting produzione
5. **üê≥ Deployment**: Manca Dockerfile/docker-compose per deploy facile

**Giudizio Complessivo**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5) - Codice di **qualit√† professionale elevata**

---

## üèóÔ∏è ARCHITETTURA E STRUTTURA

### 1. Moduli Principali

```
src/
‚îú‚îÄ‚îÄ db/                     # Database connection & schema extraction
‚îÇ   ‚îú‚îÄ‚îÄ connect.py          # ‚úÖ Secure connection con retry e TLS
‚îÇ   ‚îî‚îÄ‚îÄ schema_extract.py   # ‚úÖ Type normalization, view support
‚îú‚îÄ‚îÄ dataset_builder/
‚îÇ   ‚îî‚îÄ‚îÄ retrieval.py        # ‚úÖ SQL templates, POI/ZTL enrichment
‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py         # üî• CORE: orchestrazione preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ encoders.py         # Multi-strategy encoding (OHE, target, freq, ordinal)
‚îÇ   ‚îú‚îÄ‚îÄ imputation.py       # Group-wise imputation
‚îÇ   ‚îú‚îÄ‚îÄ outliers.py         # Ensemble outlier detection (IQR+Z-score+IsoForest)
‚îÇ   ‚îú‚îÄ‚îÄ target_transforms.py # Box-Cox, Yeo-Johnson, log con Duan smearing
‚îÇ   ‚îú‚îÄ‚îÄ transformers.py     # Scaling, PCA, correlation pruning
‚îÇ   ‚îî‚îÄ‚îÄ feature_extractors.py # WKT, JSON, GeoJSON, floor parsing
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ train.py            # üî• CORE: training loop con Optuna
‚îÇ   ‚îú‚îÄ‚îÄ tuner.py            # Optuna integration (TPE, AutoSampler)
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py       # Model evaluation e group metrics
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py          # Regression metrics + grouped metrics
‚îÇ   ‚îú‚îÄ‚îÄ diagnostics.py      # Residual analysis, drift, PI
‚îÇ   ‚îú‚îÄ‚îÄ model_zoo.py        # Factory pattern per 12+ modelli
‚îÇ   ‚îú‚îÄ‚îÄ ensembles.py        # Voting & Stacking ensembles
‚îÇ   ‚îî‚îÄ‚îÄ shap_utils.py       # SHAP feature importance
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ config.py           # ‚úÖ YAML loader con env vars expansion
    ‚îú‚îÄ‚îÄ logger.py           # ‚úÖ Structured logging con rotation
    ‚îú‚îÄ‚îÄ io.py               # ‚úÖ Save/load helpers (JSON, Parquet, CSV)
    ‚îú‚îÄ‚îÄ security.py         # ‚úÖ Credential manager + input validation
    ‚îú‚îÄ‚îÄ sql_templates.py    # ‚úÖ SQL template system
    ‚îî‚îÄ‚îÄ wandb_utils.py      # ‚úÖ W&B tracker con graceful degradation
```

### 2. Flusso di Esecuzione

```
main.py
  ‚Üì
[STEP 1: schema] ‚Üí db.schema_extract.run_schema()
  ‚Üí schema/db_schema.json
  ‚Üì
[STEP 2: dataset] ‚Üí dataset_builder.retrieval.run_dataset()
  ‚Üí data/raw/raw.parquet (con POI/ZTL opzionali)
  ‚Üì
[STEP 3: preprocessing] ‚Üí preprocessing.pipeline.run_preprocessing()
  ‚Üí data/preprocessed/
      ‚îú‚îÄ‚îÄ X_train_{profile}.parquet
      ‚îú‚îÄ‚îÄ y_train_{profile}.parquet
      ‚îú‚îÄ‚îÄ X_val_{profile}.parquet (opzionale)
      ‚îú‚îÄ‚îÄ y_val_{profile}.parquet
      ‚îú‚îÄ‚îÄ X_test_{profile}.parquet
      ‚îú‚îÄ‚îÄ y_test_{profile}.parquet
      ‚îú‚îÄ‚îÄ y_test_orig_{profile}.parquet
      ‚îú‚îÄ‚îÄ artifacts/
      ‚îÇ   ‚îú‚îÄ‚îÄ imputers.joblib
      ‚îÇ   ‚îú‚îÄ‚îÄ {profile}/
      ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ encoders.joblib
      ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ winsorizer.joblib
      ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transforms.joblib (scaler + PCA)
      ‚îî‚îÄ‚îÄ preprocessing_info.json
  ‚Üì
[STEP 4: training] ‚Üí training.train.run_training()
  ‚Üí models/
      ‚îú‚îÄ‚îÄ {model_key}/
      ‚îÇ   ‚îú‚îÄ‚îÄ model.pkl
      ‚îÇ   ‚îú‚îÄ‚îÄ metrics.json
      ‚îÇ   ‚îú‚îÄ‚îÄ optuna_trials.csv
      ‚îÇ   ‚îú‚îÄ‚îÄ shap/
      ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shap_{model}_beeswarm.png
      ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shap_{model}_bar.png
      ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ shap_values.npy (opzionale)
      ‚îÇ   ‚îú‚îÄ‚îÄ group_metrics_AI_ZonaOmi.csv
      ‚îÇ   ‚îú‚îÄ‚îÄ group_metrics_price_band.csv
      ‚îÇ   ‚îî‚îÄ‚îÄ {model}_worst_predictions.csv
      ‚îú‚îÄ‚îÄ voting/
      ‚îÇ   ‚îî‚îÄ‚îÄ model.pkl
      ‚îú‚îÄ‚îÄ stacking/
      ‚îÇ   ‚îî‚îÄ‚îÄ model.pkl
      ‚îú‚îÄ‚îÄ summary.json
      ‚îî‚îÄ‚îÄ validation_results.csv
  ‚Üì
[STEP 5: evaluation] ‚Üí training.evaluation.run_evaluation()
  ‚Üí models/evaluation_summary.json
```

---

## üíæ COME VENGONO SALVATI I RISULTATI

### 1. **Preprocessing Output** (`data/preprocessed/`)

#### File Principali per Profilo

```python
# Per ogni profilo abilitato (tree, catboost, scaled):
X_train_{profile}.parquet       # Feature di training
y_train_{profile}.parquet       # Target di training (transformed)
X_val_{profile}.parquet         # Feature di validation (se valid_fraction > 0)
y_val_{profile}.parquet         # Target di validation (transformed)
X_test_{profile}.parquet        # Feature di test
y_test_{profile}.parquet        # Target di test (transformed)
y_test_orig_{profile}.parquet   # Target di test (scala originale - EURO)
y_val_orig_{profile}.parquet    # Target di validation (scala originale)

# File backward-compatible (copia del primo profilo abilitato)
X_train.parquet
y_train.parquet
X_val.parquet (se esiste validation)
y_val.parquet
X_test.parquet
y_test.parquet
y_test_orig.parquet
y_val_orig.parquet

# Dataset combinato
preprocessed.parquet  # train + val + test con target (per visualizzazione)

# Sidecar per group metrics (evaluation)
group_cols_train_{profile}.parquet  # Colonne per raggruppamento (es. AI_ZonaOmi)
group_cols_test_{profile}.parquet
group_cols_val_{profile}.parquet
```

#### Artefatti di Trasformazione (`data/preprocessed/artifacts/`)

```python
artifacts/
‚îú‚îÄ‚îÄ imputers.joblib              # SimpleImputer per numeriche/categoriche
‚îú‚îÄ‚îÄ {profile}/
‚îÇ   ‚îú‚îÄ‚îÄ encoders.joblib          # Dict di encoders (OHE, target, freq, ordinal)
‚îÇ   ‚îú‚îÄ‚îÄ winsorizer.joblib        # Winsorizer con quantili
‚îÇ   ‚îî‚îÄ‚îÄ transforms.joblib        # Dict con 'scaler' (StandardScaler/RobustScaler) 
‚îÇ                                 # e 'pca' (PCA opzionale)
```

#### Metadata (`data/preprocessed/preprocessing_info.json`)

```json
{
  "target_column": "AI_Prezzo_Ridistribuito",
  "target_transformation": {
    "transform": "boxcox",           // Tipo: none|log|log10|sqrt|boxcox|yeojohnson
    "lambda": 0.123,                 // Lambda per Box-Cox/Yeo-Johnson
    "shift": 100.0,                  // Shift per Box-Cox (se y <= 0)
    "log10_offset": 1.0              // Offset per log10
  },
  "profiles_saved": ["tree", "catboost"],
  "feature_columns_per_profile": {
    "tree": ["AI_Superficie", "AI_Vani", ...],
    "catboost": ["AI_Superficie", "AI_ZonaOmi", ...]
  }
}
```

### 2. **Training Output** (`models/`)

#### Per Modello (`models/{model_key}/`)

```python
{model_key}/
‚îú‚îÄ‚îÄ model.pkl                    # Modello serializzato (joblib)
‚îú‚îÄ‚îÄ metrics.json                 # Metriche complete
‚îú‚îÄ‚îÄ optuna_trials.csv            # Trial history di Optuna
‚îú‚îÄ‚îÄ shap/
‚îÇ   ‚îú‚îÄ‚îÄ shap_{model}_beeswarm.png   # SHAP beeswarm plot
‚îÇ   ‚îú‚îÄ‚îÄ shap_{model}_bar.png        # SHAP bar plot (feature importance)
‚îÇ   ‚îú‚îÄ‚îÄ shap_values.npy             # SHAP values (opzionale, pu√≤ essere grande)
‚îÇ   ‚îî‚îÄ‚îÄ shap_sample.parquet         # Sample usato per SHAP
‚îú‚îÄ‚îÄ group_metrics_AI_ZonaOmi.csv           # Metriche per zona OMI
‚îú‚îÄ‚îÄ group_metrics_AI_IdCategoriaCatastale.csv  # Metriche per categoria catastale
‚îú‚îÄ‚îÄ group_metrics_price_band.csv           # Metriche per fascia di prezzo
‚îú‚îÄ‚îÄ {model}_worst_predictions.csv          # Top N worst predictions
‚îú‚îÄ‚îÄ {model}_residual_plots/
‚îÇ   ‚îú‚îÄ‚îÄ residual_vs_predicted.png
‚îÇ   ‚îú‚îÄ‚îÄ residual_vs_actual.png
‚îÇ   ‚îú‚îÄ‚îÄ residual_distribution.png
‚îÇ   ‚îî‚îÄ‚îÄ qq_plot.png
‚îî‚îÄ‚îÄ {model}_prediction_intervals.json      # Prediction intervals bootstrap
```

#### Struttura `metrics.json` (Esempio)

```json
{
  "model_key": "xgboost",
  "prefix": "tree",
  "primary_metric": "neg_root_mean_squared_error",
  "best_primary_value": -15234.56,
  "best_params": {
    "n_estimators": 1200,
    "max_depth": 6,
    "learning_rate": 0.05,
    "subsample": 0.8,
    "colsample_bytree": 0.85,
    "min_child_weight": 2.3,
    "reg_alpha": 0.5,
    "reg_lambda": 1.2,
    "gamma": 0.1
  },
  "metrics_train": {
    "r2": 0.9523,
    "rmse": 12345.67,
    "mse": 152413890.0,
    "mae": 8901.23,
    "mape": 0.0823,
    "explained_variance": 0.9530,
    "medae": 6789.45
  },
  "metrics_test": {
    "r2": 0.9012,
    "rmse": 18567.89,
    "mse": 344766666.0,
    "mae": 13456.78,
    "mape": 0.1234,
    "explained_variance": 0.9020,
    "medae": 10234.56
  },
  "metrics_train_original": {
    "r2": 0.9501,
    "rmse": 12789.45,
    "mae": 9123.45,
    "mape": 0.0845,
    "mape_floor": 0.0838
  },
  "metrics_test_original": {
    "r2": 0.8989,
    "rmse": 19234.56,
    "mae": 13890.12,
    "mape": 0.1267,
    "mape_floor": 0.1259
  },
  "smearing_factor": 1.0234,
  "overfit": {
    "gap_r2": 0.0511,
    "gap_explained_variance": 0.0510,
    "ratio_rmse": 1.5034,
    "delta_rmse": 6222.22,
    "ratio_mse": 2.2602,
    "delta_mse": 192352776.0,
    "ratio_mae": 1.5115,
    "delta_mae": 4554.55,
    "ratio_mape": 1.4993,
    "delta_mape": 0.0411,
    "ratio_medae": 1.5070,
    "delta_medae": 3445.11
  }
}
```

#### Ensemble Models (`models/voting/`, `models/stacking/`)

Stessa struttura dei singoli modelli, con `metrics.json` che include:

```json
{
  "type": "voting",  // o "stacking"
  "members": ["xgboost", "lightgbm", "catboost"],
  "final_estimator": "ridge",  // solo per stacking
  "metrics_train": {...},
  "metrics_test": {...},
  "metrics_train_original": {...},
  "metrics_test_original": {...},
  "overfit": {...}
}
```

#### Summary Files

##### `models/summary.json`

```json
{
  "models": {
    "xgboost": {
      "best_params": {...},
      "best_primary_value": -15234.56,
      "metrics_test": {...},
      "metrics_train": {...},
      "metrics_test_original": {...},
      "metrics_train_original": {...},
      "smearing_factor": 1.0234,
      "overfit": {...}
    },
    "lightgbm": {...},
    "catboost": {...}
  },
  "baselines": {
    "xgboost": {
      "metrics_test": {...}
    }
  },
  "ensembles": {
    "voting": {...},
    "stacking": {...}
  }
}
```

##### `models/validation_results.csv`

```csv
Model,Category,Test_RMSE,Test_R2
Optimized_xgboost,Optimized,18567.89,0.9012
Optimized_lightgbm,Optimized,18901.23,0.8987
Optimized_catboost,Optimized,19123.45,0.8965
Ensemble_voting,Ensemble,18234.56,0.9034
Ensemble_stacking,Ensemble,18012.34,0.9056
Baseline_xgboost,Baseline,21234.56,0.8756
```

##### `models/evaluation_summary.json`

```json
{
  "top_models": [
    {
      "Model": "Ensemble_stacking",
      "Category": "Ensemble",
      "Test_RMSE": 18012.34,
      "Test_R2": 0.9056
    },
    ...
  ],
  "test_metrics": [
    {
      "model": "xgboost",
      "r2": 0.9012,
      "rmse": 18567.89,
      "mae": 13456.78,
      "r2_orig": 0.8989,
      "rmse_orig": 19234.56,
      "mae_orig": 13890.12,
      "mape_floor_orig": 0.1259
    },
    ...
  ]
}
```

### 3. **Group Metrics** (`group_metrics_*.csv`)

Esempio di `group_metrics_AI_ZonaOmi.csv`:

```csv
group,count,r2,rmse,mse,mae,mape,medae
B1,1234,0.91,15234.56,232094567.0,10123.45,0.098,7890.12
B2,2345,0.89,17890.12,320054321.0,12345.67,0.112,9876.54
C1,987,0.87,19234.56,370068901.0,13456.78,0.125,10234.56
...
```

Esempio di `group_metrics_price_band.csv`:

```csv
group,count,r2,rmse,mse,mae,mape,medae
PREZZO_(50000.0, 150000.0],1567,0.85,8901.23,79230987.0,6789.45,0.067,5123.45
PREZZO_(150000.0, 250000.0],2890,0.89,15234.56,232094567.0,11234.56,0.089,8765.43
PREZZO_(250000.0, 400000.0],1876,0.91,23456.78,550220987.0,17890.12,0.095,14321.09
PREZZO_(400000.0, 800000.0],456,0.87,45678.90,2086567890.0,34567.89,0.108,28901.23
```

### 4. **Diagnostics Output**

#### Drift Detection (`models/drift_report.json`)

```json
{
  "features": {
    "AI_Superficie": {
      "psi": 0.089,
      "ks_statistic": 0.023,
      "ks_pvalue": 0.234
    },
    "AI_Vani": {
      "psi": 0.187,
      "ks_statistic": 0.078,
      "ks_pvalue": 0.012
    }
  },
  "alerts": [
    {
      "feature": "AI_Vani",
      "method": "psi",
      "value": 0.187,
      "threshold": 0.15,
      "severity": "moderate"
    },
    {
      "feature": "AI_Vani",
      "method": "ks_test",
      "statistic": 0.078,
      "pvalue": 0.012,
      "severity": "high"
    }
  ],
  "summary": {
    "total_features_checked": 87,
    "psi_alerts": 3,
    "ks_alerts": 5,
    "total_alerts": 8
  }
}
```

#### Prediction Intervals (`{model}_prediction_intervals.json`)

```json
{
  "80%": {
    "coverage": 0.823,
    "average_width": 34567.89,
    "average_width_pct": 21.34,
    "target_coverage": 0.8
  },
  "90%": {
    "coverage": 0.912,
    "average_width": 45678.90,
    "average_width_pct": 28.12,
    "target_coverage": 0.9
  }
}
```

---

## üìä SIGNIFICATO DEI RISULTATI

### 1. **Metriche di Regressione**

#### Metriche su Scala Trasformata (`metrics_test`)
- **R¬≤**: % di varianza spiegata (0-1, meglio se vicino a 1)
- **RMSE**: Root Mean Squared Error - errore medio quadratico (pi√π basso √® meglio)
- **MSE**: Mean Squared Error - RMSE al quadrato
- **MAE**: Mean Absolute Error - errore medio assoluto (pi√π robusto a outlier)
- **MAPE**: Mean Absolute Percentage Error - errore % medio
- **MedAE**: Median Absolute Error - mediana dell'errore assoluto (robusto)
- **Explained Variance**: Varianza spiegata (simile a R¬≤)

#### Metriche su Scala Originale (`metrics_test_original`)
**‚ö†Ô∏è IMPORTANTE**: Queste sono le metriche "reali" in EURO per interpretazione business!

- **R¬≤ original**: Performance su scala EURO (es. 0.8989 = 89.89% varianza spiegata)
- **RMSE original**: Errore medio in EURO (es. 19234.56‚Ç¨ = errore medio ¬±19k‚Ç¨)
- **MAE original**: Errore assoluto medio in EURO
- **MAPE_floor**: MAPE con floor per evitare divisioni per zero su valori piccoli

**Interpretazione Pratica**:
```
RMSE = 19234.56‚Ç¨
‚Üí In media, le predizioni sbagliano di ¬±19k‚Ç¨
‚Üí Su un immobile da 200k‚Ç¨, errore ~9.6%
‚Üí Su un immobile da 500k‚Ç¨, errore ~3.8%
```

### 2. **Diagnostiche di Overfitting**

#### Gap Metrics (train - test)
```json
"gap_r2": 0.0511
```
- **Interpretazione**: Il modello performa 5.11% peggio su test rispetto a train
- **Soglia OK**: < 0.05 (5%)
- **Moderato**: 0.05 - 0.10
- **Alto**: > 0.10 ‚Üí modello troppo overfit

#### Ratio Metrics (test / train)
```json
"ratio_rmse": 1.5034
```
- **Interpretazione**: L'errore su test √® 50% pi√π alto che su train
- **Soglia OK**: 1.0 - 1.2 (20% di degradazione)
- **Moderato**: 1.2 - 1.5
- **Alto**: > 1.5 ‚Üí modello troppo overfit

### 3. **SHAP Feature Importance**

I grafici SHAP mostrano:
- **Beeswarm plot**: Contributo di ogni feature per ogni predizione
  - Colore = valore feature (rosso alto, blu basso)
  - Asse X = SHAP value (impatto sulla predizione)
  - Feature ordinate per importanza
  
- **Bar plot**: Importanza media assoluta per feature
  - Pi√π alta la barra = pi√π importante la feature

**Esempio Interpretazione**:
```
AI_Superficie: SHAP = +0.3 (in scala log)
‚Üí Un aumento di superficie contribuisce positivamente al prezzo
‚Üí √à la feature pi√π importante per il modello
```

### 4. **Group Metrics**

Permettono di identificare **bias geografici/categoriali**:

```csv
group,count,r2,rmse,mae
Zona_B1,1234,0.91,15234.56,10123.45
Zona_C3,987,0.67,35678.90,25123.45
```

**Interpretazione**:
- Zona B1: ottima performance (R¬≤ = 0.91, RMSE = 15k‚Ç¨)
- Zona C3: scarsa performance (R¬≤ = 0.67, RMSE = 35k‚Ç¨)
- **Azione**: Investigare Zona C3 (dati mancanti? outlier? feature mancanti?)

### 5. **Price Band Metrics**

Mostrano performance per **fascia di prezzo**:

```csv
group,r2,rmse,mape
PREZZO_(50k, 150k],0.85,8901.23,0.067
PREZZO_(400k, 800k],0.87,45678.90,0.108
```

**Interpretazione**:
- Fasce basse: R¬≤ simile, ma RMSE pi√π basso (errore assoluto minore)
- Fasce alte: R¬≤ simile, ma RMSE pi√π alto (errore assoluto maggiore)
- MAPE cresce con il prezzo ‚Üí il modello sbaglia % pi√π alta sugli immobili costosi

### 6. **Drift Detection**

**PSI (Population Stability Index)**:
- < 0.1: Nessun drift significativo
- 0.1 - 0.15: Drift moderato
- \> 0.15: Drift significativo ‚Üí **modello da ritrainare**

**KS Test (Kolmogorov-Smirnov)**:
- p < 0.05: Distribuzione train vs test significativamente diversa
- **Azione**: Verificare se il test set √® rappresentativo

**Esempio Pratico**:
```json
"AI_Vani": {
  "psi": 0.187,      ‚Üí DRIFT ALTO!
  "ks_pvalue": 0.012 ‚Üí DISTRIBUZIONE DIVERSA!
}
```
‚Üí La distribuzione del numero di vani √® cambiata tra train e test
‚Üí Possibile causa: cambiamento temporale, bias nel sampling
‚Üí Azione: Ritrainare con dati pi√π recenti o investigare il motivo

### 7. **Prediction Intervals**

Quantificano l'**incertezza** della predizione:

```json
"90%": {
  "coverage": 0.912,          ‚Üí 91.2% dei valori veri cadono nell'intervallo
  "average_width": 45678.90,  ‚Üí Intervallo medio di ¬±45k‚Ç¨
  "target_coverage": 0.9      ‚Üí Target era 90%
}
```

**Interpretazione**:
- Coverage vicino al target (91.2% vs 90%) ‚Üí intervalli ben calibrati
- Average width = incertezza media del modello
- Usare per comunicare la **confidenza** della stima al cliente

**Esempio Business**:
```
Predizione: 200k‚Ç¨
Intervallo 90%: [175k‚Ç¨, 225k‚Ç¨]
‚Üí "Siamo confidenti al 90% che il prezzo reale sia tra 175k e 225k"
```

### 8. **Worst Predictions**

File `{model}_worst_predictions.csv` contiene i record con errori pi√π grandi:

```csv
true,predicted,residual,abs_residual,pct_error
450000.0,280000.0,-170000.0,170000.0,37.78
```

**Uso**:
1. Identificare pattern comuni nei worst predictions
2. Verificare se ci sono outlier non rilevati
3. Cercare feature mancanti (es. "tutti gli errori grandi sono in zona X")
4. Migliorare il modello per questi casi specifici

---

## üß™ CONFIGURAZIONI DA PROVARE (ESPERIMENTI)

### üéØ Baseline: Configurazione Attuale

**File**: `config/config.yaml`

**Setup**:
```yaml
target:
  transform: boxcox
database:
  use_poi: true
  use_ztl: true
  selected_aliases: ['A', 'AI', 'PC', 'ISC', 'II', 'PC_OZ', 'OZ', 'OV', 'C1', 'C2']
profiles:
  tree: enabled: true
  catboost: enabled: true
training:
  models: [rf, gbr, hgbt, xgboost, lightgbm, catboost]
  trials: 100
```

**Metriche Attese** (baseline per confronti):
- Test R¬≤ (original): ~0.89-0.91
- Test RMSE (original): ~18k-20k‚Ç¨
- Test MAPE: ~12-13%

---

### üìã ESPERIMENTI CONSIGLIATI

#### **CATEGORIA A: ABLATION STUDIES (Rimuovere Features)**

---

### üß™ **Esperimento A1: Senza Trasformazione Target**

**Obiettivo**: Valutare impatto della trasformazione Box-Cox sul target

**Config**: `config/config_no_transform.yaml`

```yaml
target:
  transform: none  # ‚Üê CAMBIO PRINCIPALE

# Resto identico a config.yaml
database:
  use_poi: true
  use_ztl: true
profiles:
  tree: {enabled: true}
  catboost: {enabled: true}
training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- Target senza trasformazione ‚Üí modelli meno performanti (R¬≤ pi√π basso)
- RMSE potrebbe essere pi√π alto su scala originale
- Residui potrebbero essere asimmetrici (code lunghe)

**Metriche da Confrontare**:
- R¬≤ original: atteso < 0.85 (peggiore di baseline)
- RMSE original: atteso > 22k‚Ç¨
- Residual distribution: pi√π skewed

**Come Eseguire**:
```bash
python main.py --config config/config_no_transform.yaml --steps preprocessing training evaluation
```

---

### üß™ **Esperimento A2: Senza POI (Points of Interest)**

**Obiettivo**: Valutare impatto delle feature POI (scuole, ospedali, fermate, ecc.)

**Config**: `config/config_no_poi.yaml`

```yaml
database:
  use_poi: false  # ‚Üê CAMBIO PRINCIPALE
  use_ztl: true
  selected_aliases: ['A', 'AI', 'PC', 'ISC', 'II', 'PC_OZ', 'OZ', 'OV', 'C1', 'C2']

target:
  transform: boxcox

profiles:
  tree: {enabled: true}
  catboost: {enabled: true}
training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- POI aggiungono informazione sul contesto geografico/urbanistico
- Rimuoverli ‚Üí calo di performance (specialmente in zone urbane)
- Group metrics per zona potrebbero mostrare bias maggiore

**Metriche da Confrontare**:
- R¬≤ original: atteso ~0.87-0.88 (calo ~1-2%)
- RMSE original: atteso ~20-22k‚Ç¨ (aumento ~5-10%)
- Group metrics per zona: verificare se alcune zone peggiorano molto

**SHAP Analysis**:
- Nella configurazione baseline, verificare feature importance di POI
- Se POI sono importanti ‚Üí esperimento conferma il loro valore

---

### üß™ **Esperimento A3: Senza ZTL (Zone a Traffico Limitato)**

**Obiettivo**: Valutare impatto della feature binaria ZTL

**Config**: `config/config_no_ztl.yaml`

```yaml
database:
  use_poi: true
  use_ztl: false  # ‚Üê CAMBIO PRINCIPALE
  selected_aliases: ['A', 'AI', 'PC', 'ISC', 'II', 'PC_OZ', 'OZ', 'OV', 'C1', 'C2']

target:
  transform: boxcox

profiles:
  tree: {enabled: true}
  catboost: {enabled: true}
training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- ZTL ha impatto minore rispetto a POI (√® una feature binaria singola)
- Calo di performance atteso: < 0.5%
- Se calo √® alto ‚Üí ZTL √® un proxy importante per "centro citt√†"

**Metriche da Confrontare**:
- R¬≤ original: atteso ~0.89-0.90 (calo minimo)
- RMSE original: atteso ~18.5-19.5k‚Ç¨

---

### üß™ **Esperimento A4: Senza CENED (Certificati Energetici)**

**Obiettivo**: Valutare impatto delle view `attiimmobili_cened1` e `attiimmobili_cened2`

**Config**: `config/config_no_cened.yaml`

```yaml
database:
  use_poi: true
  use_ztl: true
  selected_aliases: ['A', 'AI', 'PC', 'ISC', 'II', 'PC_OZ', 'OZ', 'OV']  # ‚Üê RIMOSSI C1, C2

target:
  transform: boxcox

profiles:
  tree: {enabled: true}
  catboost: {enabled: true}
training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- CENED contiene info su efficienza energetica (importante per valutazione immobile)
- Rimuoverlo ‚Üí calo performance, specialmente su immobili moderni
- Group metrics per categoria catastale potrebbero mostrare bias

**Metriche da Confrontare**:
- R¬≤ original: atteso ~0.88-0.89 (calo ~0.5-1%)
- Worst predictions: verificare se aumentano gli errori su immobili con certificazione

**Nota**: Verificare con SHAP nella baseline quante feature CENED sono importanti

---

### üß™ **Esperimento A5: Senza POI + ZTL + CENED (Solo Dati Base)**

**Obiettivo**: Baseline "minimalista" - solo dati immobiliari base

**Config**: `config/config_minimal.yaml`

```yaml
database:
  use_poi: false  # ‚Üê RIMOSSO
  use_ztl: false  # ‚Üê RIMOSSO
  selected_aliases: ['A', 'AI', 'PC', 'ISC', 'II', 'PC_OZ', 'OZ', 'OV']  # ‚Üê RIMOSSI C1, C2

target:
  transform: boxcox

profiles:
  tree: {enabled: true}
  catboost: {enabled: true}
training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- Configurazione pi√π semplice, meno overfitting potenziale
- Ma: performance peggiore per mancanza di contesto
- Utile per capire il "valore incrementale" delle feature aggiunte

**Metriche Attese**:
- R¬≤ original: atteso ~0.85-0.87 (calo ~3-5%)
- RMSE original: atteso ~22-25k‚Ç¨
- Overfitting gap potrebbe ridursi (meno feature = meno overfitting)

---

#### **CATEGORIA B: VARIAZIONI TARGET**

---

### üß™ **Esperimento B1: Target = AI_Prezzo_MQ (Prezzo al Metro Quadro)**

**Obiettivo**: Predire prezzo al m¬≤ invece di prezzo totale

**Config**: `config/config_target_mq.yaml`

```yaml
target:
  column_candidates: ['AI_Prezzo_MQ']  # ‚Üê CAMBIO PRINCIPALE
  transform: boxcox

database:
  use_poi: true
  use_ztl: true
  selected_aliases: ['A', 'AI', 'PC', 'ISC', 'II', 'PC_OZ', 'OZ', 'OV', 'C1', 'C2']

feature_pruning:
  include_ai_superficie: false  # ‚Üê IMPORTANTE: rimuovere superficie dalle feature!

profiles:
  tree: {enabled: true}
  catboost: {enabled: true}
training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- Prezzo/m¬≤ potrebbe essere pi√π uniforme tra immobili (meno varianza)
- R¬≤ potrebbe essere pi√π alto (target normalizzato per dimensione)
- MAPE potrebbe migliorare

**Metriche da Confrontare**:
- R¬≤ original: atteso ~0.91-0.93 (potenziale miglioramento)
- RMSE in ‚Ç¨/m¬≤: difficile confrontare con baseline (scale diverse)
- MAPE: atteso ~10-11% (miglioramento)

**Nota**: Per confronto apples-to-apples, moltiplicare predizioni per superficie e confrontare RMSE totale

---

### üß™ **Esperimento B2: Target con Log Transform (invece di Box-Cox)**

**Obiettivo**: Confrontare log semplice vs Box-Cox parametrico

**Config**: `config/config_log_transform.yaml`

```yaml
target:
  column_candidates: ['AI_Prezzo_Ridistribuito']
  transform: log  # ‚Üê CAMBIO PRINCIPALE (log1p)

database:
  use_poi: true
  use_ztl: true
  selected_aliases: ['A', 'AI', 'PC', 'ISC', 'II', 'PC_OZ', 'OZ', 'OV', 'C1', 'C2']

profiles:
  tree: {enabled: true}
  catboost: {enabled: true}
training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- Log √® pi√π semplice e interpretabile
- Box-Cox √® pi√π flessibile (trova lambda ottimale)
- Differenza attesa: < 0.5% in R¬≤

**Metriche da Confrontare**:
- R¬≤ original: atteso ~0.89-0.90 (simile o leggermente peggiore)
- Smearing factor: confrontare con Box-Cox
- Residual distribution: verificare normalit√†

---

### üß™ **Esperimento B3: Target con Yeo-Johnson (alternativa a Box-Cox)**

**Obiettivo**: Yeo-Johnson supporta valori negativi (pi√π robusto)

**Config**: `config/config_yeojohnson.yaml`

```yaml
target:
  column_candidates: ['AI_Prezzo_Ridistribuito']
  transform: yeojohnson  # ‚Üê CAMBIO PRINCIPALE

database:
  use_poi: true
  use_ztl: true
  selected_aliases: ['A', 'AI', 'PC', 'ISC', 'II', 'PC_OZ', 'OZ', 'OV', 'C1', 'C2']

profiles:
  tree: {enabled: true}
  catboost: {enabled: true}
training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- Performance simile a Box-Cox
- Pi√π robusto se ci sono outlier negativi post-preprocessing

**Metriche da Confrontare**:
- R¬≤ original: atteso ~0.89-0.91 (simile)
- Lambda: confrontare con Box-Cox lambda

---

#### **CATEGORIA C: VARIAZIONI PREPROCESSING**

---

### üß™ **Esperimento C1: Outlier Detection pi√π Aggressivo**

**Obiettivo**: Rimuovere pi√π outlier dal training set

**Config**: `config/config_outlier_aggressive.yaml`

```yaml
outliers:
  method: ensemble
  z_thresh: 2.5       # ‚Üê Pi√π stretto (default: 3.0)
  iqr_factor: 1.0     # ‚Üê Pi√π stretto (default: 1.5)
  iso_forest_contamination: 0.05  # ‚Üê Pi√π alto (default: 0.02)
  group_by_col: 'AI_IdTipologiaEdilizia'
  min_group_size: 30

target:
  transform: boxcox

database:
  use_poi: true
  use_ztl: true

profiles:
  tree: {enabled: true}
  catboost: {enabled: true}
training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- Training set pi√π pulito ‚Üí modelli pi√π semplici
- R¬≤ train potrebbe calare (meno dati)
- R¬≤ test potrebbe migliorare (meno noise)
- Overfitting gap potrebbe ridursi

**Metriche da Confrontare**:
- % outlier rimossi: atteso ~5-8% (vs ~2% baseline)
- R¬≤ train: atteso ~0.94 (calo da ~0.95)
- R¬≤ test: atteso ~0.90-0.92 (potenziale miglioramento)
- Overfitting ratio_rmse: atteso ~1.3-1.4 (miglioramento da ~1.5)

---

### üß™ **Esperimento C2: Senza Winsorization**

**Obiettivo**: Valutare impatto del clipping dei quantili estremi

**Config**: `config/config_no_winsor.yaml`

```yaml
winsorization:
  enabled: false  # ‚Üê CAMBIO PRINCIPALE

target:
  transform: boxcox

database:
  use_poi: true
  use_ztl: true

profiles:
  tree:
    enabled: true
    winsorization:
      enabled: false  # ‚Üê Assicurarsi che sia disabilitato anche a livello profilo
  catboost:
    enabled: true

training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- Senza winsor ‚Üí feature numeriche con valori estremi
- Modelli tree-based: impatto minimo (gestiscono bene outlier)
- Profilo `scaled` (se abilitato): impatto maggiore su scaling

**Metriche da Confrontare**:
- R¬≤ test: atteso simile (~0.89-0.90)
- Worst predictions: potrebbero aumentare (pi√π sensibilit√† a estremi)

---

### üß™ **Esperimento C3: Split Temporale pi√π Conservativo (Test pi√π Recente)**

**Obiettivo**: Test set pi√π recente per simulare predizione su dati nuovi

**Config**: `config/config_temporal_recent.yaml`

```yaml
temporal_split:
  mode: fraction
  fraction:
    train: 0.6    # ‚Üê CAMBIO: meno training (default: 0.7)
    valid: 0.2
    # test: 0.2 (automatico)

target:
  transform: boxcox

database:
  use_poi: true
  use_ztl: true

profiles:
  tree: {enabled: true}
  catboost: {enabled: true}
training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- Test set pi√π recente ‚Üí potenziale drift temporale
- R¬≤ test potrebbe calare (dati pi√π difficili)
- Drift detection dovrebbe segnalare pi√π features

**Metriche da Confrontare**:
- R¬≤ test: atteso ~0.87-0.89 (potenziale calo)
- Drift alerts: atteso > 5 (vs ~3 baseline)
- Group metrics: verificare se bias temporale esiste

---

### üß™ **Esperimento C4: Soglia Correlazione pi√π Bassa (Mantenere pi√π Feature)**

**Obiettivo**: Evitare pruning aggressivo di feature correlate

**Config**: `config/config_low_corr_threshold.yaml`

```yaml
correlation:
  numeric_threshold: 0.99  # ‚Üê Pi√π permissivo (default: 0.95)

target:
  transform: boxcox

database:
  use_poi: true
  use_ztl: true

profiles:
  tree:
    enabled: true
    correlation:
      numeric_threshold: 0.99
  catboost:
    enabled: true
    correlation:
      numeric_threshold: 0.99

training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- Pi√π feature ‚Üí modelli pi√π complessi
- Potenziale overfitting aumentato
- R¬≤ train potrebbe salire, R¬≤ test potrebbe calare

**Metriche da Confrontare**:
- Feature count: atteso +10-20 feature
- R¬≤ train: atteso ~0.96 (aumento)
- R¬≤ test: atteso ~0.88-0.89 (potenziale calo)
- Overfitting gap: atteso aumento

---

#### **CATEGORIA D: VARIAZIONI MODELLI**

---

### üß™ **Esperimento D1: Solo Ensemble (No Modelli Singoli)**

**Obiettivo**: Verificare se ensemble battono sempre i singoli

**Config**: `config/config_ensemble_only.yaml`

```yaml
target:
  transform: boxcox

database:
  use_poi: true
  use_ztl: true

profiles:
  tree: {enabled: true}
  catboost: {enabled: true}

training:
  models:
    # Train tutti i modelli ma focus su ensemble
    rf: {enabled: true, trials: 50}
    xgboost: {enabled: true, trials: 50}
    lightgbm: {enabled: true, trials: 50}
    catboost: {enabled: true, trials: 50}
    hgbt: {enabled: true, trials: 50}
  ensembles:
    voting:
      enabled: true
      top_n: 5  # ‚Üê Usare tutti i top 5
      tune_weights: true
    stacking:
      enabled: true
      top_n: 5
      final_estimator: ridge
      cv_folds: 5
```

**Ipotesi**:
- Ensemble quasi sempre migliori dei singoli
- Stacking > Voting (meta-learner impara combinazione ottimale)

**Metriche da Confrontare**:
- Confrontare R¬≤ di voting, stacking vs best single model
- Atteso: stacking R¬≤ ~0.91-0.92 (migliore dei singoli)

---

### üß™ **Esperimento D2: Solo XGBoost (Modello Singolo Best)**

**Obiettivo**: Training intensivo su un solo modello

**Config**: `config/config_xgboost_only.yaml`

```yaml
target:
  transform: boxcox

database:
  use_poi: true
  use_ztl: true

profiles:
  tree: {enabled: true}

training:
  trials_advanced: 500  # ‚Üê Molti pi√π trial
  models:
    xgboost:
      enabled: true
      profile: tree
      trials: 500  # ‚Üê Search space esaustivo
      base_params: {}
      search_space:
        n_estimators: {type: int, low: 500, high: 2000}
        max_depth: {type: int, low: 3, high: 10}
        learning_rate: {type: float, low: 0.005, high: 0.1, log: true}
        subsample: {type: float, low: 0.5, high: 1.0}
        colsample_bytree: {type: float, low: 0.5, high: 1.0}
        min_child_weight: {type: float, low: 1.0, high: 15.0, log: true}
        reg_alpha: {type: float, low: 1e-5, high: 10.0, log: true}
        reg_lambda: {type: float, low: 1e-5, high: 10.0, log: true}
        gamma: {type: float, low: 1e-5, high: 10.0, log: true}
  ensembles:
    voting: {enabled: false}
    stacking: {enabled: false}
```

**Ipotesi**:
- Tuning intensivo pu√≤ far emergere configurazioni migliori
- R¬≤ test atteso: ~0.90-0.92
- Tempo: ~2-3 ore (vs ~30min baseline)

**Metriche da Confrontare**:
- R¬≤ vs baseline XGBoost con 100 trial
- Verificare se best_params sono molto diversi

---

### üß™ **Esperimento D3: Modelli Lineari vs Tree-Based**

**Obiettivo**: Confrontare performance di modelli lineari (con profilo `scaled`)

**Config**: `config/config_linear_models.yaml`

```yaml
target:
  transform: boxcox

database:
  use_poi: true
  use_ztl: true

profiles:
  scaled:  # ‚Üê Profilo per modelli lineari
    enabled: true
    output_prefix: scaled
    winsorization: {enabled: true, lower_quantile: 0.01, upper_quantile: 0.99}
    scaling: {scaler_type: standard}
    pca: {enabled: true, n_components: 0.95}

training:
  models:
    ridge:
      enabled: true
      profile: scaled
      trials: 100
    lasso:
      enabled: true
      profile: scaled
      trials: 100
    elasticnet:
      enabled: true
      profile: scaled
      trials: 100
    # Per confronto
    xgboost:
      enabled: true
      profile: tree
      trials: 100
```

**Ipotesi**:
- Modelli lineari: meno performanti ma pi√π interpretabili
- R¬≤ atteso: ~0.80-0.85 (vs ~0.90 tree-based)
- Pi√π veloci da trainare

**Metriche da Confrontare**:
- R¬≤ test: confrontare ridge/lasso/elasticnet vs xgboost
- Training time
- Interpretabilit√†: coefficienti lineari vs SHAP

---

### üß™ **Esperimento D4: CatBoost con Iterazioni Elevate**

**Obiettivo**: Sfruttare appieno la potenza di CatBoost

**Config**: `config/config_catboost_heavy.yaml`

```yaml
target:
  transform: boxcox

database:
  use_poi: true
  use_ztl: true

profiles:
  catboost:
    enabled: true
    output_prefix: catboost

training:
  models:
    catboost:
      enabled: true
      profile: catboost
      trials: 200  # ‚Üê Pi√π trial
      base_params:
        allow_writing_files: false
        iterations: 2000  # ‚Üê Baseline alto
      search_space:
        iterations: {type: int, low: 1000, high: 3000}  # ‚Üê Search space alto
        depth: {type: int, low: 4, high: 10}
        learning_rate: {type: float, low: 0.001, high: 0.1, log: true}
        l2_leaf_reg: {type: float, low: 1.0, high: 20.0}
        bagging_temperature: {type: float, low: 0.0, high: 5.0}
        border_count: {type: int, low: 32, high: 255}
  ensembles:
    voting: {enabled: false}
    stacking: {enabled: false}
```

**Ipotesi**:
- CatBoost con pi√π iterazioni pu√≤ migliorare performance
- R¬≤ test atteso: ~0.91-0.93
- Rischio overfitting con troppe iterazioni

**Metriche da Confrontare**:
- R¬≤ vs baseline CatBoost
- Overfitting gap: monitorare che non esploda

---

#### **CATEGORIA E: SPLIT E VALIDAZIONE**

---

### üß™ **Esperimento E1: Cross-Validation K-Fold (No Validation Set)**

**Obiettivo**: Usare tutto il train set con CV invece di hold-out validation

**Config**: `config/config_kfold_cv.yaml`

```yaml
temporal_split:
  mode: fraction
  fraction:
    train: 0.8    # ‚Üê Pi√π train (no validation hold-out)
    valid: 0.0    # ‚Üê NESSUNA VALIDATION!
    # test: 0.2

training:
  cv_when_no_val:
    enabled: true
    kind: kfold    # ‚Üê K-Fold CV
    n_splits: 5
    shuffle: true

target:
  transform: boxcox

database:
  use_poi: true
  use_ztl: true

profiles:
  tree: {enabled: true}
  catboost: {enabled: true}

training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- Pi√π dati per training ‚Üí modelli potenzialmente migliori
- CV con K=5 ‚Üí tuning pi√π robusto (5 fold invece di 1 validation set)
- Tempo: ~5x pi√π lungo (5 fold)

**Metriche da Confrontare**:
- R¬≤ test: atteso ~0.90-0.91 (potenziale miglioramento)
- Std deviation dei CV scores: indicatore di stabilit√†

---

### üß™ **Esperimento E2: Temporal Split con Data Fissa (Non Fraction)**

**Obiettivo**: Test set = dati 2023+ (simulazione real-world)

**Config**: `config/config_temporal_date.yaml`

```yaml
temporal_split:
  mode: date  # ‚Üê CAMBIO: usa data invece di fraction
  date:
    test_start_year: 2023
    test_start_month: 1

target:
  transform: boxcox

database:
  use_poi: true
  use_ztl: true

profiles:
  tree: {enabled: true}
  catboost: {enabled: true}

training:
  models: {rf: {enabled: true}, xgboost: {enabled: true}, lightgbm: {enabled: true}, catboost: {enabled: true}}
```

**Ipotesi**:
- Test set = dati recenti ‚Üí valutazione pi√π realistica
- Potenziale drift temporale (prezzi crescenti, inflazione)
- R¬≤ test atteso: ~0.87-0.89 (potenziale calo)

**Metriche da Confrontare**:
- R¬≤ test: confrontare con fraction mode
- Drift detection: atteso pi√π alerts
- Price band metrics: verificare se prezzi sono shifted

---

---

### üìä ESPERIMENTI SUMMARY TABLE

| Esperimento | Config File | Obiettivo | Tempo Atteso | R¬≤ Test Atteso | Priorit√† |
|------------|-------------|-----------|--------------|----------------|----------|
| **A1** | `config_no_transform.yaml` | Ablation target transform | 30min | 0.85-0.87 | üî• ALTA |
| **A2** | `config_no_poi.yaml` | Ablation POI features | 25min | 0.87-0.88 | üî• ALTA |
| **A3** | `config_no_ztl.yaml` | Ablation ZTL feature | 25min | 0.89-0.90 | üü° MEDIA |
| **A4** | `config_no_cened.yaml` | Ablation CENED views | 25min | 0.88-0.89 | üü° MEDIA |
| **A5** | `config_minimal.yaml` | Solo dati base (no enrichment) | 20min | 0.85-0.87 | üî• ALTA |
| **B1** | `config_target_mq.yaml` | Target = Prezzo/m¬≤ | 30min | 0.91-0.93 | üî• ALTA |
| **B2** | `config_log_transform.yaml` | Log vs Box-Cox | 30min | 0.89-0.90 | üü° MEDIA |
| **B3** | `config_yeojohnson.yaml` | Yeo-Johnson transform | 30min | 0.89-0.91 | üü¢ BASSA |
| **C1** | `config_outlier_aggressive.yaml` | Outlier removal pi√π aggressivo | 30min | 0.90-0.92 | üü° MEDIA |
| **C2** | `config_no_winsor.yaml` | No winsorization | 30min | 0.89-0.90 | üü¢ BASSA |
| **C3** | `config_temporal_recent.yaml` | Test set pi√π recente | 30min | 0.87-0.89 | üî• ALTA |
| **C4** | `config_low_corr_threshold.yaml` | Mantenere pi√π feature | 35min | 0.88-0.89 | üü¢ BASSA |
| **D1** | `config_ensemble_only.yaml` | Focus su ensemble | 40min | 0.91-0.92 | üü° MEDIA |
| **D2** | `config_xgboost_only.yaml` | Tuning intensivo XGBoost | 2-3h | 0.90-0.92 | üî• ALTA |
| **D3** | `config_linear_models.yaml` | Lineari vs Tree-based | 25min | 0.80-0.85 | üü¢ BASSA |
| **D4** | `config_catboost_heavy.yaml` | CatBoost con molte iterazioni | 1-2h | 0.91-0.93 | üü° MEDIA |
| **E1** | `config_kfold_cv.yaml` | K-Fold CV (no validation hold-out) | 2-3h | 0.90-0.91 | üü° MEDIA |
| **E2** | `config_temporal_date.yaml` | Split temporale con data fissa | 30min | 0.87-0.89 | üî• ALTA |

---

### üéØ ESPERIMENTI CONSIGLIATI - FASE 1 (Quick Wins)

Eseguire questi **5 esperimenti** per massimo impatto con minimo tempo:

1. **A1** - No Transform: capire valore trasformazione target
2. **A2** - No POI: capire valore features geografiche
3. **A5** - Minimal: baseline senza enrichment
4. **B1** - Target MQ: confrontare prezzo totale vs prezzo/m¬≤
5. **C3** - Recent Test: valutare drift temporale

**Tempo totale Fase 1**: ~2.5 ore  
**Output**: Report comparativo con 6 configurazioni (baseline + 5 esperimenti)

---

### üöÄ ESPERIMENTI CONSIGLIATI - FASE 2 (Deep Dive)

Dopo Fase 1, eseguire questi per ottimizzazione:

1. **D2** - XGBoost Heavy: tuning intensivo best model
2. **D4** - CatBoost Heavy: sfruttare categoriche native
3. **E1** - K-Fold CV: validazione pi√π robusta
4. **D1** - Ensemble Focus: ottimizzare combinazione modelli

**Tempo totale Fase 2**: ~8-10 ore  
**Output**: Best model production-ready

---

## üìä COME CONFRONTARE I RISULTATI

### 1. **Creare Tabella Comparativa**

```python
import pandas as pd
import json
from pathlib import Path

results = []
for exp_name in ['baseline', 'no_poi', 'no_transform', 'target_mq', 'minimal']:
    model_dir = Path(f'models_{exp_name}')  # Usare models_dir diversi per esperimento
    summary = json.loads((model_dir / 'summary.json').read_text())
    
    # Best single model
    best_model = max(summary['models'].items(), key=lambda x: x[1]['metrics_test_original']['r2'])
    
    results.append({
        'Experiment': exp_name,
        'Best_Model': best_model[0],
        'R2_test_orig': best_model[1]['metrics_test_original']['r2'],
        'RMSE_test_orig': best_model[1]['metrics_test_original']['rmse'],
        'MAE_test_orig': best_model[1]['metrics_test_original']['mae'],
        'MAPE_floor': best_model[1]['metrics_test_original'].get('mape_floor', None),
        'Overfitting_gap_R2': best_model[1]['overfit']['gap_r2'],
        'Overfitting_ratio_RMSE': best_model[1]['overfit']['ratio_rmse']
    })

df_comparison = pd.DataFrame(results).sort_values('R2_test_orig', ascending=False)
df_comparison.to_csv('experiments_comparison.csv', index=False)
print(df_comparison)
```

**Output Esempio**:

```csv
Experiment,Best_Model,R2_test_orig,RMSE_test_orig,MAE_test_orig,MAPE_floor,Overfitting_gap_R2,Overfitting_ratio_RMSE
baseline,xgboost,0.9012,18567.89,13456.78,0.1259,0.0511,1.5034
target_mq,xgboost,0.9201,16234.56,11890.12,0.1089,0.0489,1.4523
no_poi,lightgbm,0.8789,21234.56,15678.90,0.1456,0.0612,1.6234
no_transform,xgboost,0.8567,23456.78,17890.12,0.1678,0.0734,1.7890
minimal,catboost,0.8456,25678.90,19123.45,0.1789,0.0678,1.6123
```

### 2. **Visualizzazione Comparativa**

```python
import matplotlib.pyplot as plt
import seaborn as sns

# R¬≤ Comparison
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Plot 1: R¬≤ Test
df_comparison.plot.bar(x='Experiment', y='R2_test_orig', ax=axes[0], legend=False)
axes[0].set_title('R¬≤ Test (Original Scale)', fontsize=14)
axes[0].set_ylabel('R¬≤')
axes[0].axhline(y=0.90, color='r', linestyle='--', label='Target: 0.90')
axes[0].legend()

# Plot 2: RMSE Test
df_comparison.plot.bar(x='Experiment', y='RMSE_test_orig', ax=axes[1], legend=False, color='orange')
axes[1].set_title('RMSE Test (‚Ç¨)', fontsize=14)
axes[1].set_ylabel('RMSE (‚Ç¨)')
axes[1].axhline(y=20000, color='r', linestyle='--', label='Target: 20k‚Ç¨')
axes[1].legend()

# Plot 3: Overfitting Gap
df_comparison.plot.bar(x='Experiment', y='Overfitting_gap_R2', ax=axes[2], legend=False, color='green')
axes[2].set_title('Overfitting Gap (R¬≤)', fontsize=14)
axes[2].set_ylabel('Gap R¬≤ (train - test)')
axes[2].axhline(y=0.05, color='r', linestyle='--', label='Threshold: 0.05')
axes[2].legend()

plt.tight_layout()
plt.savefig('experiments_comparison.png', dpi=150, bbox_inches='tight')
plt.show()
```

### 3. **Feature Importance Comparison (SHAP)**

```python
# Confrontare top 10 feature per ogni esperimento
import numpy as np

shap_comparison = {}
for exp_name in ['baseline', 'no_poi', 'target_mq']:
    model_dir = Path(f'models_{exp_name}/xgboost/shap')
    shap_values = np.load(model_dir / 'shap_values.npy', allow_pickle=True)
    feature_names = pd.read_parquet(model_dir.parent / 'shap_sample.parquet').columns
    
    # Mean absolute SHAP values
    mean_abs_shap = np.abs(shap_values).mean(axis=0)
    top_10_idx = np.argsort(mean_abs_shap)[-10:][::-1]
    
    shap_comparison[exp_name] = {
        'features': [feature_names[i] for i in top_10_idx],
        'importance': mean_abs_shap[top_10_idx]
    }

# Visualize
fig, ax = plt.subplots(figsize=(12, 6))
x = np.arange(10)
width = 0.25

for i, (exp_name, data) in enumerate(shap_comparison.items()):
    ax.bar(x + i*width, data['importance'], width, label=exp_name)

ax.set_xlabel('Feature Rank')
ax.set_ylabel('Mean |SHAP value|')
ax.set_title('Top 10 Features Comparison Across Experiments')
ax.legend()
plt.tight_layout()
plt.savefig('shap_comparison.png', dpi=150)
plt.show()
```

---

## üõ†Ô∏è SCRIPT DI AUTOMAZIONE ESPERIMENTI

### `run_experiments.sh`

```bash
#!/bin/bash

# Script per eseguire automaticamente tutti gli esperimenti
# Usage: ./run_experiments.sh

set -e  # Exit on error

BASE_CONFIG="config/config.yaml"
EXPERIMENTS_DIR="experiments_results"
mkdir -p "$EXPERIMENTS_DIR"

# Array di esperimenti: (nome, config_file, models_dir)
declare -a EXPERIMENTS=(
    "baseline:config/config.yaml:models_baseline"
    "no_poi:config/config_no_poi.yaml:models_no_poi"
    "no_transform:config/config_no_transform.yaml:models_no_transform"
    "target_mq:config/config_target_mq.yaml:models_target_mq"
    "minimal:config/config_minimal.yaml:models_minimal"
)

echo "========================================="
echo "STIMATRIX EXPERIMENTS BATCH RUN"
echo "========================================="
echo ""

for exp in "${EXPERIMENTS[@]}"; do
    IFS=':' read -r name config_file models_dir <<< "$exp"
    
    echo ">>> Running experiment: $name"
    echo "    Config: $config_file"
    echo "    Output: $models_dir"
    echo ""
    
    # Modificare temporaneamente la config per usare models_dir custom
    export MODELS_DIR="$models_dir"
    
    # Eseguire la pipeline
    python main.py --config "$config_file" --steps preprocessing training evaluation --force-reload
    
    # Copiare risultati in experiments_results
    cp "$models_dir/summary.json" "$EXPERIMENTS_DIR/summary_${name}.json"
    cp "$models_dir/validation_results.csv" "$EXPERIMENTS_DIR/validation_${name}.csv"
    
    echo "‚úÖ Completed: $name"
    echo ""
done

echo "========================================="
echo "ALL EXPERIMENTS COMPLETED"
echo "========================================="
echo ""
echo "Results saved in: $EXPERIMENTS_DIR/"
echo ""
echo "To generate comparison report, run:"
echo "  python scripts/compare_experiments.py $EXPERIMENTS_DIR/"
```

### `scripts/compare_experiments.py`

```python
#!/usr/bin/env python3
"""
Compare results from multiple experiments and generate report.

Usage:
    python scripts/compare_experiments.py experiments_results/
"""

import json
import sys
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def load_experiment_summary(summary_path: Path) -> dict:
    """Load summary.json from experiment."""
    with open(summary_path) as f:
        return json.load(f)

def extract_best_model(summary: dict) -> dict:
    """Extract best model metrics from summary."""
    models = summary.get('models', {})
    if not models:
        return None
    
    # Find best by R¬≤ test original
    best_key = max(
        models.keys(),
        key=lambda k: models[k].get('metrics_test_original', {}).get('r2', -999)
    )
    
    best = models[best_key]
    return {
        'model_key': best_key,
        'r2_test_orig': best['metrics_test_original']['r2'],
        'rmse_test_orig': best['metrics_test_original']['rmse'],
        'mae_test_orig': best['metrics_test_original']['mae'],
        'mape_floor': best['metrics_test_original'].get('mape_floor', None),
        'overfit_gap_r2': best['overfit']['gap_r2'],
        'overfit_ratio_rmse': best['overfit']['ratio_rmse']
    }

def main(experiments_dir: str):
    """Generate comparison report."""
    exp_dir = Path(experiments_dir)
    
    # Find all summary files
    summaries = list(exp_dir.glob('summary_*.json'))
    print(f"Found {len(summaries)} experiments")
    
    # Extract results
    results = []
    for summary_path in summaries:
        exp_name = summary_path.stem.replace('summary_', '')
        summary = load_experiment_summary(summary_path)
        best = extract_best_model(summary)
        
        if best:
            results.append({
                'Experiment': exp_name,
                **best
            })
    
    # Create DataFrame
    df = pd.DataFrame(results).sort_values('r2_test_orig', ascending=False)
    
    # Save CSV
    output_csv = exp_dir / 'comparison.csv'
    df.to_csv(output_csv, index=False)
    print(f"‚úÖ Saved comparison table: {output_csv}")
    
    # Print table
    print("\n" + "="*80)
    print("EXPERIMENTS COMPARISON")
    print("="*80 + "\n")
    print(df.to_string(index=False))
    print("\n")
    
    # Plot comparison
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Plot 1: R¬≤ Test
    df.plot.bar(x='Experiment', y='r2_test_orig', ax=axes[0, 0], legend=False, color='steelblue')
    axes[0, 0].set_title('R¬≤ Test (Original Scale)', fontsize=12, fontweight='bold')
    axes[0, 0].set_ylabel('R¬≤')
    axes[0, 0].axhline(y=0.90, color='red', linestyle='--', linewidth=2, label='Target: 0.90')
    axes[0, 0].legend()
    axes[0, 0].grid(axis='y', alpha=0.3)
    
    # Plot 2: RMSE Test
    df.plot.bar(x='Experiment', y='rmse_test_orig', ax=axes[0, 1], legend=False, color='darkorange')
    axes[0, 1].set_title('RMSE Test (‚Ç¨)', fontsize=12, fontweight='bold')
    axes[0, 1].set_ylabel('RMSE (‚Ç¨)')
    axes[0, 1].axhline(y=20000, color='red', linestyle='--', linewidth=2, label='Target: 20k‚Ç¨')
    axes[0, 1].legend()
    axes[0, 1].grid(axis='y', alpha=0.3)
    
    # Plot 3: MAE Test
    df.plot.bar(x='Experiment', y='mae_test_orig', ax=axes[1, 0], legend=False, color='green')
    axes[1, 0].set_title('MAE Test (‚Ç¨)', fontsize=12, fontweight='bold')
    axes[1, 0].set_ylabel('MAE (‚Ç¨)')
    axes[1, 0].grid(axis='y', alpha=0.3)
    
    # Plot 4: Overfitting Gap
    df.plot.bar(x='Experiment', y='overfit_gap_r2', ax=axes[1, 1], legend=False, color='purple')
    axes[1, 1].set_title('Overfitting Gap (R¬≤)', fontsize=12, fontweight='bold')
    axes[1, 1].set_ylabel('Gap R¬≤ (train - test)')
    axes[1, 1].axhline(y=0.05, color='red', linestyle='--', linewidth=2, label='Threshold: 0.05')
    axes[1, 1].legend()
    axes[1, 1].grid(axis='y', alpha=0.3)
    
    plt.suptitle('Experiments Performance Comparison', fontsize=16, fontweight='bold', y=1.00)
    plt.tight_layout()
    
    output_plot = exp_dir / 'comparison.png'
    plt.savefig(output_plot, dpi=150, bbox_inches='tight')
    print(f"‚úÖ Saved comparison plot: {output_plot}")
    
    # Summary statistics
    print("\n" + "="*80)
    print("SUMMARY STATISTICS")
    print("="*80 + "\n")
    print(f"Best R¬≤ Test:    {df['r2_test_orig'].max():.4f} ({df.loc[df['r2_test_orig'].idxmax(), 'Experiment']})")
    print(f"Worst R¬≤ Test:   {df['r2_test_orig'].min():.4f} ({df.loc[df['r2_test_orig'].idxmin(), 'Experiment']})")
    print(f"Best RMSE Test:  {df['rmse_test_orig'].min():.2f}‚Ç¨ ({df.loc[df['rmse_test_orig'].idxmin(), 'Experiment']})")
    print(f"Worst RMSE Test: {df['rmse_test_orig'].max():.2f}‚Ç¨ ({df.loc[df['rmse_test_orig'].idxmax(), 'Experiment']})")
    print()

if __name__ == '__main__':
    if len(sys.argv) != 2:
        print("Usage: python compare_experiments.py <experiments_dir>")
        sys.exit(1)
    
    main(sys.argv[1])
```

**Come Usare**:

```bash
# 1. Rendere eseguibile
chmod +x run_experiments.sh
chmod +x scripts/compare_experiments.py

# 2. Eseguire batch di esperimenti
./run_experiments.sh

# 3. Generare report comparativo
python scripts/compare_experiments.py experiments_results/

# Output:
# - experiments_results/comparison.csv
# - experiments_results/comparison.png
```

---

## üéì CONCLUSIONI E RACCOMANDAZIONI

### ‚úÖ Cosa Funziona Bene

1. **Architettura Modulare**: Eccellente separazione dei concern
2. **Testing Coverage**: Suite test completa e ben strutturata
3. **Configurabilit√†**: YAML con env vars √® ottimale per MLOps
4. **Experiment Tracking**: W&B integration nativa
5. **Target Transformation**: Box-Cox con Duan smearing √® state-of-the-art
6. **Diagnostics**: Residual analysis, drift, PI sono production-ready
7. **Profili Multipli**: Supporto tree/catboost/scaled √® flessibile
8. **Security**: Credential management e input validation sono robuste

### üîß Suggerimenti di Miglioramento (Non Urgenti)

#### 1. **Refactoring Codice**

```python
# File: src/preprocessing/pipeline.py
# Attuale: run_preprocessing() √® lunga ~850 righe

# Suggerito: Spezzare in funzioni
def run_preprocessing(config: Dict[str, Any]) -> Path:
    # Load and clean
    df = load_and_clean_data(config)
    
    # Feature engineering
    df = extract_features(df, config)
    
    # Temporal split
    train_df, val_df, test_df = split_temporal(df, config)
    
    # Outliers and target transform
    train_df = remove_outliers(train_df, config)
    y_train, y_val, y_test, transform_meta = transform_targets(train_df, val_df, test_df, config)
    
    # Imputation
    X_train, X_val, X_test, imputers = fit_and_apply_imputation(train_df, val_df, test_df, config)
    
    # Process profiles
    for profile_name, profile_cfg in get_enabled_profiles(config):
        process_profile(profile_name, profile_cfg, X_train, y_train, X_val, y_val, X_test, y_test, config)
    
    return save_results(config)
```

#### 2. **Caching Intermedio**

```python
# Aggiungere cache per step preprocessing
@lru_cache(maxsize=1)
def load_and_clean_data(config_hash: str):
    # Cache del raw data cleaning
    cache_file = Path(f'cache/cleaned_{config_hash}.parquet')
    if cache_file.exists():
        return pd.read_parquet(cache_file)
    # ... processing
    df.to_parquet(cache_file)
    return df
```

#### 3. **Monitoring Produzione**

```python
# File: src/utils/monitoring.py
def setup_monitoring(config):
    """Setup Prometheus/Grafana metrics."""
    from prometheus_client import Counter, Histogram
    
    PREDICTIONS = Counter('model_predictions_total', 'Total predictions')
    LATENCY = Histogram('prediction_latency_seconds', 'Prediction latency')
    ERRORS = Counter('model_errors_total', 'Total errors')
    
    return PREDICTIONS, LATENCY, ERRORS
```

#### 4. **Deployment Ready**

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    unixodbc \
    unixodbc-dev \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Healthcheck
HEALTHCHECK --interval=30s --timeout=5s --retries=3 \
  CMD python -c "import sys; sys.exit(0)"

# Run
CMD ["python", "main.py", "--config", "config/config.yaml", "--steps", "training"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  stimatrix-training:
    build: .
    environment:
      - SERVER=${DB_SERVER}
      - DATABASE=${DB_NAME}
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - WANDB_API_KEY=${WANDB_API_KEY}
      - WANDB_ENABLED=1
      - LOG_LEVEL=INFO
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    command: python main.py --steps training evaluation
```

### üöÄ Prossimi Passi Consigliati

**Short-term (1-2 settimane)**:
1. Eseguire esperimenti Fase 1 (A1, A2, A5, B1, C3)
2. Analizzare risultati e identificare best configuration
3. Documentare insights e pubblicare report interno

**Mid-term (1 mese)**:
1. Eseguire esperimenti Fase 2 (D2, D4, E1, D1)
2. Ottimizzare best model con tuning intensivo
3. Testare su holdout set finale (se disponibile)

**Long-term (2-3 mesi)**:
1. Deploy modello in produzione (API REST + monitoring)
2. Implementare CI/CD pipeline per retraining automatico
3. Setup A/B testing per confronto modelli in produzione

---

## üìö RIFERIMENTI E RISORSE

### Documentazione Interna

- **README.md**: Guida completa della pipeline
- **notebooks/README.md**: Guida EDA e notebooks
- **sql/README.md**: Documentazione template SQL
- **tests/**: Suite test con esempi di uso

### Paper e Metodologie

- **Box-Cox Transformation**: Box, G. E. P., & Cox, D. R. (1964). "An analysis of transformations"
- **Duan Smearing**: Duan, N. (1983). "Smearing Estimate: A Nonparametric Retransformation Method"
- **PSI (Population Stability Index)**: Siddiqi, N. (2006). "Credit Risk Scorecards"
- **SHAP Values**: Lundberg & Lee (2017). "A Unified Approach to Interpreting Model Predictions"

### Tool Documentation

- **Optuna**: https://optuna.org/
- **SHAP**: https://shap.readthedocs.io/
- **Weights & Biases**: https://docs.wandb.ai/
- **Scikit-learn**: https://scikit-learn.org/
- **XGBoost**: https://xgboost.readthedocs.io/
- **LightGBM**: https://lightgbm.readthedocs.io/
- **CatBoost**: https://catboost.ai/

---

**Fine della Review Completa**  
**Autore**: AI Assistant  
**Data**: 2025-11-11

Per domande o chiarimenti, fare riferimento ai file di configurazione e documentazione interna.
